---
title: "Monte Carlo Simulation"
subtitle: "Lecture 10"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "March 4, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: 
            method: mathjax
            url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        touch: false
        controls: true
execute:
    freeze: auto
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using LaTeXStrings
using Measures
using Animations

Random.seed!(1)
```

# Previously In Class

## Probability Models

- Probability Models for Data-Generating Processes
- Can be used for statistical models and simulation models (discrepancy)

## What's Next?

- How do we:
    - Simulate from models/propagate uncertainty;
    - Go beyond MLE/MAP estimation for parameter values?
- Model assessment/selection

# Monte Carlo Simulation

## Stochastic Simulation

**Goal**: Estimate $\mathbb{E}_p\left[f(x)\right]$, $x \sim p(x)$

::: {.fragment .fade-in}
::: {.incremental}
- Standard approach: Compute $\mathbb{E}_\mathcal{D}\left[f(x)\right] = \int_X f(x)p(x)dx$
- Monte Carlo: 
  - Sample $x^1, x^2, \ldots, x^N \sim p(x)$
  - Estimate $\mathbb{E}_\mathcal{D}\left[f(x)\right] \approx \sum_{n=1}^N f(x^n)$ / N
:::
:::

## Monte Carlo Process Schematic

```{dot}
//| fig-width: 100%
digraph G {
    graph [
        rankdir=LR
        layout=dot
    ]
    node [
        fontname = "IBM Plex Sans, sans-serif"
        fontsize=25
    ]
    edge [
        arrowsize=0.75
        labeldistance=3
        penwidth=3
        fontname = "IBM Plex Sans, sans-serif"
        fontsize=25
        style=dashed
        color="#b31b1b"
        fontcolor="#b31b1b"
    ]
    a [label="Probability\n Distribution"]
    b [label = "Random\n Samples"]
    c [label="Model"]
    d [label="Outputs"]

    a -> b [
        label="Sample"
    ]
    b -> c [
        label="Input"
    ]
    c -> d [
        label="Simulate"
    ]
}

```

## Goals of Monte Carlo

Monte Carlo is a broad method, which can be used to:

1. Obtain probability distributions of outputs;
2. Estimate deterministic quantities (***Monte Carlo estimation***).

## MC Example: Finding $\pi$

How can we use MC to estimate $\pi$?

**Hint**: Think of $\pi$ as an expected value...

## MC Example: Finding $\pi$

:::: {.columns}
::: {.column width=50%}
::: {.fragment .fade-in}
Finding $\pi$ by sampling random values from the unit square and computing the fraction in the unit circle. This is an example of **Monte Carlo integration**.

$$\frac{\text{Area of Circle}}{\text{Area of Square}} = \frac{\pi}{4}$$
:::
:::
::: {.column width=50%}
:::{.fragment .fade-in}
```{julia}
#| echo: true
#| label: fig-mcmc-pi
#| fig-cap: "MCMC Estimation of pi"
#| fig-height: 100%
#| code-fold: true

function circleShape(r)
    θ = LinRange(0, 2 * π, 500)
    r * sin.(θ), r * cos.(θ)
end

nsamp = 3000
unif = Uniform(-1, 1)
x = rand(unif, (nsamp, 2))
l = mapslices(v -> sum(v.^2), x, dims=2)
in_circ = l .< 1
pi_est = [4 * mean(in_circ[1:i]) for i in 1:nsamp]

plt1 = plot(
    1,
    xlim = (-1, 1),
    ylim = (-1, 1),
    legend = false,
    markersize = 4,
    framestyle = :origin,
    tickfontsize=16,
    grid=:false
    )
plt2 = plot(
    1,
    xlim = (1, nsamp),
    ylim = (3, 3.5),
    legend = :false,
    linewidth=3, 
    color=:black,
    tickfontsize=16,
    guidefontsize=16,
    xlabel="Iteration",
    ylabel="Estimate",
    right_margin=5mm
)
hline!(plt2, [π], color=:red, linestyle=:dash)
plt = plot(plt1, plt2, layout=grid(2, 1, heights=[2/3, 1/3]), size=(600, 500))

plot!(plt, circleShape(1), linecolor=:blue, lw=1, aspectratio=1, subplot=1)


mc_anim = @animate for i = 1:nsamp
    if l[i] < 1
        scatter!(plt[1], Tuple(x[i, :]), color=:blue, markershape=:x, subplot=1)
    else
        scatter!(plt[1], Tuple(x[i, :]), color=:red, markershape=:x, subplot=1)
    end
    push!(plt, 2, i, pi_est[i])
end every 100

gif(mc_anim, "figures/mc_pi.gif", fps=3)
```
:::
:::
::::

## MC Example: Dice

:::: {.columns}
::: {.column width=50%}
What is the probability of rolling 4 dice for a total of 19?

::: {.fragment .fade-in}
Can simulate dice rolls and find the frequency of 19s among the samples.
:::
:::
::: {.column width=50%}

::: {.fragment .fade-in}
```{julia}
#| echo: true
#| fig-width: 100%
#| label: fig-mcmc-dice
#| fig-cap: "MCMC Estimation of pi"
#| fig-height: 100%
#| code-fold: true

function dice_roll_repeated(n_trials, n_dice)
    dice_dist = DiscreteUniform(1, 6) 
	roll_results = zeros(n_trials)
	for i=1:n_trials
		roll_results[i] = sum(rand(dice_dist, n_dice))
	end
	return roll_results
end

nsamp = 10000
# roll four dice 10000 times
rolls = dice_roll_repeated(nsamp, 4) 

# calculate probability of 19
sum(rolls .== 19) / length(rolls)

# initialize storage for frequencies by sample length
avg_freq = zeros(length(rolls)) 
std_freq = zeros(length(rolls)) 

# compute average frequencies of 19
avg_freq[1] = (rolls[1] == 19)
count = 1
for i=2:length(rolls)
    avg_freq[i] = (avg_freq[i-1] * (i-1) + (rolls[i] == 19)) / i
    std_freq[i] = 1/sqrt(i-1) * std(rolls[1:i] .== 19)
end

plt = plot(
    1,
    xlim = (1, nsamp),
    ylim = (0, 0.1),
    legend = :false,
    tickfontsize=16,
    guidefontsize=16,
    xlabel="Iteration",
    ylabel="Estimate",
    right_margin=8mm,
    color=:black,
    linewidth=3,
    size=(600, 400)
)
hline!(plt, [0.0432], color="red", 
    linestyle=:dash) 

mc_anim = @animate for i = 1:nsamp
    push!(plt, 1, i, avg_freq[i])
end every 100

gif(mc_anim, "figures/mc_dice.gif", fps=10)

```
:::
:::
::::

## Monte Carlo and Uncertainty Propagation

**Monte Carlo simulation**: propagate uncertainties from 

This is an example of **uncertainty propagation**: draw samples from some distribution, and run them through one or more models to find the (conditional) probability of outcomes of interest (for good or bad).

## Uncertainty Propagation Flowchart

:::: {.columns}
::: {.column width=40%}
**For example**: What is the probability that a levee will be overtopped given climate and extreme sea-level uncertainty?
:::
::: {.column width=60%}
```{mermaid}
%%| mermaid-format: png
%%| fig-align: center
%%| fig-width: 7.5

flowchart LR
    a(("Climate\n Sensitivity"))
    b["Global\n Temperature"]
    c(("Sea-Level\n Sensitivity"))
    d["Sea-Level\n Rise"]
    e(("GEV\n Parameters"))
    f["Storm Tides"]
    g["Overtopping"]

    a-->b
    c-->d
    b-->d
    e-->f
    d-->g
    f-->g

    classDef sample fill:#fad,stroke:#333
    classDef metric fill:#ad3,stroke:#333
    classDef model fill:#3df,stroke:#333

    class a,c,e sample
    class g metric
    class b,d,f model
```
:::
::::

## What Do We Need For MC?

::: {.incremental}
1. Simulation Model (Numerical/Statistical)
2. ***Input Distributions***
:::

## On Random Number Generators

:::: {.columns}
::: {.column width=40%}
Random number generators are not *really* random, only **pseudorandom**.

This is why setting a seed is important. But even that can go wrong...  
:::
::: {.column width=60%}

![XKCD Cartoon 221: Random Number](https://imgs.xkcd.com/comics/random_number.png){width=90%}

::: {.caption}
Source: [XKCD #221](https://xkcd.com/221/)
:::
:::
::::


# Why Monte Carlo Works

## Monte Carlo: Formal Approach

**Formally**: Monte Carlo estimation as the computation of the expected value of a random quantity $Y$, $\mu = \mathbb{E}[Y]$.

To do this, generate $n$ independent and identically distributed values $Y_1, \ldots, Y_n$.  Then the sample estimate is

$$\tilde{\mu}_n = \frac{1}{n}\sum_{i=1}^n Y_i$$

## The Law of Large Numbers

If 

(1) $Y$ is a random variable and its expectation exists and 

(2) $Y_1, \ldots, Y_n$ are independently and identically distributed

Then by the **weak law of large numbers**:

$$\lim_{n \to \infty} \mathbb{P}\left(\left|\tilde{\mu}_n - \mu\right| \leq \varepsilon \right) = 1$$

## The Law of Large Numbers

In other words, *eventually* Monte Carlo estimates will get within an arbitrary error of the true expectation. 

But how large is large enough?


## Monte Carlo Sample Mean

The sample mean $\tilde{\mu}_n = \frac{1}{n}\sum_{i=1}^n Y_i$ is itself a random variable.

::: {.fragment .fade-in}
With some assumptions (the mean of $Y$ exists and $Y$ has finite variance), the expected Monte Carlo sample mean $\mathbb{E}[\tilde{\mu}_n]$ is

$$\frac{1}{n}\sum_{i=1}^n \mathbb{E}[Y_i] = \frac{1}{n} n \mu = \mu$$

:::

## Monte Carlo Error

We'd like to know more about the error of this estimate for a given sample size. The variance of this estimator is

$$\tilde{\sigma}_n^2 = \text{Var}\left(\tilde{\mu}_n\right) = \mathbb{E}\left((\tilde{\mu}_n - \mu)^2\right) = \frac{\sigma_Y^2}{n}$$

::: {.fragment .fade-in}
So as $n$ increases, the *standard error* decreases:

$$\tilde{\sigma}_n = \frac{\sigma_Y}{\sqrt{n}}$$
:::


## Monte Carlo Error

In other words, if we want to decrease the Monte Carlo error by 10x, we need 100x additional samples. **This is not an ideal method for high levels of accuracy.** 

::: {.fragment .fade-in}
::: {.quote}
> Monte Carlo is an extremely bad method. It should only be used when all alternative methods are worse.

::: {.cite}
--- Sokal, *Monte Carlo Methods in Statistical Mechanics*, 1996
:::
:::
:::

::: {.fragment .fade-in}

But...often most alternatives *are* worse!
:::

## When Might We Want to Use Monte Carlo?

::: {.fragment .fade-in}
- All models are wrong, and so there always exists some irreducible model error. Can we reduce the Monte Carlo error enough so it's less than the model error and other uncertainties?
- We often need a lot of simulations. Do we have enough computational power?
:::

## When Might We Want to Use Monte Carlo?

If you can compute your answers analytically or through quadrature, you probably should. 

But for *many* "real" problems, this is either

1. Not possible (or computationally intractable);
2. Requires a lot of stylization and simplification.

## Monte Carlo Confidence Intervals

**Basic Idea**: The *Central Limit Theorem* says that with enough samples, the errors are normally distributed:

$$\left\|\tilde{\mu}_n - \mu\right\| \to \mathcal{N}\left(0, \frac{\sigma_Y^2}{n}\right)$$

## Monte Carlo Confidence Intervals

The $\alpha$-confidence interval is:
$$\tilde{\mu}_n \pm \Phi^{-1}\left(1 - \frac{\alpha}{2}\right) \frac{\sigma_Y}{\sqrt{n}}$$

For example, the 95% confidence interval is $$\tilde{\mu}_n \pm 1.96 \frac{\sigma_Y}{\sqrt{n}}.$$

## Sidebar: Estimating $\sigma_Y$

We don't know the standard deviation $\sigma_Y$.

But we can estimate it using the simulation standard deviation:

## Implications of Monte Carlo Error

Converging at a rate of $1/\sqrt{n}$ is not great. But:

- All models are wrong, and so there always exists some irreducible model error. 
- We often need a lot of simulations. Do we have enough computational power?

# Implementing Monte Carlo

## Basic Strategy

Store the results for each iteration and update the mean/standard deviation:

$$\tilde{\mu}_{n+1} = n \tilde{\mu}_n + f(Y_{n+1})$$

$$\tilde{\sigma}_{n+1} = \frac{\hat{\sigma}_{n+1}}{\sqrt{n+1}}$$

## Example: Dice Rolls




# Key Points and Upcoming Schedule

## Key Points

::: {.incremental}
- Monte Carlo: estimate by simulating summary statistics
- Instead of computing integrals, approximate through sample summaries
- MC estimates are themselves random quantities
- Confidence intervals obtained through Central Limit Theorem
- $\tilde{\mu}_n \to \mu$ at rate $1/\sqrt{n}$: not great!
:::

## Next Classes

**Wednesday**: Monte Carlo Examples: Flood Risk and Climate Change

**Next Week**: The Bootstrap

## Assessments

**Exercise 7**: Assigned, due Friday

**Reading**: @Kale2021-fv

# References

## References