---
title: "MCMC: Convergence and Example"
subtitle: "Lecture 15"
author: "Vivek Srikrishnan"
course: "BEE 4850"
institution: "Cornell University"
date: "March 20, 2024"
format:
    revealjs:
        slide-number: c/t
        show-slide-number: all
        center-title-slide: true
        width: 1280
        height: 720
        transition: none
        toc: true
        toc-depth: 1
        toc-title: "Overview"
        history: false
        link-external-newwindow: true
        theme: ../sass/slides.scss
        template-partials:
            - title-slide.html
        menu:
            numbers: true
        html-math-method: 
            method: mathjax
            url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
        include-in-header: mathjax-config.html
        date-format: long
        email-obfuscation: javascript
        chalkboard:
            theme: whiteboard
            buttons: true
        touch: false
        controls: true
execute:
    freeze: auto
---

```{julia}
#| output: false

import Pkg
Pkg.activate(@__DIR__)
Pkg.instantiate()
```

```{julia}
#| output: false

using Random
using DataFrames
using DataFramesMeta
using CSV
using Dates
using Distributions
using ColorSchemes
using Plots
using StatsPlots
using StatsBase
using LaTeXStrings
using Measures
using LinearAlgebra
using MCMCChains
using Optim
using Turing

Random.seed!(1)
```

# Last Class(es)

## Markov Chain Strategy

- Generate an appropriate Markov chain so that its stationary distribution of the target distribution $\pi$;
- Run its dynamics long enough to converge to the stationary distribution;
- Use the resulting ensemble of states as Monte Carlo samples from $\pi$ .

## Markov Chain Convergence

Given a Markov chain $\{X_t\}_{t=1, \ldots, T}$ returned from this procedure, sampling from distribution $\pi$:

- $\mathbb{P}(X_t = y) \to \pi(y)$ as $t \to \infty$
- This means the chain can be considered a *dependent* sample approximately distributed from $\pi$.
- The first values (the *transient portion*) of the chain are highly dependent on the initial value.

## The Metropolis-Hastings Algorithm

Given $X_t = x_t$:

1. Generate $Y_t \sim q(y | x_t)$;
2. Set $X_{t+1} = Y_t$ with probability $\rho(x_t, Y_t)$, where
    $$\rho(x, y) = \min \left\{\frac{\pi(y)}{\pi(x)}\frac{q(x | y)}{q(y | x)}, 1\right\},$$
    else set $X_{t+1} = x_t$.

## Proposals

- "Goldilocks" proposal: acceptance rate 30-45%.
- Proposal distribution $q$ plays a big role in the **effective sample size** (ESS):
  $$N_\text{eff} = \frac{N}{1+2\sum_{t=1}^\infty \rho_t}$$

## Sampling Efficiency Example

::: {.center}
![MCMC Sampling for Various Proposals](figures/mcmc-trace.svg)
:::

# MCMC Convergence


## Transient Chain Portion

What do we do with the transient portion of the chain?

::: {.fragment .fade-in}
- Discard as *burn-in*;
- Just run the chain longer.
:::

## How To Identify Convergence?

**Short answer**: There is no guarantee! Judgement based on an accumulation of evidence from various heuristics.

- The good news &mdash; getting the precise "right" end of the transient chain doesn't matter. 
- If a few transient iterations remain, the effect will be washed out with a large enough post-convergence chain.

## Heuristics for Convergence

Compare distribution (histogram/kernel density plot) after half of the chain to full chain.

::: {#fig-convergence layout-ncol=2}
![2000 Iterations](figures/mh-densitycheck-2000.svg){width=100%}

![10000 Iterations](figures/mh-densitycheck-10000.svg){width=100%}

:::

## Gelman-Rubin Diagnostic

@Gelman1992-da

- Run multiple chains from "overdispersed" starting points
- Compare intra-chain and inter-chain variances
- Summarized as $\hat{R}$ statistic: closer to 1 implies better convergence.
- Can also check distributions across multiple chains vs. the half-chain check.

## On Multiple Chains

Unless a specific scheme is used, multiple chains are not a solution for issues of convergence, as each individual chain needs to converge and have burn-in discarded/watered-down. 

This means multiple chains are more useful for diagnostics, but once they've all been run long enough, can mix samples freely.


## Heuristics for Convergence

- If you're more interested in the mean estimate, can also look at the its stability by iteration or the *Monte Carlo standard error*.
- Look at traceplots; do you see sudden "jumps"?
- **When in doubt, run the chain longer.**

# Increasing Efficiency

## Adaptive Metropolis-Hastings

Adjust proposal density to hit target acceptance rate.

- Need to be cautious about detailed balance.
- Typical strategy is to adapt for a portion of the initial chain (part of the burn-in), then run longer with that proposal.

## Hamiltonian Monte Carlo

- **Idea**: Use proposals which steer towards "typical set" without collapsing towards the mode (based on Hamiltonian vector field);
- Requires gradient information: can be obtained through autodifferentiation; challenging for external models;
- Can be very efficient due to potential for anti-correlated samples, but very sensitive to parameterization.
- Same principles for evaluating convergence apply.

# MCMC Example: Modeling Storm Surge Extremes

## Data

```{julia}
#| output: true
#| echo: true
#| code-fold: true
#| code-overflow: wrap
#| fig-align: center
#| label: fig-surge-data
#| fig-cap: Annual maxima surge data from the San Francisco, CA tide gauge.

# load SF tide gauge data
# read in data and get annual maxima
function load_data(fname)
    date_format = DateFormat("yyyy-mm-dd HH:MM:SS")
    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data
    df = @chain fname begin
        CSV.read(DataFrame; header=false)
        rename("Column1" => "year", "Column2" => "month", "Column3" => "day", "Column4" => "hour", "Column5" => "gauge")
        # need to reformat the decimal date in the data file
        @transform :datetime = DateTime.(:year, :month, :day, :hour)
        # replace -99999 with missing
        @transform :gauge = ifelse.(abs.(:gauge) .>= 9999, missing, :gauge)
        select(:datetime, :gauge)
    end
    return df
end

dat = load_data("data/surge/h551.csv")

# detrend the data to remove the effects of sea-level rise and seasonal dynamics
ma_length = 366
ma_offset = Int(floor(ma_length/2))
moving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]
dat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))

# group data by year and compute the annual maxima
dat_ma = dropmissing(dat_ma) # drop missing data
dat_annmax = combine(dat_ma -> dat_ma[argmax(dat_ma.residual), :], groupby(DataFrames.transform(dat_ma, :datetime => x->year.(x)), :datetime_function))
delete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet
rename!(dat_annmax, :datetime_function => :Year)
select!(dat_annmax, [:Year, :residual])

# make plots
p1 = plot(
    dat_annmax.Year,
    dat_annmax.residual;
    xlabel="Year",
    ylabel="Annual Max Tide Level (mm)",
    label=false,
    marker=:circle,
    markersize=5,
    tickfontsize=16,
    guidefontsize=18
)
p2 = histogram(
    dat_annmax.residual,
    normalize=:pdf,
    orientation=:horizontal,
    label=:false,
    xlabel="PDF",
    xlims=(0, 0.006),
    ylabel="",
    yticks=[],
    xticks = [],
    tickfontsize=16,
    guidefontsize=18
)

l = @layout [a{0.7w} b{0.3w}]
plot(p1, p2; layout=l, link=:y, ylims=(1000, 1700), bottom_margin=5mm, left_margin=5mm)
plot!(size=(1000, 450))
```

## Probability Model (Annual Maxima)

\begin{gather*}
y_t \sim \text{GEV}(\mu, \sigma, \xi) \\
\mu \sim \mathcal{LogNormal}(7, 0.25) \\
\sigma \sim \mathcal{TN}(0, 100; 0, \infty) \\
\xi \sim \mathcal{N}(0, 0.1)
\end{gather*}

## Prior Predictive Check

```{julia}
#| output: true
#| echo: true
#| code-fold: true
#| code-overflow: wrap
#| fig-align: center
#| label: fig-surge-ppc
#| fig-cap: Prior predictive check for surge model.

# sample from priors
μ = rand(LogNormal(7, 0.25), 1000)
σ = rand(truncated(Normal(0, 100), lower=0), 1000)
ξ = rand(Normal(0, 0.1), 1000)
# simulate
# define return periods and cmopute return levels for parameters
return_periods = 2:100
return_levels = zeros(1_000, length(return_periods))
for i in 1:1_000
    return_levels[i, :] = quantile.(GeneralizedExtremeValue(μ[i], σ[i], ξ[i]), 1 .- (1 ./ return_periods))
end

plt_prior_1 = plot(; ylabel="Return Level (m)", xlabel="Return Period (yrs)", tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=10mm, left_margin=10mm, legend=:topleft)
for idx in 1:1_000
    label = idx == 1 ? "Prior" : false
    plot!(plt_prior_1, return_periods, return_levels[idx, :]; color=:black, alpha=0.1, label=label)
end
plt_prior_1
```

## Probabilistic Programming Languages

- Rely on more advanced methods (*e.g.* Hamiltonian Monte Carlo) to draw samples more efficiently.
- Use automatic differentiation to compute gradients.
- Syntax closely resembles statistical model specification.
- Examples:
  - [`Turing.jl`](https://turing.ml/dev/) in Julia
  - [`PyMC`](https://www.pymc.io/welcome.html) in Python
  - [`Stan`](https://mc-stan.org/), cross-language

## Turing Model Specification

```{julia}
#| output: true
#| echo: true
#| code-overflow: wrap
#| code-line-numbers: "|2-6|7-8"

@model function sf_surge(y)
    ## pick priors
    μ ~ LogNormal(7, 0.25) # location
    σ ~ truncated(Normal(0, 100); lower=0) # scale
    ξ ~ Normal(0, 0.1) # shape

    ## likelihood
    y .~ GeneralizedExtremeValue(μ, σ, ξ)
end
```

## Sampling with Turing

```{julia}
#| output: true
#| echo: true
#| code-overflow: wrap
#| code-line-numbers: "1-6|7"

surge_chain = let # variables defined in a let...end block are temporary
    model = sf_surge(dat_annmax.residual) # initialize model with data
    sampler = NUTS() # use the No-U-Turn Sampler; there are other options
    nsamples = 10_000
    sample(model, sampler, nsamples; drop_warmup=true)
end
summarystats(surge_chain)
```

## Sampling Visualization

```{julia}
#| output: true
#| echo: true
#| code-overflow: wrap
#| code-fold: true
#| label: fig-surge-visualization
#| fig-cap: Sampler visualization for surge chain

plot(surge_chain, size=(1200, 500), left_margin=5mm, bottom_margin=5mm)
```

## Optimizing with Turing

We can also use `Turing.jl` along with `Optim.jl` to get the MLE and MAP.

## MLE

```{julia}
#| echo: true
mle_surge = optimize(sf_surge(dat_annmax.residual), MLE())
coeftable(mle_surge)
```

## MAP

```{julia}
#| echo: true

map_surge = optimize(sf_surge(dat_annmax.residual), MAP())
coeftable(map_surge)
```

## Posterior Visualization

```{julia}
#| output: true
#| echo: true
#| code-overflow: wrap
#| code-fold: true
#| label: fig-surge-posterior
#| fig-cap: Posterior visualization for surge chain

p1 = histogram(surge_chain[:μ], label="Samples", normalize=:pdf, legend=:topleft, xlabel=L"μ", ylabel=L"p(μ|y)")
p2 = histogram(surge_chain[:σ], label="Samples", normalize=:pdf, legend=:topleft, xlabel=L"σ", ylabel=L"p(σ|y)")
p3 = histogram(surge_chain[:ξ], label="Samples", normalize=:pdf, legend=:topleft, xlabel=L"σ", ylabel=L"p(σ|y)")
p = plot(p1, p2, p3, tickfontsize=16, guidefontsize=18, legendfontsize=18, left_margin=10mm, bottom_margin=10mm, layout = @layout [a b c])
vline!(p, mean(surge_chain)[:, 2]', color=:purple, linewidth=3, label="Posterior Mean")
plot!(p, size=(1200, 450))
```

## Correlations

```{julia}
#| output: true
#| echo: true
#| code-overflow: wrap
#| code-fold: true
#| label: fig-surge-correlations
#| fig-cap: Posterior correlations

p1 = histogram2d(surge_chain[:μ], surge_chain[:σ], normalize=:pdf, legend=false, xlabel=L"μ", ylabel=L"σ")
p2 = histogram2d(surge_chain[:μ], surge_chain[:ξ], normalize=:pdf, legend=false, xlabel=L"μ", ylabel=L"ξ")
p3 = histogram2d(surge_chain[:σ], surge_chain[:ξ], normalize=:pdf, legend=false, xlabel=L"σ", ylabel=L"ξ")
p = plot(p1, p2, p3, tickfontsize=16, guidefontsize=18, left_margin=5mm, bottom_margin=5mm, layout = @layout [a b c])
plot!(p, size=(1200, 450))
```

## Posterior Predictive Checks

```{julia}
#| output: true
#| echo: true
#| code-overflow: wrap
#| code-fold: true
#| label: fig-surge-posterior-predictive
#| fig-cap: Posterior predictive checks

plt_rt = plot(; ylabel="Return Level (m)", xlabel="Return Period (yrs)", tickfontsize=16, legendfontsize=18, guidefontsize=18, bottom_margin=10mm, left_margin=10mm, legend=:topleft)
for idx in 1:1000
    μ = surge_chain[:μ][idx]
    σ = surge_chain[:σ][idx]
    ξ = surge_chain[:ξ][idx]
    return_levels[idx, :] = quantile.(GeneralizedExtremeValue(μ, σ, ξ), 1 .- (1 ./ return_periods))
    label = idx == 1 ? "Posterior" : false
    plot!(plt_rt, return_periods, return_levels[idx, :]; color=:black, alpha=0.05, label=label, linewidth=0.5)
end
# plot return level quantiles
rl_q = mapslices(col -> quantile(col, [0.025, 0.5, 0.975]), return_levels, dims=1)
plot!(plt_rt, return_periods, rl_q[[1,3], :]', color=:green, linewidth=2, label="95% CI")
plot!(plt_rt, return_periods, rl_q[2, :], color=:red, linewidth=2, label="Posterior Median")
# plot data
scatter!(plt_rt, return_periods, quantile(dat_annmax.residual, 1 .- (1 ./ return_periods)), label="Data", color=:black)
plot!(plt_rt, size=(1200, 500))
plt_rt
```

## Multiple Chains

```{julia}
#| output: true
#| echo: true
#| code-overflow: wrap

surge_chain = let # variables defined in a let...end block are temporary
    model = sf_surge(dat_annmax.residual) # initialize model with data
    sampler = NUTS() # use the No-U-Turn Sampler; there are other options
    nsamples = 10_000
    nchains = 4
    sample(model, sampler, MCMCThreads(), nsamples, nchains; drop_warmup=true)
end
gelmandiag(surge_chain)
```

## Plotting Multiple Chains

```{julia}
#| output: true
#| echo: true
#| code-overflow: wrap
#| code-fold: true
#| label: fig-surge-visualization-multiple
#| fig-cap: Sampler visualization for multiple surge chains

plot(surge_chain)
plot!(size=(1200, 500))
```

# Key Points and Upcoming Schedule

## Key Points (Convergence)

- Must rely on "accumulation of evidence" from heuristics for determination about convergence to stationary distribution.
- Transient portion of chain: Meh. Some people worry about this too much. Discard or run the chain longer.
- Parallelizing solves few problems, but running multiple chains can be useful for diagnostics.

## Next Classes

**Monday**: MCMC Lab (No exercises these weeks)

**Next Wednesday**: Literature Presentations (email slides by 9pm Tuesday night).

## Assessments

- **Homework 3**: Due 3/22

# References

## References