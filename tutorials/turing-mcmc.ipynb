{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Markov Chain Monte Carlo With Turing\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial will give some examples of using `Turing.jl` and Markov\n",
        "Chain Monte Carlo to sample from posterior distributions.\n",
        "\n",
        "## Setup"
      ],
      "id": "637d4ee9-715b-4fc5-8a4f-4aff47235ee0"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "using Turing\n",
        "using Distributions\n",
        "using Plots\n",
        "default(fmt = :png) # the tide gauge data is long, this keeps images a manageable size\n",
        "using LaTeXStrings\n",
        "using StatsPlots\n",
        "using Measures\n",
        "using StatsBase\n",
        "using Optim\n",
        "using Random\n",
        "using DataFrames\n",
        "using DataFramesMeta\n",
        "using Dates\n",
        "using CSV"
      ],
      "id": "4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As this tutorial involves random number generation, we will set a random\n",
        "seed to ensure reproducibility."
      ],
      "id": "00653195-d016-4cde-b90f-acf11640a9f2"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "Random.seed!(1);"
      ],
      "id": "6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting A Linear Regression Model\n",
        "\n",
        "Let’s start with a simple example: fitting a linear regression model to\n",
        "simulated data.\n",
        "\n",
        "> **Positive Control Tests**\n",
        ">\n",
        "> Simulating data with a known data-generating process and then trying\n",
        "> to obtain the parameters for that process is an important step in any\n",
        "> workflow.\n",
        "\n",
        "### Simulating Data\n",
        "\n",
        "The data-generating process for this example will be: $$\n",
        "\\begin{gather}\n",
        "y = 5 + 2x + \\varepsilon \\\\\n",
        "\\varepsilon \\sim \\text{Normal}(0, 3),\n",
        "\\end{gather}\n",
        "$$ where $\\varepsilon$ is so-called “white noise”, which adds\n",
        "stochasticity to the data set. The generated dataset is shown in\n",
        "<a href=\"#fig-scatter-regression\" class=\"quarto-xref\">Figure 1</a>."
      ],
      "id": "659e5290-faa9-4761-82bb-5d63b67d8430"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [],
      "id": "cell-fig-scatter-regression"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Specification\n",
        "\n",
        "The statistical model for a standard linear regression problem is $$\n",
        "\\begin{gather}\n",
        "y = a + bx + \\varepsilon \\\\\n",
        "\\varepsilon \\sim \\text{Normal}(0, \\sigma).\n",
        "\\end{gather}\n",
        "$$\n",
        "\n",
        "Rearranging, we can rewrite the likelihood function as:\n",
        "$$y \\sim \\text{Normal}(\\mu, \\sigma),$$ where $\\mu = a + bx$. This means\n",
        "that we have three parameters to fit: $a$, $b$, and $\\sigma^2$.\n",
        "\n",
        "Next, we need to select priors on our parameters. We’ll use relatively\n",
        "generic distributions to avoid using the information we have (since we\n",
        "generated the data ourselves), but in practice, we’d want to use any\n",
        "relevant information that we had from our knowledge of the problem.\n",
        "Let’s use relatively diffuse normal distributions for the trend\n",
        "parameters $a$ and $b$ and a half-normal distribution (a normal\n",
        "distribution truncated at 0, to only allow positive values) for the\n",
        "variance $\\sigma^2$, as recommended by Gelman (2006).\n",
        "\n",
        "$$\n",
        "\\begin{gather}\n",
        "a \\sim \\text{Normal(0, 10)} \\\\\n",
        "b \\sim \\text{Normal(0, 10)} \\\\\n",
        "\\sigma \\sim \\text{Half-Normal}(0, 25)\n",
        "\\end{gather}\n",
        "$$\n",
        "\n",
        "### Using Turing\n",
        "\n",
        "#### Coding the Model\n",
        "\n",
        "`Turing.jl` uses the `@model` macro to specify the model function. We’ll\n",
        "follow the setup in the [Turing\n",
        "documentation](https://turinglang.org/dev/tutorials/05-linear-regression).\n",
        "\n",
        "To specify distributions on parameters (and the data, which can be\n",
        "thought of as uncertain parameters in Bayesian statistics), use a tilde\n",
        "`~`, and use equals `=` for transformations (which we don’t have in this\n",
        "case)."
      ],
      "id": "94a3a70d-41df-4c83-9d70-5dfd823b1096"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "linear_regression (generic function with 2 methods)"
            ]
          }
        }
      ],
      "source": [
        "\n",
        "@model function linear_regression(x, y)\n",
        "    # set priors\n",
        "    σ ~ truncated(Normal(0, 25); lower=0)\n",
        "    a ~ Normal(0, 10)\n",
        "    b ~ Normal(0, 10)\n",
        "\n",
        "    # compute the likelihood\n",
        "    for i = 1:length(y)\n",
        "        # compute the mean value for the data point\n",
        "        μ = a + b * x[i]\n",
        "        y[i] ~ Normal(μ, σ)\n",
        "    end\n",
        "end"
      ],
      "id": "10"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Fitting The Model\n",
        "\n",
        "Now we can call the sampler to draw from the posterior. We’ll use the\n",
        "[No-U-Turn\n",
        "sampler](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo#No_U-Turn_Sampler)\n",
        "(Hoffman & Gelman, 2014), which is a Hamiltonian Monte Carlo algorithm\n",
        "(a different category of MCMC sampler than the Metropolis-Hastings\n",
        "algorithm discussed in class). We’ll also use 4 chains so we can test\n",
        "that the chains are well-mixed, and each chain will be run for 5,000\n",
        "iterations[1]\n",
        "\n",
        "[1] Hamiltonian Monte Carlo samplers often need to be run for fewer\n",
        "iterations than Metropolis-Hastings samplers, as the exploratory step\n",
        "uses information about the gradient of the statistical model, versus the\n",
        "random walk of Metropolis-Hastings. The disadvantage is that this\n",
        "gradient information must be available, which is not always the case for\n",
        "external simulation models. Simulation models coded in Julia can usually\n",
        "be automatically differentiated by Turing’s tools, however."
      ],
      "id": "d27eacbc-a60f-4d1b-9434-7d00385f72d1"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n",
            "└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/FSyVk/src/sample.jl:382\n",
            "Sampling (1 threads)   0%|                              |  ETA: N/A\n",
            "┌ Info: Found initial step size\n",
            "└   ϵ = 0.0001953125\n",
            "┌ Info: Found initial step size\n",
            "└   ϵ = 0.0125\n",
            "Sampling (1 threads)  25%|███████▌                      |  ETA: 0:00:17\n",
            "Sampling (1 threads)  50%|███████████████               |  ETA: 0:00:06\n",
            "┌ Info: Found initial step size\n",
            "└   ϵ = 0.0125\n",
            "┌ Info: Found initial step size\n",
            "└   ϵ = 0.0015625\n",
            "Sampling (1 threads)  75%|██████████████████████▌       |  ETA: 0:00:02\n",
            "Sampling (1 threads) 100%|██████████████████████████████| Time: 0:00:07\n",
            "Sampling (1 threads) 100%|██████████████████████████████| Time: 0:00:07\n",
            "chain = MCMC chain (5000×15×4 Array{Float64, 3})"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Chains MCMC chain (5000×15×4 Array{Float64, 3}):\n",
              "\n",
              "Iterations        = 1001:1:6000\n",
              "Number of chains  = 4\n",
              "Samples per chain = 5000\n",
              "Wall duration     = 5.35 seconds\n",
              "Compute duration  = 4.45 seconds\n",
              "parameters        = σ, a, b\n",
              "internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n",
              "\n",
              "Summary Statistics\n",
              "  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n",
              "      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n",
              "\n",
              "           σ    4.3453    0.7874    0.0081   9570.0574   9218.8306    1.0001   ⋯\n",
              "           a    1.7125    1.9475    0.0218   8067.8069   9568.8777    1.0012   ⋯\n",
              "           b    2.3534    0.1726    0.0019   8279.1454   9423.1745    1.0012   ⋯\n",
              "                                                                1 column omitted\n",
              "\n",
              "Quantiles\n",
              "  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n",
              "      Symbol   Float64   Float64   Float64   Float64   Float64\n",
              "\n",
              "           σ    3.1211    3.7870    4.2366    4.7887    6.2171\n",
              "           a   -2.1004    0.4181    1.6793    2.9941    5.6166\n",
              "           b    2.0116    2.2403    2.3557    2.4671    2.6891"
            ]
          }
        }
      ],
      "source": [
        "# set up the sampler\n",
        "model = linear_regression(x, y)\n",
        "n_chains = 4\n",
        "n_per_chain = 5000\n",
        "chain = sample(model, NUTS(), MCMCThreads(), n_per_chain, n_chains, drop_warmup=true)\n",
        "@show chain"
      ],
      "id": "12"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How can we interpret the output? The first parts of the summary\n",
        "statistics are straightforward: we get the mean, standard deviation, and\n",
        "Monte Carlo standard error (`mcse`) of each parameter. We also get\n",
        "information about the effective sample size (ESS)[1] and $\\hat{R}$,\n",
        "which measures the ratio of within-chain variance and across-chain\n",
        "variance as a check for convergence[2].\n",
        "\n",
        "In this case, we can see that we were generally able to recover the\n",
        "“true” data-generating values of $\\sigma = 4$ and $b = 2$, but $a$ is\n",
        "slightly off (the mean is 3, rather than the data-generating value of\n",
        "5). In fact, there is substantial uncertainty about $a$, with a 95%\n",
        "credible interval of $(3.1, 11.4)$ (compared to $(1.4, 2.2)$ for $b$).\n",
        "This isn’t surprising: given the variance of the noise $\\sigma^2$, there\n",
        "are many different intercepts which could fit within that spread.\n",
        "\n",
        "Let’s now plot the chains for visual inspection.\n",
        "\n",
        "[1] The ESS reflects the efficiency of the sampler: this is an estimate\n",
        "of the equivalent number of independent samples; the more correlated the\n",
        "samples, the lower the ESS.\n",
        "\n",
        "[2] The closer $\\hat{R}$ is to 1, the better."
      ],
      "id": "304fb3df-680c-4e26-9a52-0873142e42f9"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot(chain)"
      ],
      "id": "cell-fig-chains-regression"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see from\n",
        "<a href=\"#fig-chains-regression\" class=\"quarto-xref\">Figure 2</a> that\n",
        "our chains mixed well and seem to have converged to similar\n",
        "distributions! The traceplots have a “hairy caterpiller” appearance,\n",
        "suggesting relatively little autocorrelation. We can also see how much\n",
        "more uncertainty there is with the intercept $a$, while the slope $b$ is\n",
        "much more constrained.\n",
        "\n",
        "Another interesting comparison we can make is with the\n",
        "maximum-likelihood estimate (MLE), which we can obtain through\n",
        "optimization."
      ],
      "id": "00c3efe8-90f9-4773-83e6-1bc0ab1f3bf1"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "mle_model = linear_regression(x, y)\n",
        "mle = optimize(mle_model, MLE())\n",
        "coef(mle)"
      ],
      "id": "16"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We could also get the maximum *a posteriori* (MAP) estimate, which\n",
        "includes the prior density, by replacing `MLE()` with `MAP()`.\n",
        "\n",
        "### Model Diagnostics and Posterior Predictive Checks\n",
        "\n",
        "One advantage of the Bayesian modeling approach here is that we have\n",
        "access to a *generative model*, or a model which we can use to generate\n",
        "datasets. This means that we can now use Monte Carlo simulation,\n",
        "sampling from our posteriors, to look at how uncertainty in the\n",
        "parameter estimates propagates through the model. Let’s write a function\n",
        "which gets samples from the MCMC chains and generates datasets."
      ],
      "id": "dfa1bfd9-8112-490e-b27f-85a9fc8b1117"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "mc_predict_regression (generic function with 1 method)"
            ]
          }
        }
      ],
      "source": [
        "function mc_predict_regression(x, chain)\n",
        "    # get the posterior samples\n",
        "    a = Array(group(chain, :a))\n",
        "    b = Array(group(chain, :b))\n",
        "    σ = Array(group(chain, :σ))\n",
        "\n",
        "    # loop and generate alternative realizations\n",
        "    μ = a' .+ x * b'\n",
        "    y = zeros((length(x), length(a)))\n",
        "    for i = 1:length(a)\n",
        "        y[:, i] = rand.(Normal.(μ[:, i], σ[i]))\n",
        "    end\n",
        "    return y\n",
        "end"
      ],
      "id": "18"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can generate a predictive interval and median and compare to the\n",
        "data."
      ],
      "id": "eb2d0df2-fffd-4459-abcf-97e713491a5a"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "21×20000 Matrix{Float64}:\n",
              "  2.57253   8.12422   6.05525  -0.274393  …   2.77879   2.31592   4.96407\n",
              " -1.73707   6.42483  -1.51082   2.78258       8.21127  10.6669    5.41855\n",
              " 10.6507    3.40205   6.88069   8.17338       5.0066    4.10358   1.03898\n",
              "  8.38618  12.1977    6.51741  11.2268       12.4627    9.13304  12.5438\n",
              " 10.4858    6.25163   9.79537  18.7078       16.2906   22.4295    8.02768\n",
              "  7.97038  10.3047    5.25073  18.7412    …  11.0386   18.171    12.5637\n",
              " 25.8893   15.4041    3.72335  20.372        15.8956   15.5514   14.0513\n",
              " 20.3258   22.4881   10.3922   18.5969       22.193    17.1508   23.8562\n",
              " 26.1442   19.585    17.2116   20.0882       28.5813   26.9506   17.6067\n",
              " 15.9402   23.8775   18.5295   19.338        22.8047   23.8674   24.0946\n",
              "  ⋮                                       ⋱                      \n",
              " 26.323    34.0608   18.7044   24.1611       33.1358   32.6087   25.1177\n",
              " 32.276    38.1796   28.7813   32.0347       30.4168   33.4686   32.3732\n",
              " 33.9976   30.2147   35.8013   34.2581       42.2725   36.9098   30.6196\n",
              " 36.0762   39.7378   36.319    35.9592    …  35.9193   38.4562   39.5119\n",
              " 37.1296   45.6465   28.8537   39.1454       37.5871   31.5874   38.5199\n",
              " 49.3035   39.6972   33.6027   38.9979       40.9558   34.8524   45.6937\n",
              " 43.4395   45.36     48.6332   46.0882       49.9398   44.0458   45.3838\n",
              " 42.9935   45.0904   38.0495   50.6099       52.7832   47.3516   42.5187\n",
              " 43.2621   48.83     57.0209   49.6115    …  52.5978   52.9936   53.2847"
            ]
          }
        }
      ],
      "source": [
        "x_pred = 0:20\n",
        "y_pred = mc_predict_regression(x_pred, chain)"
      ],
      "id": "20"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notice the dimension of `y_pred`: we have 20,000 columns, because we\n",
        "have 4 chains with 5,000 samples each. If we had wanted to subsample\n",
        "(which might be necessary if we had hundreds of thousands or millions of\n",
        "samples), we could have done that within `mc_linear_regression` before\n",
        "simulation."
      ],
      "id": "c36da218-a238-4b70-b082-1486549b80da"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get the boundaries for the 95% prediction interval and the median\n",
        "y_ci_low = quantile.(eachrow(y_pred), 0.025)\n",
        "y_ci_hi = quantile.(eachrow(y_pred), 0.975)\n",
        "y_med = quantile.(eachrow(y_pred), 0.5)"
      ],
      "id": "22"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let’s plot the prediction interval and median, and compare to the\n",
        "original data."
      ],
      "id": "419f3aea-1139-4541-92fb-2576b1798e77"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot prediction interval\n",
        "plot(x_pred, y_ci_low, fillrange=y_ci_hi, xlabel=L\"$x$\", ylabel=L\"$y$\", fillalpha=0.3, fillcolor=:blue, label=\"95% Prediction Interval\", legend=:topleft, linealpha=0)\n",
        "plot!(x_pred, y_med, color=:blue, label=\"Prediction Median\")\n",
        "scatter!(x, y, color=:red, label=\"Data\")"
      ],
      "id": "cell-fig-prediction-regression"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From\n",
        "<a href=\"#fig-prediction-regression\" class=\"quarto-xref\">Figure 3</a>,\n",
        "it looks like our model might be slightly under-confident, as with 20\n",
        "data points, we would expect 5% of them (or 1 data point) to be outside\n",
        "the 95% prediction interval. It’s hard to tell with only 20 data points,\n",
        "though! We could resolve this by tightening our priors, but this depends\n",
        "on how much information we used to specify them in the first place. The\n",
        "goal shouldn’t be to hit a specific level of uncertainty, but if there\n",
        "is a sound reason to tighten the priors, we could do so.\n",
        "\n",
        "Now let’s look at the residuals from the posterior median and the data.\n",
        "The partial autocorrelations plotted in\n",
        "<a href=\"#fig-residuals-regression\" class=\"quarto-xref\">Figure 4</a> are\n",
        "not fully convincing, as there are large autocorrelation coefficients\n",
        "with long lags, but the dataset is quite small, so it’s hard to draw\n",
        "strong conclusions. We won’t go further down this rabbit hole as we know\n",
        "our data-generating process involved independent noise, but for a real\n",
        "dataset, we might want to try a model specification with autocorrelated\n",
        "errors to compare."
      ],
      "id": "a3bf91f9-17ea-42be-9be2-d96eb909b374"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate the median predictions and residuals\n",
        "y_pred_data = mc_predict_regression(x, chain)\n",
        "y_med_data = quantile.(eachrow(y_pred_data), 0.5)\n",
        "residuals = y_med_data .- y\n",
        "\n",
        "# plot the residuals and a line to show the zero\n",
        "plot(pacf(residuals, 1:4), line=:stem, marker=:circle, legend=:false, grid=:false, linewidth=2, xlabel=\"Lag\", ylabel=\"Partial Autocorrelation\", markersize=8, tickfontsize=14, guidefontsize=16, legendfontsize=16)\n",
        "hline!([0], linestyle=:dot, color=:red)"
      ],
      "id": "cell-fig-residuals-regression"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting Extreme Value Models to Tide Gauge Data\n",
        "\n",
        "Let’s now look at an example of fitting an extreme value distribution\n",
        "(namely, a [generalized extreme value\n",
        "distribution](https://en.wikipedia.org/wiki/Generalized_extreme_value_distribution),\n",
        "or GEV) to tide gauge data. GEV distributions have three parameters:\n",
        "\n",
        "-   $\\mu$, the *location* parameter, which reflects the positioning of\n",
        "    the bulk of the GEV distribution;\n",
        "-   $\\sigma$, the *scale* parameter, which reflects the width of the\n",
        "    bulk;\n",
        "-   $\\xi$, the *shape* parameter, which reflects the thickness and\n",
        "    boundedness of the tail.\n",
        "\n",
        "The shape parameter $\\xi$ is often of interest, as there are three\n",
        "classes of GEV distributions corresponding to different signs:\n",
        "\n",
        "-   $\\xi < 0$ means that the distribution is bounded;\n",
        "-   $\\xi = 0$ means that the distribution has a thinner tail, so the\n",
        "    “extreme extremes” are less likely;\n",
        "-   $\\xi > 0$ means that the distribution has a thicker tail.\n",
        "\n",
        "### Load Data\n",
        "\n",
        "First, let’s load the data. We’ll use [data from the University of\n",
        "Hawaii Sea Level Center](https://uhslc.soest.hawaii.edu/datainfo/)\n",
        "(Caldwell et al., 2015) for San Francisco, from 1897-2013. If you don’t\n",
        "have this data and are working with the notebook, download it\n",
        "[here](https://uhslc.soest.hawaii.edu/data/csv/rqds/hourly/h551a.csv).\n",
        "We’ll assume it’s in a `data/` subdirectory, but change the path as\n",
        "needed.\n",
        "\n",
        "The dataset consists of dates and hours and the tide-gauge measurement,\n",
        "in mm. We’ll load the dataset into a `DataFrame`."
      ],
      "id": "1866490a-3ca7-488d-848f-1dde9f4d49a6"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "load_data (generic function with 1 method)"
            ]
          }
        }
      ],
      "source": [
        "function load_data(fname)\n",
        "    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n",
        "    df = @chain fname begin\n",
        "        CSV.File(; delim=',', header=false)\n",
        "        DataFrame\n",
        "        rename(\"Column1\" => \"year\",\n",
        "                \"Column2\" => \"month\",\n",
        "                \"Column3\" => \"day\",\n",
        "                \"Column4\" => \"hour\",\n",
        "                \"Column5\" => \"gauge\")\n",
        "        # need to reformat the decimal date in the data file\n",
        "        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n",
        "        # replace -99999 with missing\n",
        "        @transform :gauge = ifelse.(abs.(:gauge) .>= 9999, missing, :gauge)\n",
        "        select(:datetime, :gauge)\n",
        "    end\n",
        "    return df\n",
        "end"
      ],
      "id": "28"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "dat = load_data(\"data/h551a.csv\")\n",
        "first(dat, 6)"
      ],
      "id": "8b26af7f-b679-4049-a876-b65ab55892dc"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "@df dat plot(:datetime, :gauge, label=\"Observations\", bottom_margin=9mm)\n",
        "xaxis!(\"Date\", xrot=30)\n",
        "yaxis!(\"Mean Water Level\")"
      ],
      "id": "cell-fig-raw-data"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we need to detrend the data to remove the impacts of sea-level\n",
        "rise. We do this by removing a one-year moving average, centered on the\n",
        "data point, per the recommendation of Arns et al. (2013)."
      ],
      "id": "8de0a6f7-8e2c-46ce-9cc2-48b25e414e30"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate the moving average and subtract it off\n",
        "ma_length = 366\n",
        "ma_offset = Int(floor(ma_length/2))\n",
        "moving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\n",
        "dat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n",
        "\n",
        "# plot\n",
        "@df dat_ma plot(:datetime, :residual, label=\"Detrended Observations\", bottom_margin=9mm)\n",
        "xaxis!(\"Date\", xrot=30)\n",
        "yaxis!(\"Mean Water Level\")"
      ],
      "id": "cell-fig-data-detrend"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The last step in preparing the data is to find the annual maxima. We can\n",
        "do this using the `groupby`, `transform`, and `combine` functions from\n",
        "`DataFrames.jl`, as below."
      ],
      "id": "613b7d4d-c36e-4c47-abaf-6a8e2019857a"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate the annual maxima\n",
        "dat_ma = dropmissing(dat_ma)\n",
        "dat_annmax = combine(dat_ma -> dat_ma[argmax(dat_ma.residual), :],\n",
        "                groupby(DataFrames.transform(dat_ma, :datetime => x->year.(x)), :datetime_function))\n",
        "delete!(dat_annmax, nrow(dat_annmax))\n",
        "\n",
        "# make a histogram of the maxima to see the distribution\n",
        "histogram(dat_annmax.residual, label=false)\n",
        "ylabel!(\"Count\")\n",
        "xlabel!(\"Mean Water Level (mm)\")"
      ],
      "id": "cell-fig-annmax-histogram"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fit The Model"
      ],
      "id": "ba3c4523-120b-45fa-94d6-48ce57db2bd0"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n",
            "└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/FSyVk/src/sample.jl:382\n",
            "Sampling (1 threads)   0%|                              |  ETA: N/A\n",
            "┌ Info: Found initial step size\n",
            "└   ϵ = 0.00625\n",
            "┌ Info: Found initial step size\n",
            "└   ϵ = 0.00625\n",
            "Sampling (1 threads)  25%|███████▌                      |  ETA: 0:00:08\n",
            "Sampling (1 threads)  50%|███████████████               |  ETA: 0:00:03\n",
            "┌ Info: Found initial step size\n",
            "└   ϵ = 0.05\n",
            "┌ Info: Found initial step size\n",
            "└   ϵ = 0.0125\n",
            "Sampling (1 threads)  75%|██████████████████████▌       |  ETA: 0:00:01\n",
            "Sampling (1 threads) 100%|██████████████████████████████| Time: 0:00:04\n",
            "Sampling (1 threads) 100%|██████████████████████████████| Time: 0:00:04\n",
            "gev_chain = MCMC chain (5000×15×4 Array{Float64, 3})"
          ]
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {
            "text/plain": [
              "Chains MCMC chain (5000×15×4 Array{Float64, 3}):\n",
              "\n",
              "Iterations        = 1001:1:6000\n",
              "Number of chains  = 4\n",
              "Samples per chain = 5000\n",
              "Wall duration     = 4.31 seconds\n",
              "Compute duration  = 3.94 seconds\n",
              "parameters        = μ, σ, ξ\n",
              "internals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n",
              "\n",
              "Summary Statistics\n",
              "  parameters        mean       std      mcse     ess_bulk     ess_tail      rh ⋯\n",
              "      Symbol     Float64   Float64   Float64      Float64      Float64   Float ⋯\n",
              "\n",
              "           μ   1257.7315    5.6957    0.0506   12696.2628   13400.5284    1.00 ⋯\n",
              "           σ     57.1542    4.2702    0.0380   12773.6781   13549.3731    1.00 ⋯\n",
              "           ξ      0.0295    0.0629    0.0006   12648.9948   12406.1796    1.00 ⋯\n",
              "                                                               2 columns omitted\n",
              "\n",
              "Quantiles\n",
              "  parameters        2.5%       25.0%       50.0%       75.0%       97.5%\n",
              "      Symbol     Float64     Float64     Float64     Float64     Float64\n",
              "\n",
              "           μ   1246.7931   1253.8659   1257.6699   1261.4807   1269.1167\n",
              "           σ     49.4276     54.2105     56.9600     59.8460     66.0719\n",
              "           ξ     -0.0823     -0.0146      0.0254      0.0699      0.1627"
            ]
          }
        }
      ],
      "source": [
        "@model function gev_annmax(y)               \n",
        "    μ ~ Normal(1000, 100)\n",
        "    σ ~ truncated(Normal(0, 100); lower=0)\n",
        "    ξ ~ Normal(0, 0.5)\n",
        "\n",
        "    y ~ GeneralizedExtremeValue(μ, σ, ξ)\n",
        "end\n",
        "\n",
        "gev_model = gev_annmax(dat_annmax.residual)\n",
        "n_chains = 4\n",
        "n_per_chain = 5000\n",
        "gev_chain = sample(gev_model, NUTS(), MCMCThreads(), n_per_chain, n_chains; drop_warmup=true)\n",
        "@show gev_chain"
      ],
      "id": "38"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot(gev_chain)"
      ],
      "id": "cell-fig-gev-traceplot"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From <a href=\"#fig-gev-traceplot\" class=\"quarto-xref\">Figure 8</a>, it\n",
        "looks like all of the chains have converged to the same distribution;\n",
        "the Gelman-Rubin diagnostic is also close to 1 for all parameters. Next,\n",
        "we can look at a corner plot to see how the parameters are correlated."
      ],
      "id": "97e7d634-092b-4b98-bf47-cdd952e9f2d0"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "corner(gev_chain)"
      ],
      "id": "cell-fig-gev-corner"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"#fig-gev-corner\" class=\"quarto-xref\">Figure 9</a> suggests that\n",
        "the location and scale parameters $\\mu$ and $\\sigma$ are positively\n",
        "correlated. This makes some intuitive sense, as increasing the location\n",
        "parameter shifts the bulk of the distribution in a positive direction,\n",
        "and the increasing scale parameter then increases the likelihood of\n",
        "lower values. However, if these parameters are increased, the shape\n",
        "parameter $\\xi$ decreases, as the tail of the GEV does not need to be as\n",
        "thick due to the increased proximity of outliers to the bulk.\n",
        "\n",
        "Arns, A., Wahl, T., Haigh, I. D., Jensen, J., & Pattiaratchi, C. (2013).\n",
        "<span class=\"nocase\">Estimating extreme water level probabilities: A\n",
        "comparison of the direct methods and recommendations for best\n",
        "practise</span>. *Coast. Eng.*, *81*, 51–66.\n",
        "<https://doi.org/10.1016/j.coastaleng.2013.07.003>\n",
        "\n",
        "Caldwell, P. C., Merrifield, M. A., & Thompson, P. R. (2015). Sea level\n",
        "measured by tide gauges from global oceans — the joint archive for sea\n",
        "level holdings (NCEI accession 0019568). NOAA National Centers for\n",
        "Environmental Information (NCEI). <https://doi.org/10.7289/V5V40S7W>\n",
        "\n",
        "Gelman, A. (2006). Prior distributions for variance parameters in\n",
        "hierarchical models (comment on article by Browne and Draper). *Bayesian\n",
        "Anal.*, *1*(3), 515–533. <https://doi.org/10.1214/06-BA117A>\n",
        "\n",
        "Hoffman, M. D., & Gelman, A. (2014). The No-U-Turn sampler: Adaptively\n",
        "setting path lengths in Hamiltonian Monte Carlo. *J. Mach. Learn. Res.*,\n",
        "*15*(47), 1593–1623."
      ],
      "id": "2a80c82d-9d73-41bf-b8ec-034264101928"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernel_info": {
      "name": "julia"
    },
    "kernelspec": {
      "name": "julia",
      "display_name": "Julia",
      "language": "julia"
    },
    "language_info": {
      "name": "julia",
      "codemirror_mode": "julia",
      "version": "1.10.4"
    }
  }
}