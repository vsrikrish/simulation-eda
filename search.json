[
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Class Schedule",
    "section": "",
    "text": "This page contains a schedule of the topics, content, and assignments for the semester. Note that this schedule will be updated as necessary the semester progresses, with all changes documented here.\nInstructions to save the slides as PDFs can be found here. The key is to hit E when the slides are open in your browser (ideally Chrome, but it may work in others) to toggle into PDF print mode.\nReadings can be accessed using the link (when needed, through the Cornell library with a Cornell login) or on Canvas.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\nDate\nTopic\nSlides\nReading\nHomework\n\n\n\n\n\n1\n1/22\nClass Introduction\n\n\n\n\n\n\n2\n1/27\nNull Hypothesis Testing and Statistical Significance\n\nLloyd & Oreskes (2018)\n\n\n\n\n\n1/29\nProbability Fundamentals\n\n\n\n\n\n\n3\n2/3\nProbability Models for Data\n\nShmueli (2010)\n\n\n\n\n\n2/5\nTime Series\n\n\n\n\n\n\n4\n2/10\nBayesian Statistics\n\n\n\n\n\n\n\n2/12\nMarkov Chain Monte Carlo\n\n\n\n\n\n\n5\n2/17\nFebruary Break\n\n\n\n\n\n\n\n2/19\n\n\n\n\n\n\n\n6\n2/24\n\n\n\n\n\n\n\n\n2/26\n\n\n\n\n\n\n\n7\n3/3\n\n\n\n\n\n\n\n\n3/5\n\n\n\n\n\n\n\n8\n3/10\n\n\n\n\n\n\n\n\n3/12\n\n\n\n\n\n\n\n9\n3/17\n\n\n\n\n\n\n\n\n3/19\n\n\n\n\n\n\n\n10\n3/24\n\n\n\n\n\n\n\n\n3/26\n\n\n\n\n\n\n\n\n3/31\nSpring Break\n\n\n\n\n\n\n\n4/2\nSpring Break\n\n\n\n\n\n\n11\n4/7\n\n\n\n\n\n\n\n\n4/9\n\n\n\n\n\n\n\n12\n4/14\n\n\n\n\n\n\n\n\n4/16\n\n\n\n\n\n\n\n13\n4/21\n\n\n\n\n\n\n\n\n4/23\n\n\n\n\n\n\n\n14\n4/28\n\n\n\n\n\n\n\n\n4/30\n\n\n\n\n\n\n\n15\n5/5",
    "crumbs": [
      "Course Information",
      "Class Schedule"
    ]
  },
  {
    "objectID": "lit_critique.html",
    "href": "lit_critique.html",
    "title": "Literature Critique Instructions",
    "section": "",
    "text": "For the literature critique, you should pick a paper of your choice involving some aspect of data analysis and critically evaluate its statistical and scientific choices. Your critique should discuss how the statistical, data, and modeling choices support (or don’t) the scientific conclusions of the paper and provide ideas for how these could be strengthened.\nYou should submit a short written evaluation (2-3 pages, not including figures or references) on 5/5.",
    "crumbs": [
      "Literature Critique Instructions"
    ]
  },
  {
    "objectID": "tutorials/julia-plots.html",
    "href": "tutorials/julia-plots.html",
    "title": "Tutorial: Making Plots with Julia",
    "section": "",
    "text": "This tutorial will give some examples of plotting and plotting features in Julia, as well as providing references to some relevant resources. The main plotting library is Plots.jl, but there are some others that provide useful features.",
    "crumbs": [
      "Julia Tutorials",
      "Making Plots"
    ]
  },
  {
    "objectID": "tutorials/julia-plots.html#overview",
    "href": "tutorials/julia-plots.html#overview",
    "title": "Tutorial: Making Plots with Julia",
    "section": "",
    "text": "This tutorial will give some examples of plotting and plotting features in Julia, as well as providing references to some relevant resources. The main plotting library is Plots.jl, but there are some others that provide useful features.",
    "crumbs": [
      "Julia Tutorials",
      "Making Plots"
    ]
  },
  {
    "objectID": "tutorials/julia-plots.html#some-resources",
    "href": "tutorials/julia-plots.html#some-resources",
    "title": "Tutorial: Making Plots with Julia",
    "section": "Some Resources",
    "text": "Some Resources\n\nPlots.jl useful tips\nPlots.jl examples\nPlot attributes\nAxis attributes\nColor names",
    "crumbs": [
      "Julia Tutorials",
      "Making Plots"
    ]
  },
  {
    "objectID": "tutorials/julia-plots.html#demos",
    "href": "tutorials/julia-plots.html#demos",
    "title": "Tutorial: Making Plots with Julia",
    "section": "Demos",
    "text": "Demos\n\nusing Plots\nusing Random\nRandom.seed!(1);\n\n\nLine Plots\nTo generate a basic line plot, use plot.\n\ny = rand(5)\nplot(y, label=\"original data\", legend=:topright)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere’s a lot of customization here that can occur, a lot of which is discussed in the docs or can be found with some Googling.\n\n\nAdding Plot Elements\nNow we can add some other lines and point markers.\n\ny2 = rand(5)\ny3 = rand(5)\nplot!(y2, label=\"new data\")\nscatter!(y3, label=\"even more data\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember that an exclamation mark (!) at the end of a function name means that function modifies an object in-place, so plot! and scatter! modify the current plotting object, they don’t create a new plot.\n\n\nRemoving Plot Elements\nSometimes we want to remove legends, axes, grid lines, and ticks.\n\nplot!(legend=false, axis=false, grid=false, ticks=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nAspect Ratio\nIf we want to have a square aspect ratio, use ratio = 1.\n\nv = rand(5)\nplot(v, ratio=1, legend=false)\nscatter!(v)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeatmaps\nA heatmap is effectively a plotted matrix with colors chosen according to the values. Use clim to specify a fixed range for the color limits.\n\nA = rand(10, 10)\nheatmap(A, clim=(0, 1), ratio=1, legend=false, axis=false, ticks=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\nM = [ 0 1 0; 0 0 0; 1 0 0]\nwhiteblack = [RGBA(1,1,1,0), RGB(0,0,0)]\nheatmap(c=whiteblack, M, aspect_ratio = 1, ticks=.5:3.5, lims=(.5,3.5), gridalpha=1, legend=false, axis=false, ylabel=\"i\", xlabel=\"j\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustom Colors\n\nusing Colors\n\nmycolors = [colorant\"lightslateblue\",colorant\"limegreen\",colorant\"red\"]\nA = [i for i=50:300, j=1:100]\nheatmap(A, c=mycolors, clim=(1,300))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Areas Under Curves\n\ny = rand(10)\nplot(y, fillrange= y.*0 .+ .5, label= \"above/below 1/2\", legend =:top)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = LinRange(0,2,100)\ny1 = exp.(x)\ny2 = exp.(1.3 .* x)\nplot(x, y1, fillrange = y2, fillalpha = 0.35, c = 1, label = \"Confidence band\", legend = :topleft)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx = -3:.01:3\nareaplot(x, exp.(-x.^2/2)/√(2π),alpha=.25,legend=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM = [1 2 3; 7 8 9; 4 5 6; 0 .5 1.5]\nareaplot(1:3, M, seriescolor = [:red :green :blue ], fillalpha = [0.2 0.3 0.4])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nusing SpecialFunctions\nf = x-&gt;exp(-x^2/2)/√(2π)\nδ = .01\nplot()\nx = √2 .* erfinv.(2 .*(δ/2 : δ : 1) .- 1)\nareaplot(x, f.(x), seriescolor=[ :red,:blue], legend=false)\nplot!(x, f.(x),c=:black)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Shapes\n\nrectangle(w, h, x, y) = Shape(x .+ [0,w,w,0], y .+ [0,0,h,h])\ncircle(r,x,y) = (θ = LinRange(0,2π,500); (x.+r.*cos.(θ), y.+r.*sin.(θ)))\nplot(circle(5,0,0), ratio=1, c=:red, fill=true)\nplot!(rectangle(5*√2,5*√2,-2.5*√2,-2.5*√2),c=:white,fill=true,legend=false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting Distributions\nThe StatsPlots.jl package is very useful for making various plots of probability distributions.\n\nusing Distributions, StatsPlots\nplot(Normal(2, 5))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter(LogNormal(0.8, 1.5))\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also use this functionality to plot distributions of data in tabular data structures like DataFrames.\n\nusing DataFrames\ndat = DataFrame(a = 1:10, b = 10 .+ rand(10), c = 10 .* rand(10))\n@df dat density([:b :c], color=[:black :red])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEditing Plots Manually\n\npl = plot(1:4,[1, 4, 9, 16])\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npl.attr\n\nRecipesPipeline.DefaultsDict with 30 entries:\n  :dpi                      =&gt; 96\n  :background_color_outside =&gt; :match\n  :plot_titlefontvalign     =&gt; :vcenter\n  :warn_on_unsupported      =&gt; true\n  :background_color         =&gt; RGBA{Float64}(1.0, 1.0, 1.0, 1.0)\n  :inset_subplots           =&gt; nothing\n  :size                     =&gt; (672, 480)\n  :display_type             =&gt; :auto\n  :overwrite_figure         =&gt; true\n  :html_output_format       =&gt; :auto\n  :plot_titlefontfamily     =&gt; :match\n  :plot_titleindex          =&gt; 0\n  :foreground_color         =&gt; RGB{N0f8}(0.0, 0.0, 0.0)\n  :window_title             =&gt; \"Plots.jl\"\n  :plot_titlefontrotation   =&gt; 0.0\n  :extra_plot_kwargs        =&gt; Dict{Any, Any}()\n  :pos                      =&gt; (0, 0)\n  :plot_titlefonthalign     =&gt; :hcenter\n  :tex_output_standalone    =&gt; false\n  ⋮                         =&gt; ⋮\n\n\n\npl.series_list[1]\n\nPlots.Series(RecipesPipeline.DefaultsDict(:plot_object =&gt; Plot{Plots.GRBackend() n=1}, :subplot =&gt; Subplot{1}, :label =&gt; \"y1\", :fillalpha =&gt; nothing, :linealpha =&gt; nothing, :linecolor =&gt; RGBA{Float64}(0.0, 0.6056031704619725, 0.9786801190138923, 1.0), :x_extrema =&gt; (NaN, NaN), :series_index =&gt; 1, :markerstrokealpha =&gt; nothing, :markeralpha =&gt; nothing…))\n\n\n\npl[:size]=(300,200)\n\n(300, 200)\n\n\n\npl\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLog-Scaled Axes\n\nxx = .1:.1:10\nplot(xx.^2, xaxis=:log, yaxis=:log)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot(exp.(x), yaxis=:log)",
    "crumbs": [
      "Julia Tutorials",
      "Making Plots"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html",
    "href": "tutorials/julia-basics.html",
    "title": "Tutorial: Julia Basics",
    "section": "",
    "text": "This tutorial will give some examples of basic Julia commands and syntax.",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#overview",
    "href": "tutorials/julia-basics.html#overview",
    "title": "Tutorial: Julia Basics",
    "section": "",
    "text": "This tutorial will give some examples of basic Julia commands and syntax.",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#getting-help",
    "href": "tutorials/julia-basics.html#getting-help",
    "title": "Tutorial: Julia Basics",
    "section": "Getting Help",
    "text": "Getting Help\n\nCheck out the official documentation for Julia: https://docs.julialang.org/en/v1/.\nStack Overflow is a commonly-used resource for programming assistance.\nAt a code prompt or in the REPL, you can always type ?functionname to get help.",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#comments",
    "href": "tutorials/julia-basics.html#comments",
    "title": "Tutorial: Julia Basics",
    "section": "Comments",
    "text": "Comments\nComments hide statements from the interpreter or compiler. It’s a good idea to liberally comment your code so readers (including yourself!) know why your code is structured and written the way it is. Single-line comments in Julia are preceded with a #. Multi-line comments are preceded with #= and ended with =#",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#suppressing-output",
    "href": "tutorials/julia-basics.html#suppressing-output",
    "title": "Tutorial: Julia Basics",
    "section": "Suppressing Output",
    "text": "Suppressing Output\nYou can suppress output using a semi-colon (;).\n\n4+8;\n\nThat didn’t show anything, as opposed to:\n\n4+8\n\n12",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#variables",
    "href": "tutorials/julia-basics.html#variables",
    "title": "Tutorial: Julia Basics",
    "section": "Variables",
    "text": "Variables\nVariables are names which correspond to some type of object. These names are bound to objects (and hence their values) using the = operator.\n\nx = 5\n\n5\n\n\nVariables can be manipulated with standard arithmetic operators.\n\n4 + x\n\n9\n\n\nAnother advantage of Julia is the ability to use Greek letters (or other Unicode characters) as variable names. For example, type a backslash followed by the name of the Greek letter (i.e. \\alpha) followed by TAB.\n\nα = 3\n\n3\n\n\nYou can also include subscripts or superscripts in variable names using \\_ and \\^, respectively, followed by TAB. If using a Greek letter followed by a sub- or super-script, make sure you TAB following the name of the letter before the sub- or super-script. Effectively, TAB after you finish typing the name of each \\character.\n\nβ₁ = 10 # The name of this variable was entered with \\beta + TAB + \\_1 + TAB\n\n10\n\n\nHowever, try not to overwrite predefined names! For example, you might not want to use π as a variable name…\n\nπ\n\nπ = 3.1415926535897...\n\n\nIn the grand scheme of things, overwriting π is not a huge deal unless you want to do some trigonometry. However, there are more important predefined functions and variables that you may want to be aware of. Always check that a variable or function name is not predefined!",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#data-types",
    "href": "tutorials/julia-basics.html#data-types",
    "title": "Tutorial: Julia Basics",
    "section": "Data Types",
    "text": "Data Types\nEach datum (importantly, not the variable which is bound to it) has a data type. Julia types are similar to C types, in that they require not only the type of data (Int, Float, String, etc), but also the precision (which is related to the amount of memory allocated to the variable). Issues with precision won’t be a big deal in this class, though they matter when you’re concerned about performance vs. decimal accuracy of code.\nYou can identify the type of a variable or expression with the typeof() function.\n\ntypeof(\"This is a string.\")\n\nString\n\n\n\ntypeof(x)\n\nInt64\n\n\n\nNumeric types\nA key distinction is between an integer type (or Int) and a floating-point number type (or float). Integers only hold whole numbers, while floating-point numbers correspond to numbers with fractional (or decimal) parts. For example, 9 is an integer, while 9.25 is a floating point number. The difference between the two has to do with the way the number is stored in memory. 9, an integer, is handled differently in memory than 9.0, which is a floating-point number, even though they’re mathematically the same value.\n\ntypeof(9)\n\nInt64\n\n\n\ntypeof(9.25)\n\nFloat64\n\n\nSometimes certain function specifications will require you to use a Float variable instead of an Int. One way to force an Int variable to be a Float is to add a decimal point at the end of the integer.\n\ntypeof(9.)\n\nFloat64\n\n\n\n\nStrings\nStrings hold characters, rather than numeric values. Even if a string contains what seems like a number, it is actually stored as the character representation of the digits. As a result, you cannot use arithmetic operators (for example) on this datum.\n\n\"5\" + 5\n\nMethodError: MethodError(+, (\"5\", 5), 0x0000000000007af3)\nMethodError: no method matching +(::String, ::Int64)\n\nClosest candidates are:\n  +(::Any, ::Any, !Matched::Any, !Matched::Any...)\n   @ Base operators.jl:587\n  +(!Matched::Missing, ::Number)\n   @ Base missing.jl:123\n  +(!Matched::BigInt, ::Union{Int16, Int32, Int64, Int8})\n   @ Base gmp.jl:555\n  ...\n\nStacktrace:\n [1] top-level scope\n   @ ~/Teaching/BEE4850/website/tutorials/julia-basics.qmd:104\n\n\nHowever, you can try to tell Julia to interpret a string encoding a numeric character as a numeric value using the parse() function. This can also be used to encode a numeric data as a string.\n\nparse(Int64, \"5\") + 5\n\n10\n\n\nTwo strings can be concatenated using *:\n\n\"Hello\" * \" \" * \"there\"\n\n\"Hello there\"\n\n\n\n\nBooleans\nBoolean variables (or Bools) are logical variables, that can have true or false as values.\n\nb = true\n\ntrue\n\n\nNumerical comparisons, such as ==, !=, or &lt;, return a Bool.\n\nc = 9 &gt; 11\n\nfalse\n\n\nBools are important for logical flows, such as if-then-else blocks or certain types of loops.",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#mathematical-operations",
    "href": "tutorials/julia-basics.html#mathematical-operations",
    "title": "Tutorial: Julia Basics",
    "section": "Mathematical operations",
    "text": "Mathematical operations\nAddition, subtraction, multiplication, and division work as you would expect. Just pay attention to types! The type of the output is influenced by the type of the inputs: adding or multiplying an Int by a Float will always result in a Float, even if the Float is mathematically an integer. Division is a little special: dividing an Int by another Int will still return a float, because Julia doesn’t know ahead of time if the denominator is a factor of the numerator.\n\n3 + 5\n\n8\n\n\n\n3 * 2\n\n6\n\n\n\n3 * 2.\n\n6.0\n\n\n\n6 - 2\n\n4\n\n\n\n9 / 3\n\n3.0\n\n\nRaising a base to an exponent uses ^, not **.\n\n3^2\n\n9\n\n\nJulia allows the use of updating operators to simplify updating a variable in place (in other words, using x += 5 instead of x = x + 5.\n\nBoolean algebra\nLogical operations can be used on variables of type Bool. Typical operators are && (and), || (or), and ! (not).\n\ntrue && true\n\ntrue\n\n\n\ntrue && false\n\nfalse\n\n\n\ntrue || false\n\ntrue\n\n\n\n!true\n\nfalse\n\n\nComparisons can be chained together.\n\n3 &lt; 4 || 8 == 12\n\ntrue\n\n\nWe didn’t do this above, since Julia doesn’t require it, but it’s easier to understand these types of compound expressions if you use parentheses to signal the order of operations. This helps with debugging!\n\n(3 &lt; 4) || (8 == 12)\n\ntrue",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#data-structures",
    "href": "tutorials/julia-basics.html#data-structures",
    "title": "Tutorial: Julia Basics",
    "section": "Data Structures",
    "text": "Data Structures\nData structures are containers which hold multiple values in a convenient fashion. Julia has several built-in data structures, and there are many extensions provided in additional packages.\n\nTuples\nTuples are collections of values. Julia will pay attention to the types of these values, but they can be mixed. Tuples are also immutable: their values cannot be changed once they are defined.\nTuples can be defined by just separating values with commas.\n\ntest_tuple = 4, 5, 6\n\n(4, 5, 6)\n\n\nTo access a value, use square brackets and the desired index. Note: Julia indexing starts at 1, not 0!\n\ntest_tuple[1]\n\n4\n\n\nAs mentioned above, tuples are immutable. What happens if we try to change the value of the first element of test_tuple?\n\ntest_tuple[1] = 5\n\nMethodError: MethodError(setindex!, ((4, 5, 6), 5, 1), 0x0000000000007af3)\nMethodError: no method matching setindex!(::Tuple{Int64, Int64, Int64}, ::Int64, ::Int64)\nStacktrace:\n [1] top-level scope\n   @ ~/Teaching/BEE4850/website/tutorials/julia-basics.qmd:222\n\n\nTuples also do not have to hold the same types of values.\n\ntest_tuple_2 = 4, 5., 'h'\ntypeof(test_tuple_2)\n\nTuple{Int64, Float64, Char}\n\n\nTuples can also be defined by enclosing the values in parentheses.\ntest_tuple_3 = (4, 5., 'h')\ntypeof(test_tuple_3)\n\n\nArrays\nArrays also hold multiple values, which can be accessed based on their index position. Arrays are commonly defined using square brackets.\n\ntest_array = [1, 4, 7, 8]\ntest_array[2]\n\n4\n\n\nUnlike tuples, arrays are mutable, and their contained values can be changed later.\n\ntest_array[1] = 6\ntest_array\n\n4-element Vector{Int64}:\n 6\n 4\n 7\n 8\n\n\nArrays also can hold multiple types. Unlike tuples, this causes the array to no longer care about types at all.\n\ntest_array_2 = [6, 5., 'h']\ntypeof(test_array_2)\n\nVector{Any} (alias for Array{Any, 1})\n\n\nCompare this with test_array:\n\ntypeof(test_array)\n\nVector{Int64} (alias for Array{Int64, 1})\n\n\n\n\nDictionaries\nInstead of using integer indices based on position, dictionaries are indexed by keys. They are specified by passing key-value pairs to the Dict() method.\n\ntest_dict = Dict(\"A\"=&gt;1, \"B\"=&gt;2)\ntest_dict[\"B\"]\n\n2\n\n\n\n\nComprehensions\nCreating a data structure with more than a handful of elements can be tedious to do by hand. If your desired array follows a certain pattern, you can create structures using a comprehension. Comprehensions iterate over some other data structure (such as an array) implicitly and populate the new data structure based on the specified instructions.\n\n[i^2 for i in 0:1:5]\n\n6-element Vector{Int64}:\n  0\n  1\n  4\n  9\n 16\n 25\n\n\nFor dictionaries, make sure that you also specify the keys.\n\nDict(string(i) =&gt; i^2 for i in 0:1:5)\n\nDict{String, Int64} with 6 entries:\n  \"4\" =&gt; 16\n  \"1\" =&gt; 1\n  \"5\" =&gt; 25\n  \"0\" =&gt; 0\n  \"2\" =&gt; 4\n  \"3\" =&gt; 9",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#functions",
    "href": "tutorials/julia-basics.html#functions",
    "title": "Tutorial: Julia Basics",
    "section": "Functions",
    "text": "Functions\nA function is an object which accepts a tuple of arguments and maps them to a return value. In Julia, functions are defined using the following syntax.\n\nfunction my_actual_function(x, y)\n    return x + y\nend\nmy_actual_function(3, 5)\n\n8\n\n\nFunctions in Julia do not require explicit use of a return statement. They will return the last expression evaluated in their definition. However, it’s good style to explicitly return function outputs. This improves readability and debugging, especially when functions can return multiple expressions based on logical control flows (if-then-else blocks).\nFunctions in Julia are objects, and can be treated like other objects. They can be assigned to new variables or passed as arguments to other functions.\n\ng = my_actual_function\ng(3, 5)\n\n8\n\n\n\nfunction function_of_functions(f, x, y)\n    return f(x, y)\nend\nfunction_of_functions(g, 3, 5)\n\n8\n\n\n\nShort and Anonymous Functions\nIn addition to the long form of the function definition shown above, simple functions can be specified in more compact forms when helpful.\nThis is the short form:\n\nh₁(x) = x^2 # make the subscript using \\_1 + &lt;TAB&gt;\nh₁(4)\n\n16\n\n\nThis is the anonymous form:\n\nx-&gt;sin(x)\n(x-&gt;sin(x))(π/4)\n\n0.7071067811865475\n\n\n\n\nMutating Functions\nThe convention in Julia is that functions should not modify (or mutate) their input data. The reason for this is to ensure that the data is preserved. Mutating functions are mainly appropriate for applications where performance needs to be optimized, and making a copy of the input data would be too memory-intensive.\nIf you do write a mutating function in Julia, the convention is to add a ! to its name, like my_mutating_function!(x).\n\n\nOptional arguments\nThere are two extremes with regard to function parameters which do not always need to be changed. The first is to hard-code them into the function body, which has a clear downside: when you do want to change them, the function needs to be edited directly. The other extreme is to treat them as regular arguments, passing them every time the function is called. This has the downside of potentially creating bloated function calls, particularly when there is a standard default value that makes sense for most function evaluations.\nMost modern languages, including Julia, allow an alternate solution, which is to make these arguments optional. This involves setting a default value, which is used unless the argument is explicitly defined in a function call.\n\nfunction setting_optional_arguments(x, y, c=0.5)\n    return c * (x + y)\nend\n\nsetting_optional_arguments (generic function with 2 methods)\n\n\nIf we want to stick with the fixed value \\(c=0.5\\), all we have to do is call setting_optional_arguments with the x and y arguments.\n\nsetting_optional_arguments(3, 5)\n\n4.0\n\n\nOtherwise, we can pass a new value for c.\n\nsetting_optional_arguments(3, 5, 2)\n\n16\n\n\n\n\nPassing data structures as arguments\nInstead of passing variables individually, it may make sense to pass a data structure, such as an array or a tuple, and then unpacking within the function definition. This is straightforward in long form: access the appropriate elements using their index.\nIn short or anonymous form, there is a trick which allows the use of readable variables within the function definition.\n\nh₂((x,y)) = x*y # enclose the input arguments in parentheses to tell Julia to expect and unpack a tuple\n\nh₂ (generic function with 1 method)\n\n\n\nh₂((2, 3)) # this works perfectly, as we passed in a tuple\n\n6\n\n\n\nh₂(2, 3) # this gives an error, as h₂ expects a single tuple, not two different numeric values\n\nMethodError: MethodError(Main.Notebook.h₂, (2, 3), 0x0000000000007afd)\nMethodError: no method matching h₂(::Int64, ::Int64)\n\nClosest candidates are:\n  h₂(::Any)\n   @ Main.Notebook ~/Teaching/BEE4850/website/tutorials/julia-basics.qmd:368\n\nStacktrace:\n [1] top-level scope\n   @ ~/Teaching/BEE4850/website/tutorials/julia-basics.qmd:376\n\n\n\nh₂([3, 10]) # this also works with arrays instead of tuples\n\n30\n\n\n\n\nVectorized operations\nJulia uses dot syntax to vectorize an operation and apply it element-wise across an array.\nFor example, to calculate the square root of 3:\n\nsqrt(3)\n\n1.7320508075688772\n\n\nTo calculate the square roots of every integer between 1 and 5:\n\nsqrt.([1, 2, 3, 4, 5])\n\n5-element Vector{Float64}:\n 1.0\n 1.4142135623730951\n 1.7320508075688772\n 2.0\n 2.23606797749979\n\n\nThe same dot syntax is used for arithmetic operations over arrays, since these operations are really functions.\n\n[1, 2, 3, 4] .* 2\n\n4-element Vector{Int64}:\n 2\n 4\n 6\n 8\n\n\nVectorization can be faster and is more concise to write and read than applying the same function to multiple variables or objects explicitly, so take advantage!\n\n\nReturning multiple values\nYou can return multiple values by separating them with a comma. This implicitly causes the function to return a tuple of values.\n\nfunction return_multiple_values(x, y)\n    return x + y, x * y\nend\nreturn_multiple_values(3, 5)\n\n(8, 15)\n\n\nThese values can be unpacked into multiple variables.\n\nn, ν = return_multiple_values(3, 5)\nn\n\n8\n\n\n\nν\n\n15\n\n\n\n\nReturning nothing\nSometimes you don’t want a function to return any values at all. For example, you might want a function that only prints a string to the console.\n\nfunction print_some_string(x)\n    println(\"x: $x\")\n    return nothing\nend\nprint_some_string(42)\n\nx: 42",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#printing-text-output",
    "href": "tutorials/julia-basics.html#printing-text-output",
    "title": "Tutorial: Julia Basics",
    "section": "Printing Text Output",
    "text": "Printing Text Output\nThe Text() function returns its argument as a plain text string. Notice how this is different from evaluating a string!\n\nText(\"I'm printing a string.\")\n\nI'm printing a string.\n\n\nText() is used in this tutorial as it returns the string passed to it. To print directly to the console, use println().\n\nprintln(\"I'm writing a string to the console.\")\n\nI'm writing a string to the console.\n\n\n\nPrinting Variables In a String\nWhat if we want to include the value of a variable inside of a string? We do this using string interpolation, using $variablename inside of the string.\n\nbar = 42\nText(\"Now I'm printing a variable: $bar\")\n\nNow I'm printing a variable: 42",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#control-flows",
    "href": "tutorials/julia-basics.html#control-flows",
    "title": "Tutorial: Julia Basics",
    "section": "Control Flows",
    "text": "Control Flows\nOne of the tricky things about learning a new programming language can be getting used to the specifics of control flow syntax. These types of flows include conditional if-then-else statements or loops.\n\nConditional Blocks\nConditional blocks allow different pieces of code to be evaluated depending on the value of a boolean expression or variable. For example, if we wanted to compute the absolute value of a number, rather than using abs():\n\nfunction our_abs(x)\n    if x &gt;= 0\n        return x\n    else\n        return -x\n    end\nend\n\nour_abs (generic function with 1 method)\n\n\n\nour_abs(4)\n\n4\n\n\n\nour_abs(-4)\n\n4\n\n\nTo nest conditional statements, use elseif.\n\nfunction test_sign(x)\n    if x &gt; 0\n        return Text(\"x is positive.\")\n    elseif x &lt; 0\n        return Text(\"x is negative.\")\n    else\n        return Text(\"x is zero.\")\n    end\nend\n\ntest_sign (generic function with 1 method)\n\n\n\ntest_sign(-5)\n\nx is negative.\n\n\n\ntest_sign(0)\n\nx is zero.\n\n\n\n\nLoops\nLoops allow expressions to be evaluated repeatedly until they are terminated. The two main types of loops are while loops and for loops.\n\nWhile loops\nwhile loops continue to evaluate an expression so long as a specified boolean condition is true. This is useful when you don’t know how many iterations it will take for the desired goal to be reached.\n\nfunction compute_factorial(x)\n    factorial = 1\n    while (x &gt; 1)\n        factorial *= x\n        x -= 1\n    end\n    return factorial\nend\ncompute_factorial(5)\n\n120\n\n\n\nWhile loops can easily turn into infinite loops if the condition is never meaningfully updated. Be careful, and look there if your programs are getting stuck. Also, If the expression in a while loop is false when the loop is reached, the loop will never be evaluated.\n\n\n\nFor loops\nfor loops run for a finite number of iterations, based on some defined index variable.\n\nfunction add_some_numbers(x)\n    total_sum = 0 # initialize at zero since we're adding\n    for i=1:x # the counter i is updated every iteration\n        total_sum += i\n    end\n    return total_sum\nend\nadd_some_numbers(4)\n\n10\n\n\nfor loops can also iterate over explicitly passed containers, rather than iterating over an incrementally-updated index sequence. Use the in keyword when defining the loop.\n\nfunction add_passed_numbers(set)\n    total_sum = 0\n    for i in set # this is the syntax we use when we want i to correspond to different container values\n        total_sum += i\n    end\n    return total_sum\nend\nadd_passed_numbers([1, 3, 5])\n\n9",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#linear-algebra",
    "href": "tutorials/julia-basics.html#linear-algebra",
    "title": "Tutorial: Julia Basics",
    "section": "Linear algebra",
    "text": "Linear algebra\nMatrices are defined in Julia as 2d arrays. Unlike basic arrays, matrices need to contain the same data type so Julia knows what operations are allowed. When defining a matrix, use semicolons to separate rows. Row elements should not be separated by commas.\n\ntest_matrix = [1 2 3; 4 5 6]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\nYou can also specify matrices using spaces and newlines.\n\ntest_matrix_2 = [1 2 3\n                 4 5 6]\n\n2×3 Matrix{Int64}:\n 1  2  3\n 4  5  6\n\n\nFinally, matrices can be created using comprehensions by separating the inputs by a comma.\n\n[i*j for i in 1:1:5, j in 1:1:5]\n\n5×5 Matrix{Int64}:\n 1   2   3   4   5\n 2   4   6   8  10\n 3   6   9  12  15\n 4   8  12  16  20\n 5  10  15  20  25\n\n\nVectors are treated as 1d matrices.\n\ntest_row_vector = [1 2 3]\n\n1×3 Matrix{Int64}:\n 1  2  3\n\n\n\ntest_col_vector = [1; 2; 3]\n\n3-element Vector{Int64}:\n 1\n 2\n 3\n\n\nMany linear algebra operations on vectors and matrices can be loaded using the LinearAlgebra package.",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "tutorials/julia-basics.html#package-management",
    "href": "tutorials/julia-basics.html#package-management",
    "title": "Tutorial: Julia Basics",
    "section": "Package management",
    "text": "Package management\nSometimes you might need functionality that does not exist in base Julia. Julia handles packages using the Pkg package manager. After finding a package which has the functions that you need, you have two options: 1. Use the package management prompt in the Julia REPL (the standard Julia interface; what you get when you type julia in your terminal). Enter this by typing ] at the standard green Julia prompt julia&gt;. This will become a blue pkg&gt;. You can then download and install new packages using add packagename. 2. From the standard prompt, enter using Pkg; Pkg.add(packagename). The packagename package can then be used by adding using packagename to the start of the script.",
    "crumbs": [
      "Julia Tutorials",
      "Julia Basics"
    ]
  },
  {
    "objectID": "project/proposal.html",
    "href": "project/proposal.html",
    "title": "Project Proposal Instructions",
    "section": "",
    "text": "Your project should involve analyzing a data set of interest using model-based simulation. The proposal is an opportunity to think through and articulate the core science question, hypotheses, and modeling approaches, and get feedback about the plan."
  },
  {
    "objectID": "project/proposal.html#what-makes-a-good-project",
    "href": "project/proposal.html#what-makes-a-good-project",
    "title": "Project Proposal Instructions",
    "section": "What Makes A Good Project?",
    "text": "What Makes A Good Project?\nYour project topic should have the following features:\n\nFocus on a data set which can inform a relevant science question;\nInvolve one or more specific hypotheses which can be translated into a simulation model (either numerical or statistical is fine);\nIf only one hypothesis is considered, a meaningful “null” model should be included for comparison."
  },
  {
    "objectID": "project/proposal.html#proposal-guidelines",
    "href": "project/proposal.html#proposal-guidelines",
    "title": "Project Proposal Instructions",
    "section": "Proposal Guidelines",
    "text": "Proposal Guidelines\nThe proposal should have the following components, and be no more than 5 pages (not including references and plots) with 1 inch margins (on all sides) and at least 11 point font.\n\nBackground: What is the scientific context of your project? What question are you trying to address and why does it matter? What are the hypotheses you are focusing on?\nData Overview: Where was the data obtained from? In what context was it collected (e.g. is it observational data, collected in a lab, or reconstructed) and what implications that might have for its use in testing your hypotheses? Include some exploratory analysis and plots. Are there errors included in the data? Will you need any auxiliary data?\nModel Specification: What model(s) are you proposing to use to study the data in the context of your hypotheses? If you’re using a numerical simulation model, what parameters will you treat as uncertain and what is the probability model for the residuals? If you’re using a statistical model, what choices are you making about distributions? Include some initial exploratory analyses to analyze initial goodness-of-fit.\nReferences: Make sure to include any references cited in your proposal."
  },
  {
    "objectID": "project/simulation.html",
    "href": "project/simulation.html",
    "title": "Simulation Study Instructions",
    "section": "",
    "text": "The goal of the simulation study is to implement your model(s) and evaluate their simulations to assess the differences between them. Your goal is not to “test” relevant hypotheses yet, but to visualize and describe the differences in the model hindcasts and projections."
  },
  {
    "objectID": "project/simulation.html#simulation-study-guidelines",
    "href": "project/simulation.html#simulation-study-guidelines",
    "title": "Simulation Study Instructions",
    "section": "Simulation Study Guidelines",
    "text": "Simulation Study Guidelines\nThe proposal should have the following components with 1 inch margins (on all sides) and at least 11 point font.\n\nCalibration: Find and report appropriate parameter values for your model(s). This can be done through finding the maximum likelihood or maximum a posteriori estimates, with relevant uncertainty estimates obtained from the bootstrap or sampling from the posterior distribution using MCMC. Whatever approach you take, make sure to justify it and any associated assumptions (e.g. prior distributions).\nSimulation: Using the calibrated model(s), simulate synthetic datasets and compute useful summaries. Include useful graphical checks (including, if appropriate, hindcasts and projections) as needed.\nComparison: Discuss the differences in the calibrations and simulations and what they mean for your hypotheses(s). Were there any surprises?\nReferences: Make sure to include any references cited in your proposal."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About This Website",
    "section": "",
    "text": "This website contains course materials for the Spring 2025 edition of Environmental Data Analysis and Simulation, taught by Vivek Srikrishnan at Cornell University."
  },
  {
    "objectID": "about.html#acknowledgements",
    "href": "about.html#acknowledgements",
    "title": "About This Website",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMuch of the material for this course has evolved from discussions and work with Klaus Keller, Tony Wong, Casey Helgeson, Ben Seiyon Lee and James Doss-Gollin. Many thanks also to Andrew Gelman and Christian Robert, whose work heavily inspired many aspects of this course and which I refer back to regularly. Kieran Healy also wrote an outstanding book on Data Visualization which I learned a great deal from and still reference.\nThe layout for this site was also inspired by and draws from STA 210 at Duke University and Andrew Heiss’s course materials at Georgia State."
  },
  {
    "objectID": "about.html#tools-and-generation-workflow",
    "href": "about.html#tools-and-generation-workflow",
    "title": "About This Website",
    "section": "Tools and Generation Workflow",
    "text": "Tools and Generation Workflow\nThis website was built with Quarto, which allows me to integrate Julia code and output with the web content, pdfs, and slides in an amazingly clean fashion, while simplifying the process of generation. All materials can be generated through a simple workflow from the [GitHub Repository]."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#null-hypothesis-significance-testing",
    "href": "slides/lecture02-2-probreview.html#null-hypothesis-significance-testing",
    "title": "Probability Fundamentals",
    "section": "Null Hypothesis Significance Testing",
    "text": "Null Hypothesis Significance Testing\n\nBinary “significant”/“non-significant” decision framework;\nAccept or reject null hypothesis \\(\\mathcal{H}_0\\) based on p-value of test statistic (relative to significance level \\(\\alpha\\));\n“Typical” statistical models for \\(\\mathcal{H}_0\\) often chosen out of computational convenience\nThere be dragons: Multiple comparisons, lack of statistical power, reading rejection of \\(\\mathcal{H}_0\\) as acceptance of alternative \\(\\mathcal{H}\\)"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#what-nhst-means-and-doesnt",
    "href": "slides/lecture02-2-probreview.html#what-nhst-means-and-doesnt",
    "title": "Probability Fundamentals",
    "section": "What NHST Means (and Doesn’t)",
    "text": "What NHST Means (and Doesn’t)\n\nRejection of \\(\\mathcal{H}_0\\) does not mean \\(\\mathcal{H}\\) is true\nNot rejecting \\(\\mathcal{H}_0\\) does not mean \\(\\mathcal{H}\\) is false.\nSays nothing about \\(p(\\mathcal{H}_0 | y)\\)\n“Simply” a measure of surprise at seeing data under null model."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#what-is-uncertainty",
    "href": "slides/lecture02-2-probreview.html#what-is-uncertainty",
    "title": "Probability Fundamentals",
    "section": "What is Uncertainty?",
    "text": "What is Uncertainty?\n\n\n\n…A departure from the (unachievable) ideal of complete determinism…\n\n\n— Walker et al. (2003)"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#types-of-uncertainty",
    "href": "slides/lecture02-2-probreview.html#types-of-uncertainty",
    "title": "Probability Fundamentals",
    "section": "Types of Uncertainty",
    "text": "Types of Uncertainty\n\n\n\n\n\n\n\n\n\n\n\nUncertainty Type\nSource\nExample(s)\n\n\n\n\nAleatory uncertainty\nRandomness\nDice rolls, Instrument imprecision\n\n\nEpistemic uncertainty\nLack of knowledge\nClimate sensitivity, Premier League champion\n\n\n\n\n\n\n\nWhich Uncertainty Type Meme\n\n\n\n\n\nNote that the distinction between aleatory and epistemic uncertainty is somewhat arbitrary (aside from maybe some quantum effects). For example, we often think of coin tosses as aleatory, but if we had perfect information about the toss, we might be able to predict the outcome with less uncertainty. There’s a famous paper by Persi Diaconis where he collaborated with engineers to build a device which could arbitrary bias a “fair” coin toss.\nBut in practice, this doesn’t really matter: the key thing is whether for a given model we’re treating the uncertainty as entirely random (e.g. white noise) versus being interested in the impacts of that uncertainty on the outcome of interest. And there’s a representation theorem by the Bayesian actuary Bruno de Finetti which shows that, under a condition called exchangeability, we can think of any random sequence as arising from an independent and identically distributed process, so the practical difference can collapse further."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#probability",
    "href": "slides/lecture02-2-probreview.html#probability",
    "title": "Probability Fundamentals",
    "section": "Probability",
    "text": "Probability\nProbability is a language for expressing uncertainty.\nThe axioms of probability are straightforward:\n\n\\(\\mathcal{P}(E) \\geq 0\\);\n\\(\\mathcal{P}(\\Omega) = 1\\);\n\\(\\mathcal{P}(\\cup_{i=1}^\\infty E_i) = \\sum_{i=1}^\\infty \\mathcal{P}(E_i)\\) for disjoint \\(E_i\\).\n\n\nThe third is a generalization of the definition of independent events to sets of outcomes."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#frequentist-vs-bayesian-probability",
    "href": "slides/lecture02-2-probreview.html#frequentist-vs-bayesian-probability",
    "title": "Probability Fundamentals",
    "section": "Frequentist vs Bayesian Probability",
    "text": "Frequentist vs Bayesian Probability\n\n\nFrequentist:\n\nProbability as frequency over repeated observations.\nData are random, but parameters are not.\nHow consistent are estimates for different data?\n\n\n\nBayesian:\n\nProbability as degree of belief/betting odds.\nData and parameters are random variables;\nEmphasis on conditional probability."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#but-what-like-is-probability",
    "href": "slides/lecture02-2-probreview.html#but-what-like-is-probability",
    "title": "Probability Fundamentals",
    "section": "But What, Like, Is Probability?",
    "text": "But What, Like, Is Probability?\n\n\nFrequentist vs. Bayesian: different interpretations with some different methods and formalisms.\nWe will freely borrow from each school depending on the purpose and goal of an analysis.\n\n\n\n\nDefinitions of Probability Meme"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#probability-distributions",
    "href": "slides/lecture02-2-probreview.html#probability-distributions",
    "title": "Probability Fundamentals",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nDistributions are mathematical representations of probabilities over a range of possible outcomes.\n\\[x \\to \\mathbb{P}_{\\color{green}\\mathcal{D}}[x] = p_{\\color{green}\\mathcal{D}}\\left(x | {\\color{purple}\\theta}\\right)\\]\n\n\\({\\color{green}\\mathcal{D}}\\): probability distribution (often implicit);\n\\({\\color{purple}\\theta}\\): distribution parameters"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#sampling-notation",
    "href": "slides/lecture02-2-probreview.html#sampling-notation",
    "title": "Probability Fundamentals",
    "section": "Sampling Notation",
    "text": "Sampling Notation\nTo write \\(x\\) is sampled from \\(\\mathcal{D}(\\theta) = p(x|\\theta)\\): \\[x \\sim \\mathcal{D}(\\theta)\\]\nFor example, for a normal distribution: \\[x \\overset{\\text{i.i.d.}}{\\sim} \\mathcal{N}(\\mu, \\sigma)\\]\n\n“i.i.d.” means “identically and independently distributed.””"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#probability-density-function",
    "href": "slides/lecture02-2-probreview.html#probability-density-function",
    "title": "Probability Fundamentals",
    "section": "Probability Density Function",
    "text": "Probability Density Function\nA continuous distribution \\(\\mathcal{D}\\) has a probability density function (PDF) \\(f_\\mathcal{D}(x) = p(x | \\theta)\\).\nThe probability of \\(x\\) occurring in an interval \\((a, b)\\) is \\[\\mathbb{P}[a \\leq x \\leq b] = \\int_a^b f_\\mathcal{D}(x)dx.\\]\nImportant: \\(\\mathbb{P}(x = x^*)\\) is zero!"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#probability-mass-functions",
    "href": "slides/lecture02-2-probreview.html#probability-mass-functions",
    "title": "Probability Fundamentals",
    "section": "Probability Mass Functions",
    "text": "Probability Mass Functions\nDiscrete distributions have probability mass functions (PMFs) which are defined at point values, e.g. \\(p(x = x^*) \\neq 0\\).\n\nUnlike continuous distributions, we can talk about the probability of individual values for discrete distributions, which a PMF provides versus a PDF. But in general these are the same things."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#cumulative-density-functions",
    "href": "slides/lecture02-2-probreview.html#cumulative-density-functions",
    "title": "Probability Fundamentals",
    "section": "Cumulative Density Functions",
    "text": "Cumulative Density Functions\n\n\nIf \\(\\mathcal{D}\\) is a distribution with PDF \\(f_\\mathcal{D}(x)\\), the cumulative density function (CDF) of \\(\\mathcal{D}\\) is \\(F_\\mathcal{D}(x)\\):\n\\[F_\\mathcal{D}(x) = \\int_{-\\infty}^x f_\\mathcal{D}(u)du.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Relationship of CDF and PDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#relationship-between-pdfs-and-cdfs",
    "href": "slides/lecture02-2-probreview.html#relationship-between-pdfs-and-cdfs",
    "title": "Probability Fundamentals",
    "section": "Relationship Between PDFs and CDFs",
    "text": "Relationship Between PDFs and CDFs\nSince \\[F_\\mathcal{D}(x) = \\int_{-\\infty}^x f_\\mathcal{D}(u)du,\\]\nif \\(f_\\mathcal{D}\\) is continuous at \\(x\\), the Fundamental Theorem of Calculus gives: \\[f_\\mathcal{D}(x) = \\frac{d}{dx}F_\\mathcal{D}(x).\\]\n\nThe value of the CDF is the amount of probability “below” the value. So e.g. for a one-sided statistical test, the p-value is the complement of the CDF at the value of the test statistic."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#quantiles",
    "href": "slides/lecture02-2-probreview.html#quantiles",
    "title": "Probability Fundamentals",
    "section": "Quantiles",
    "text": "Quantiles\n\n\nThe quantile function is the inverse of the CDF:\n\\[q(\\alpha) = F^{-1}_\\mathcal{D}(\\alpha)\\]\nSo \\[x_0 = q(\\alpha) \\iff \\mathbb{P}_\\mathcal{D}(X &lt; x_0) = \\alpha.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Relationship of CDF and PDF\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 2"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#distributions-are-assumptions",
    "href": "slides/lecture02-2-probreview.html#distributions-are-assumptions",
    "title": "Probability Fundamentals",
    "section": "Distributions Are Assumptions",
    "text": "Distributions Are Assumptions\nSpecifying a distribution is making an assumption about observations and any applicable constraints.\nExamples: If your observations are…\n\nContinuous and fat-tailed? Cauchy distribution\nContinuous and bounded? Beta distribution\nSums of positive random variables? Gamma or Normal distribution."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#why-use-a-normal-distribution",
    "href": "slides/lecture02-2-probreview.html#why-use-a-normal-distribution",
    "title": "Probability Fundamentals",
    "section": "Why Use a Normal Distribution?",
    "text": "Why Use a Normal Distribution?\n\n\nTwo main reasons to use linear models/normal distributions:\n\nInferential: “Least informative” distribution assuming knowledge of just mean and variance;\nGenerative: Central Limit Theorem (summed fluctuations are asymptotically normal)\n\n\n\n\n\nWeight stack Gaussian distribution\n\n\n\nSource: r/GymMemes\n\n\n\nOne key thing: normal distributions are the “least informative” distribution given constraints on mean and variance. So all else being equal, this is a useful machine if all we’re interested in are those two moments."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#statistics-of-random-variables-are-random-variables",
    "href": "slides/lecture02-2-probreview.html#statistics-of-random-variables-are-random-variables",
    "title": "Probability Fundamentals",
    "section": "Statistics of Random Variables are Random Variables",
    "text": "Statistics of Random Variables are Random Variables\nThe sum or mean of a random sample is itself a random variable:\n\\[\\bar{X}_n = \\frac{1}{n}\\sum_{i=1}^n X_i \\sim \\mathcal{D}_n\\]\n\n\\(\\mathcal{D}_n\\): The sampling distribution of the mean (or sum, or other estimate of interest)."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#sampling-distributions",
    "href": "slides/lecture02-2-probreview.html#sampling-distributions",
    "title": "Probability Fundamentals",
    "section": "Sampling Distributions",
    "text": "Sampling Distributions\n\nIllustration of the Sampling Distribution"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#central-limit-theorem",
    "href": "slides/lecture02-2-probreview.html#central-limit-theorem",
    "title": "Probability Fundamentals",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nIf\n\n\\(\\mathbb{E}[X_i] = \\mu\\)\nand \\(\\text{Var}(X_i) = \\sigma^2 &lt; \\infty\\),\n\n\\[\\begin{align*}\n&\\bbox[yellow, 10px, border:5px solid red]\n{\\lim_{n \\to \\infty} \\sqrt{n}(\\bar{X}_n - \\mu ) = \\mathcal{N}(0, \\sigma^2)} \\\\\n\\Rightarrow &\\bbox[yellow, 10px, border:5px solid red] {\\bar{X}_n \\overset{\\text{approx}}{\\sim} \\mathcal{N}(\\mu, \\sigma^2/n)}\n\\end{align*}\\]"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#central-limit-theorem-more-intuitive",
    "href": "slides/lecture02-2-probreview.html#central-limit-theorem-more-intuitive",
    "title": "Probability Fundamentals",
    "section": "Central Limit Theorem (More Intuitive)",
    "text": "Central Limit Theorem (More Intuitive)\n\n\nFor a large enough set of samples, the sampling distribution of a sum or mean of random variables is approximately a normal distribution, even if the random variables themselves are not.\n\n\n\n\nSmall n Meme\n\n\n\nSource: Unknown"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#what-distribution-should-i-use",
    "href": "slides/lecture02-2-probreview.html#what-distribution-should-i-use",
    "title": "Probability Fundamentals",
    "section": "“What Distribution Should I Use?”",
    "text": "“What Distribution Should I Use?”\nThere is no right answer to this, no matter what a statistical test tells you.\n\nWhat assumptions are justifiable from theory?\nWhat information do you have?"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#what-distribution-should-i-use-1",
    "href": "slides/lecture02-2-probreview.html#what-distribution-should-i-use-1",
    "title": "Probability Fundamentals",
    "section": "“What Distribution Should I Use?”",
    "text": "“What Distribution Should I Use?”\nFor example, suppose our data are counts of events:\n\nIf you know something about rates, you can use a Poisson distribution\nIf you know something about probabilities, you can use a Binomial distribution."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#q-q-plots",
    "href": "slides/lecture02-2-probreview.html#q-q-plots",
    "title": "Probability Fundamentals",
    "section": "Q-Q Plots",
    "text": "Q-Q Plots\n\n\nOne exploratory method to see if your data is reasonably described by a theoretical distribution is a Q-Q plot.\n\n\n\nCode\nsamps = rand(Normal(0, 3), 20)\nqqplot(Normal, samps, tickfontsize=16, guidefontsize=18, linewidth=3, markersize=6)\nxlabel!(\"Theoretical Quantiles\")\nylabel!(\"Empirical Quantiles\")\nplot!(size=(500, 450))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#fat-tailed-data-and-q-q-plots",
    "href": "slides/lecture02-2-probreview.html#fat-tailed-data-and-q-q-plots",
    "title": "Probability Fundamentals",
    "section": "Fat-Tailed Data and Q-Q Plots",
    "text": "Fat-Tailed Data and Q-Q Plots\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Normal vs Cauchy Distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Q-Q Plot\n\n\n\n\n\n\n\nFigure 4: Q-Q Plot for Cauchy Data and Normal Distribution"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#likelihood",
    "href": "slides/lecture02-2-probreview.html#likelihood",
    "title": "Probability Fundamentals",
    "section": "Likelihood",
    "text": "Likelihood\nHow do we “fit” distributions to a dataset?\nLikelihood of data to have come from distribution \\(f(\\mathbf{x} | \\theta)\\):\n\\[\\mathcal{L}(\\theta | \\mathbf{x}) = \\underbrace{f(\\mathbf{x} | \\theta)}_{\\text{PDF}}\\]\n\nThe likelihood gives us a measure of how probable a dataset is from a given distribution. It’s the PDF of the distribution at the data.\nBut the perspective is flipped: instead of fixing a distribution and calculating the probability of some data, we fix the data and look at how the probability of observing that data changes as the distribution changes."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#normal-distribution-pdf",
    "href": "slides/lecture02-2-probreview.html#normal-distribution-pdf",
    "title": "Probability Fundamentals",
    "section": "Normal Distribution PDF",
    "text": "Normal Distribution PDF\n\\[f_\\mathcal{D}(x) = p(x | \\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}\\left(\\frac{x - \\mu}{\\sigma}^2\\right)\\right)\\]\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#likelihood-of-multiple-samples",
    "href": "slides/lecture02-2-probreview.html#likelihood-of-multiple-samples",
    "title": "Probability Fundamentals",
    "section": "Likelihood of Multiple Samples",
    "text": "Likelihood of Multiple Samples\nFor multiple (independent) samples \\(\\mathbf{x} = \\{x_1, \\ldots, x_n\\}\\):\n\\[\\mathcal{L}(\\theta | \\mathbf{x}) = \\prod_{i=1}^n \\mathcal{L}(\\theta | x_i).\\]"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#likelihood-example",
    "href": "slides/lecture02-2-probreview.html#likelihood-example",
    "title": "Probability Fundamentals",
    "section": "Likelihood Example",
    "text": "Likelihood Example\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nLikelihood\n\n\n\n\n\\(N(0, 1)\\)\n1.9e-8"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#likelihood-example-1",
    "href": "slides/lecture02-2-probreview.html#likelihood-example-1",
    "title": "Probability Fundamentals",
    "section": "Likelihood Example",
    "text": "Likelihood Example\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nLikelihood\n\n\n\n\n\\(N(0, 1)\\)\n1.9e-8\n\n\n\\(N(-1, 2)\\)\n1.5e-8"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#likelihood-example-2",
    "href": "slides/lecture02-2-probreview.html#likelihood-example-2",
    "title": "Probability Fundamentals",
    "section": "Likelihood Example",
    "text": "Likelihood Example\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistribution\nLikelihood\n\n\n\n\n\\(N(0, 1)\\)\n1.9e-8\n\n\n\\(N(-1, 2)\\)\n1.5e-8\n\n\n\\(N(-1, 1)\\)\n4.6e-8"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#maximizing-likelihood",
    "href": "slides/lecture02-2-probreview.html#maximizing-likelihood",
    "title": "Probability Fundamentals",
    "section": "Maximizing Likelihood",
    "text": "Maximizing Likelihood\nTo find the parameters \\(\\hat{\\theta}\\) which best fit the data:\n\\(\\hat{\\theta} = \\max_\\theta \\mathcal{L}(\\theta | \\mathbf{x})\\)"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#generally-maximizing-likelihood",
    "href": "slides/lecture02-2-probreview.html#generally-maximizing-likelihood",
    "title": "Probability Fundamentals",
    "section": "Generally Maximizing Likelihood",
    "text": "Generally Maximizing Likelihood\n\n\nCan use optimization algorithms to maximize \\(\\theta \\to \\mathcal{L}(\\theta | x).\\)\nDragons: Probability calculations tend to under- and overflow due to floating point precision.\n\n\n\n\nFloating Point Logarithms meme"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#confidence-intervals",
    "href": "slides/lecture02-2-probreview.html#confidence-intervals",
    "title": "Probability Fundamentals",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\n\n\nFrequentist estimates have confidence intervals, which will contain the “true” parameter value for \\(\\alpha\\)% of data samples.\nNo guarantee that an individual CI contains the true value (with any “probability”)!\n\n\n\n\nHorseshoe Illustration\n\n\n\nSource: https://www.wikihow.com/Throw-a-Horseshoe\n\n\n\nConfidence intervals only capture uncertainty in parameter inferences due to data uncertainty, though this language sometimes gets misused to also refer to data/estimand uncertainty."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#example-95-cis-for-n0.4-2",
    "href": "slides/lecture02-2-probreview.html#example-95-cis-for-n0.4-2",
    "title": "Probability Fundamentals",
    "section": "Example: 95% CIs for N(0.4, 2)",
    "text": "Example: 95% CIs for N(0.4, 2)\n\nCode\n# set up distribution\nmean_true = 0.4\nn_cis = 100 # number of CIs to compute\ndist = Normal(mean_true, 2)\n\n# use sample size of 100\nsamples = rand(dist, (100, n_cis))\n# mapslices broadcasts over a matrix dimension, could also use a loop\nsample_means = mapslices(mean, samples; dims=1)\nsample_sd = mapslices(std, samples; dims=1) \nmc_sd = 1.96 * sample_sd / sqrt(100)\nmc_ci = zeros(n_cis, 2) # preallocate\nfor i = 1:n_cis\n    mc_ci[i, 1] = sample_means[i] - mc_sd[i]\n    mc_ci[i, 2] = sample_means[i] + mc_sd[i]\nend\n# find which CIs contain the true value\nci_true = (mc_ci[:, 1] .&lt; mean_true) .&& (mc_ci[:, 2] .&gt; mean_true)\n# compute percentage of CIs which contain the true value\nci_frac1 = 100 * sum(ci_true) ./ n_cis\n\n# plot CIs\np1 = plot([mc_ci[1, :]], [1, 1], linewidth=3, color=:deepskyblue, label=\"95% Confidence Interval\", title=\"Sample Size 100\", yticks=:false, legend=:false)\nfor i = 2:n_cis\n    if ci_true[i]\n        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:deepskyblue, label=:false)\n    else\n        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:red, label=:false)\n    end\nend\nvline!(p1, [mean_true], color=:black, linewidth=2, linestyle=:dash, label=\"True Value\") # plot true value as a vertical line\nxaxis!(p1, \"Estimate\")\nplot!(p1, size=(500, 350)) # resize to fit slide\n\n# use sample size of 1000\nsamples = rand(dist, (1000, n_cis))\n# mapslices broadcasts over a matrix dimension, could also use a loop\nsample_means = mapslices(mean, samples; dims=1)\nsample_sd = mapslices(std, samples; dims=1) \nmc_sd = 1.96 * sample_sd / sqrt(1000)\nmc_ci = zeros(n_cis, 2) # preallocate\nfor i = 1:n_cis\n    mc_ci[i, 1] = sample_means[i] - mc_sd[i]\n    mc_ci[i, 2] = sample_means[i] + mc_sd[i]\nend\n# find which CIs contain the true value\nci_true = (mc_ci[:, 1] .&lt; mean_true) .&& (mc_ci[:, 2] .&gt; mean_true)\n# compute percentage of CIs which contain the true value\nci_frac2 = 100 * sum(ci_true) ./ n_cis\n\n# plot CIs\np2 = plot([mc_ci[1, :]], [1, 1], linewidth=3, color=:deepskyblue, label=\"95% Confidence Interval\", title=\"Sample Size 1,000\", yticks=:false, legend=:false)\nfor i = 2:n_cis\n    if ci_true[i]\n        plot!(p2, [mc_ci[i, :]], [i, i], linewidth=2, color=:deepskyblue, label=:false)\n    else\n        plot!(p2, [mc_ci[i, :]], [i, i], linewidth=2, color=:red, label=:false)\n    end\nend\nvline!(p2, [mean_true], color=:black, linewidth=2, linestyle=:dash, label=\"True Value\") # plot true value as a vertical line\nxaxis!(p2, \"Estimate\")\nplot!(p2, size=(500, 350)) # resize to fit slide\n\ndisplay(p1)\ndisplay(p2)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sample Size 100\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Sample Size 1,000\n\n\n\n\n\n\n\nFigure 6: Display of 95% confidence intervals\n\n\n\n95% of the CIs contain the true value (left) vs. 94% (right)"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#predictive-intervals",
    "href": "slides/lecture02-2-probreview.html#predictive-intervals",
    "title": "Probability Fundamentals",
    "section": "Predictive Intervals",
    "text": "Predictive Intervals\n\n\nPredictive intervals capture uncertainty in an estimand.\nWith what probability would I see a particular outcome in the future?\nOften need to construct these using simulation.\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Two different 95% credible intervals.\n\n\n\n\n\n\nDue to this non-uniqueness, the typical convention is to use the “equal tailed” interval based on quantiles."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#next-classes",
    "href": "slides/lecture02-2-probreview.html#next-classes",
    "title": "Probability Fundamentals",
    "section": "Next Classes",
    "text": "Next Classes\nNext Week: Probability Models for Data"
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#assessments",
    "href": "slides/lecture02-2-probreview.html#assessments",
    "title": "Probability Fundamentals",
    "section": "Assessments",
    "text": "Assessments\nHomework 1 due next Friday (2/7).\nQuiz: Due before next class.\nReading: Annotate/submit writing before next class, will reserve time for discussion."
  },
  {
    "objectID": "slides/lecture02-2-probreview.html#references-scroll-for-full-list",
    "href": "slides/lecture02-2-probreview.html#references-scroll-for-full-list",
    "title": "Probability Fundamentals",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nWalker, W. E., Harremoës, P., Rotmans, J., Sluijs, J. P. van der, Asselt, M. B. A. van, Janssen, P., & Krayer von Krauss, M. P. (2003). Defining uncertainty: A conceptual basis for uncertainty management in model-based decision support. Integrated Assessment, 4, 5–17. https://doi.org/10.1076/iaij.4.1.5.16466"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#science-as-decision-making-under-uncertainty",
    "href": "slides/lecture02-1-nhst.html#science-as-decision-making-under-uncertainty",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Science as Decision-Making Under Uncertainty",
    "text": "Science as Decision-Making Under Uncertainty\n\n\nGoal is to draw insights:\n\nAbout causes and effects;\nAbout interventions.\n\nBut our models are simplifications and our observations are uncertain!\n\n\n\n\nXKCD 2440\n\n\n\nSource: XKCD 2440\n\n\n\nThese decisions are complicated by model simplifications and observational uncertainties.\nSo we can never actually “know” something is true: we instead assess the consistency of evidence with predictions from theory.\nBut often the predictions are not black-and-white and must be understood probabilistically."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#data-generation-approximates-reality",
    "href": "slides/lecture02-1-nhst.html#data-generation-approximates-reality",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Data Generation Approximates Reality",
    "text": "Data Generation Approximates Reality\n\n\n\n\n\nEstimand Estimator Cake\n\n\n\n\n\n\n\nEstimand Estimator Cake\n\n\n\n\n\n\n\n\nEstimate Cake\n\n\n\n\n\nSource: Richard McElreath\n\n\nGoal is to start with some “true” process, then apply a procedure (experimental/observational + statistical) and recover what is hopefully a good estimate.\nBut lots can go wrong in this process!"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#bayesian-risk-based-decision-analysis",
    "href": "slides/lecture02-1-nhst.html#bayesian-risk-based-decision-analysis",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Bayesian (Risk-Based) Decision Analysis",
    "text": "Bayesian (Risk-Based) Decision Analysis\nTake some decision \\(d(x)\\) based on \\(x\\).\n\\[\\overbrace{R(d(x))}^{\\text{risk}} = \\int_Y \\overbrace{\\mathcal{L}(d(x), y)}^{\\text{loss function}} \\overbrace{\\pi(y | x)}^{\\substack{\\text{probability} \\\\ \\text{of outcome}}}dy\\]\n\nThen the optimal decision is \\(\\hat{\\alpha} = \\underset{\\alpha}{\\operatorname{argmin}} R(\\alpha)\\)."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#pascals-wager-as-bda",
    "href": "slides/lecture02-1-nhst.html#pascals-wager-as-bda",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Pascal’s Wager as BDA",
    "text": "Pascal’s Wager as BDA\n\n\n\nLoss of mistaken belief: -c\nLoss of mistaken disbelief: \\(-\\infty\\)\nLoss of correct disbelief: +c\nLoss of correct belief: \\(+\\infty\\)\n\nPascal’s conclusion: “Optimal” decision is belief regardless of asserted probability of God’s existence.\n\n\n\n\nBlaise Pascal\n\n\n\nSource: Wikipedia\n\n\n\nBlaise Pascal formulated a decision problem as evidence that rationality could not be used to justify sin/a lack of belief in God. This was done in what we now recognize as Bayesian terms (though it predates Bayes)."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#standard-parameter-estimators",
    "href": "slides/lecture02-1-nhst.html#standard-parameter-estimators",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Standard Parameter Estimators",
    "text": "Standard Parameter Estimators\nWhat if we want to estimate a parameter \\(\\hat{\\theta}\\) from data \\(x\\)?\n\n\n\n\n\n\n\n\nLoss Function\n\\(\\mathcal{L}(\\hat{\\theta}, \\theta)\\)\n\\(\\hat{\\alpha}\\)\n\n\n\n\nQuadratic\n\\(\\|\\hat{\\theta} - \\theta\\|^2\\)\n\\(\\text{Mean}(x)\\)\n\n\nLinear\n\\(\\|\\hat{\\theta} - \\theta\\|\\)\n\\(\\text{Median}(x)\\)\n\n\n0-1\n\\(\\begin{cases} 0 & \\hat{\\theta} \\neq \\theta \\\\ 1 & \\hat{\\theta} = \\theta \\end{cases}\\)\n\\(\\text{Mode}(x)\\)"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#risk-based-analysis-the-original-statistical-decision-making",
    "href": "slides/lecture02-1-nhst.html#risk-based-analysis-the-original-statistical-decision-making",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Risk-Based Analysis: The Original Statistical Decision-Making",
    "text": "Risk-Based Analysis: The Original Statistical Decision-Making\n\n\n\n\n\nOrbit of Ceres\n\n\n\nSource: Wikipedia\n\n\n\n\n\nPiazzi’s Measurements\n\n\n\nSource: Wikipedia\n\n\n\nThis type of statistical decision-making prompted the introduction of ordinary least squares by Gauss in the 1800s. The question was to predict the position of Ceres given a few error-prone observations by Giuseppe Piazzi."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#origin-of-ordinary-least-squares",
    "href": "slides/lecture02-1-nhst.html#origin-of-ordinary-least-squares",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Origin of Ordinary Least Squares",
    "text": "Origin of Ordinary Least Squares\n\n\nGauss (1809): Risk/Bayesian argument for OLS estimator from quadratic loss.\n\n\n\n\nGerman 10 Mark Note with Gauss\n\n\n\nSource: National Curve Bank\n\n\n\n\n\n\nGauss\n\n\n\nSource: Wikipedia"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#questions-we-might-like-to-answer",
    "href": "slides/lecture02-1-nhst.html#questions-we-might-like-to-answer",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Questions We Might Like To Answer",
    "text": "Questions We Might Like To Answer\n\n\nAre high water levels influenced by environmental change?\nDoes some environmental condition have an effect on water quality/etc?\nDoes a drug or treatment have some effect?"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#onus-probandi-incumbit-ei-qui-dicit-non-ei-qui-negat",
    "href": "slides/lecture02-1-nhst.html#onus-probandi-incumbit-ei-qui-dicit-non-ei-qui-negat",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Onus probandi incumbit ei qui dicit, non ei qui negat",
    "text": "Onus probandi incumbit ei qui dicit, non ei qui negat\n\n\nCore assumption: Burden of proof is on someone claiming an effect (or a similar hypothesis).\n\n\n\n\nNull Hypothesis Meme\n\n\n\n\nThe title of this slide is a reference to the burden of proof is on the person who affirms, not one who denies. Can think of this as similar to Ockham’s razor or someone mantras: we want to propose hypotheses about scientific phenomena and then see if the evidence supports it."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#null-hypothesis-significance-testing",
    "href": "slides/lecture02-1-nhst.html#null-hypothesis-significance-testing",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Null Hypothesis Significance Testing",
    "text": "Null Hypothesis Significance Testing\n\n\n\nCheck if the data is consistent with a “null” model;\nIf the data is unlikely from the null model (to some level of significance), this is evidence for the alternative.\nIf the data is consistent with the null, there is no need for an alternative hypothesis.\n\n\n\n\n\nAlternative Hypothesis Meme\n\n\n\n\nFor scientific hypotheses, this has been encoded in the NHST paradigm:\n\nThink of a “null” hypothesis and look for evidence that it is reasonably consistent with the data.\nIf the data can be explained by the null, then we have no clear evidence for the alternative hypothesis.\nIf the data is highly unlikely under the null hypothesis, then that gives us reason to reject the null and favor the alternative."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#from-null-hypothesis-to-null-model",
    "href": "slides/lecture02-1-nhst.html#from-null-hypothesis-to-null-model",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "From Null Hypothesis to Null Model",
    "text": "From Null Hypothesis to Null Model\n\n\n…the null hypothesis must be exact, that is free of vagueness and ambiguity, because it must supply the basis of the ‘problem of distribution,’ of which the test of significance is the solution.\n\n\n— R. A. Fisher, The Design of Experiments, 1935.\n\n\n\nThe trick is to go from a null scientific hypothesis to a null statistical model, hence Fisher’s comment about the need for a null hypothesis to be “exact”."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#example-high-water-nonstationarity",
    "href": "slides/lecture02-1-nhst.html#example-high-water-nonstationarity",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Example: High Water Nonstationarity",
    "text": "Example: High Water Nonstationarity\n\n\nCode\n# load SF tide gauge data\n# read in data and get annual maxima\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n    # This uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n    df = @chain fname begin\n        CSV.read(DataFrame; header=false)\n        rename(\"Column1\" =&gt; \"year\", \"Column2\" =&gt; \"month\", \"Column3\" =&gt; \"day\", \"Column4\" =&gt; \"hour\", \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n        select(:datetime, :gauge)\n    end\n    return df\nend\n\ndat = load_data(\"data/surge/h551.csv\")\n\n# detrend the data to remove the effects of sea-level rise and seasonal dynamics\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# group data by year and compute the annual maxima\ndat_ma = dropmissing(dat_ma) # drop missing data\ndat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :], groupby(transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\ndelete!(dat_annmax, nrow(dat_annmax)) # delete 2023; haven't seen much of that year yet\nrename!(dat_annmax, :datetime_function =&gt; :Year)\nselect!(dat_annmax, [:Year, :residual])\ndat_annmax.residual = dat_annmax.residual / 1000 # convert to m\n\n# make plots\np1 = plot(\n    dat_annmax.Year,\n    dat_annmax.residual;\n    xlabel=\"Year\",\n    ylabel=\"Annual Max Tide Level (m)\",\n    label=false,\n    marker=:circle,\n    markersize=5,\n    tickfontsize=16,\n    guidefontsize=18,\n    left_margin=5mm, \n    bottom_margin=5mm\n)\n\nn = nrow(dat_annmax)\nlinfit = lm(@formula(residual ~ Year), dat_annmax)\npred = coef(linfit)[1] .+ coef(linfit)[2] * dat_annmax.Year\n\nplot!(p1, dat_annmax.Year, pred, linewidth=3, label=\"Linear Trend\")\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Annual maxima surge data from the San Francisco, CA tide gauge.\n\n\n\n\n\nFor example, consider this annual extreme high water dataset from San Francisco from 1897 through 2022. A linear fit gives us an observed trend of 0.4 mm/yr. Is that meaningful?\nNote that we didn’t do anything yet to justify whether linear regression is a non-stupid thing to do (hint: it’s a stupid thing to do): we’ll talk about this more later."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#the-null-is-the-trend-real",
    "href": "slides/lecture02-1-nhst.html#the-null-is-the-trend-real",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "The Null: Is The Trend Real?",
    "text": "The Null: Is The Trend Real?\n\\(\\mathcal{H}_0\\) (Null Hypothesis):\n\nThe “trend” is just due to chance, there is no long-term trend in the data.\n\n\n\nStatistically:\n\n\\[y = \\underbrace{b}_{\\text{constant}} + \\underbrace{\\varepsilon}_{\\text{residuals}}, \\qquad \\varepsilon \\underbrace{\\sim}_{\\substack{\\text{distributed} \\\\ {\\text{according to}}}} \\mathcal{N}(0, \\sigma^2) \\]"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#an-alternative-hypothesis",
    "href": "slides/lecture02-1-nhst.html#an-alternative-hypothesis",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "An Alternative Hypothesis",
    "text": "An Alternative Hypothesis\n\\(\\mathcal{H}\\):\n\nThe trend is likely non-zero in time.\n\n\n\nStatistically:\n\n\\[y = a \\times t + b + \\varepsilon, \\qquad \\varepsilon \\sim Normal(0, \\sigma^2) \\]"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#null-test",
    "href": "slides/lecture02-1-nhst.html#null-test",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Null Test",
    "text": "Null Test\nComparing \\(\\mathcal{H}\\) with \\(\\mathcal{H}_0\\):\n\n\\(\\mathcal{H}\\): \\(a \\neq 0\\)\n\\(\\mathcal{H}_0\\): \\(a = 0\\)\n\n\nIn this example, our null is an example of a point-null hypothesis."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#computing-the-test-statistic",
    "href": "slides/lecture02-1-nhst.html#computing-the-test-statistic",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Computing the Test Statistic",
    "text": "Computing the Test Statistic\nFor this type of null hypothesis test, our test statistic is the slope of the OLS fit \\[\\hat{a} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{(x_i - \\bar{x})^2}.\\]\n\nAssuming the null, the sampling distribution of the statistic is \\[\\frac{\\hat{a}}{SE_{\\hat{a}}} \\sim t_{n-2}.\\]"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#statistical-significance",
    "href": "slides/lecture02-1-nhst.html#statistical-significance",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Statistical Significance",
    "text": "Statistical Significance\nIs the value of the test statistic consistent with the null hypothesis?\n\n\nMore formally, could the test statistic have been reasonably observed from a random sample given the null hypothesis?"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#p-values-quantification-of-surprise",
    "href": "slides/lecture02-1-nhst.html#p-values-quantification-of-surprise",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "p-Values: Quantification of “Surprise”",
    "text": "p-Values: Quantification of “Surprise”\n\n\nOne-Tailed Test:\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Illustration of a p-value\n\n\n\n\n\nTwo-Tailed Test:\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Illustration of a two-tailed p-value\n\n\n\n\n\n\nIn the tide gauge test, for a two-tailed test, the p-value is 0.02. What does that mean?"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#error-types",
    "href": "slides/lecture02-1-nhst.html#error-types",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Error Types",
    "text": "Error Types\n\n\n\n\n\n\n\nNull Hypothesis Is\n\n\n\n\n\n\n\n\nTrue\n\n\nFalse\n\n\n\n\nDecision About Null Hypothesis\n\n\nDon’t reject\n\n\nTrue negative (probability \\(1-\\alpha\\))\n\n\nType II error (probability \\(\\beta\\))\n\n\n\n\nReject\n\n\nType I Error (probability \\(\\alpha\\))\n\n\nTrue positive (probability \\(1-\\beta\\))\n\n\n\n\nThe general testing framework is built around Type I (false positive) and Type II (false negative) errors."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#navigating-type-i-and-ii-errors",
    "href": "slides/lecture02-1-nhst.html#navigating-type-i-and-ii-errors",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Navigating Type I and II Errors",
    "text": "Navigating Type I and II Errors\nThe standard null hypothesis significance framework is based on balancing the chance of making Type I (false positive) and Type II (false negative) errors.\nIdea: Set a significance level \\(\\alpha\\) which is an “acceptable” probability of making a Type I error.\nAside: The probability \\(1-\\beta\\) of correctly rejecting \\(H_0\\) is the power.\n\nNote that these are frequentist concepts, not applicable to a single dataset (which give rise to p-values, which are random values).\nIf we only run a single experiment all we can claim is that if we had run a long series of experiments we would have had 100α% false positives had H0 been true and 100β% false negatives had H1 been true provided we got the power calculations right. Note the conditionals."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#p-value-and-significance",
    "href": "slides/lecture02-1-nhst.html#p-value-and-significance",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "p-Value and Significance",
    "text": "p-Value and Significance\nCommon practice: If the p-value is sufficiently small (below \\(\\alpha\\)), reject the null hypothesis with \\(1-\\alpha\\) confidence, or declare that the alternative hypothesis is statistically significant at the \\(1-\\alpha\\) level.\nThis can mean:\n\n\nThe null hypothesis is not true for that data-generating process;\nThe null hypothesis is true but the data is an outlying sample.\n\n\n\nThis is a strange hybrid of two schools of frequentist statistics, that of Fisher and of Neyman-Pearson. Fisher viewed p-values as weak evidence which was part of an inductive process (even when assuming unbiased sampling and accurate measurement), while the Neyman-Pearson significance framework was based on quantitative specifications of alternative hypotheses with explicitly calculated power."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#what-p-values-are-not",
    "href": "slides/lecture02-1-nhst.html#what-p-values-are-not",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "What p-Values Are Not",
    "text": "What p-Values Are Not\n\n\n\nProbability that the null hypothesis is true (this is never computed);\nAn indication of the effect size (or the stakes of that effect).\n\n\n\\[ \\underbrace{p(S \\geq \\hat{S}) | \\mathcal{H}_0)}_{\\text{p-value}} \\neq \\underbrace{p(\\mathcal{H}_0 | S \\geq \\hat{S})}_{\\substack{\\text{probability of} \\\\ \\text{null}}}!\\]\n\n\nA p-value is a random variable which depends on the data. It’s evidence which can be collected from a single experiment but can not establish .\nOver repeated experiments, reasoning about validity of the null should have the right properties assuming the experiments are conducted faithfully."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#statistical-significance-scientific-significance",
    "href": "slides/lecture02-1-nhst.html#statistical-significance-scientific-significance",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Statistical Significance ≠ Scientific Significance",
    "text": "Statistical Significance ≠ Scientific Significance\n\n\nStatistical significance does not mean anything about:\n\nwhether the alternative hypothesis is “true”;\nan accurate reflection of the data-generating process.\n\n\n\n\n\nHypothesis vs. Causal Meme"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#what-is-any-statistical-test-doing",
    "href": "slides/lecture02-1-nhst.html#what-is-any-statistical-test-doing",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "What is Any Statistical Test Doing?",
    "text": "What is Any Statistical Test Doing?\n\nAssume the null hypothesis \\(\\mathcal{H}_0\\).\nCompute the test statistic \\(\\hat{S}\\) for the sample.\nObtain the sampling distribution of the test statistic \\(S\\) under \\(H_0\\).\nCalculate \\(\\mathbb{P}(S &gt; \\hat{S})\\) (the p-value)."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#why-was-mathcalh_0-chosen",
    "href": "slides/lecture02-1-nhst.html#why-was-mathcalh_0-chosen",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Why Was \\(\\mathcal{H}_0\\) chosen?",
    "text": "Why Was \\(\\mathcal{H}_0\\) chosen?\n\n\n\nOften out of convenience for the test.\nPoint-null hypotheses are almost always wrong for the social and environmental sciences.\n\n\n\n\n\nPoint-Null Hypothesis Meme"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#non-uniqueness-of-null-models",
    "href": "slides/lecture02-1-nhst.html#non-uniqueness-of-null-models",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Non-Uniqueness of “Null” Models",
    "text": "Non-Uniqueness of “Null” Models\n\n\nIs there a trend in the SF tide gauge trend data?\n\nTrend as regression (\\(p \\approx 0.02\\))\nMann-Kendall test for monotonic trend (\\(p \\approx 0.5\\))\n\n\n\n\n\nNon-Uniqueness of Null Models\n\n\n\nSource: McElreath (2020, fig. 1.2)"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#statistical-test-zoo",
    "href": "slides/lecture02-1-nhst.html#statistical-test-zoo",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Statistical Test Zoo",
    "text": "Statistical Test Zoo\n\n\n\n\n\nZoo of Statistical Tests\n\n\n\nSource: McElreath (2020, fig. 1.1)\n\n\n\n\n\n\nZoo of Statistical Tests"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#multiple-comparisons",
    "href": "slides/lecture02-1-nhst.html#multiple-comparisons",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Multiple Comparisons",
    "text": "Multiple Comparisons\n\n\nIf you conduct multiple statistical tests, you must account for all of these in the p-value computation and assessment of significance.\nImportant: This includes model selection!\n\n\n\n\nMultiple Comparisons Meme\n\n\n\n\nThe core issue is the standard test statistics have the right Type I/Type II properties for each individual test, but multiple tests distort these frequencies, sometimes quite dramatically.\nFor example, suppose each individual test has a 5% Type I error rate. If you test 100 different models, and the errors are independent, you would expect 5 false positives, one of which would be selected by minimizing the p-value. The probability of at least one type I error is &gt;99%.\nThere are a number of corrections (Bonferroni being the most common), but sometimes stepwise tests are subtle, including cases of model selection followed by model fitting. You can also use simulation to estimate the false-positive rate for the procedure under a null data-generating process, which ties into the broader methods we’ll discuss."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#results-are-flashy-but-meaningless-without-methods",
    "href": "slides/lecture02-1-nhst.html#results-are-flashy-but-meaningless-without-methods",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Results Are Flashy, But Meaningless Without Methods",
    "text": "Results Are Flashy, But Meaningless Without Methods\n\nElton John Results Section Meme\nSource: Richard McElreath"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#interpretability-of-p-values-and-significance",
    "href": "slides/lecture02-1-nhst.html#interpretability-of-p-values-and-significance",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Interpretability of p-Values and Significance",
    "text": "Interpretability of p-Values and Significance\n\n\n\np-values are often confused with hypothesis probabilities or Type I error rates\np-values are a continuous measure of “surprise” at seeing a dataset given the null, but “significance” is binary.\n\n\n\n\n\nXKCD #1478\n\n\n\nSource: XKCD"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#practical-results-of-nhst",
    "href": "slides/lecture02-1-nhst.html#practical-results-of-nhst",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Practical Results of NHST",
    "text": "Practical Results of NHST\n\n\nPerhaps most damningly:\nThe null hypothesis approach, as described here and typically practiced has empirically failed to maintain rigor and credibility in the scientific literature (Ioannidis, 2005; Szucs & Ioannidis, 2017).\n\n\n\n\nHow Could Stats Do This Meme\n\n\n\nSource: Richard McElreath"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#practical-results-of-nhst-1",
    "href": "slides/lecture02-1-nhst.html#practical-results-of-nhst-1",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Practical Results of NHST",
    "text": "Practical Results of NHST\n\n\n\nOverconfident confidence intervals;\nStrawman null hypotheses;\nBiased sampling;\nLack of replications;\np-hacking.\n\n\n\n\n\nHow Could Stats Do This Meme\n\n\n\nSource: Richard McElreath"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#what-might-be-more-satisfying",
    "href": "slides/lecture02-1-nhst.html#what-might-be-more-satisfying",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "What Might Be More Satisfying?",
    "text": "What Might Be More Satisfying?\n\n\nConsideration of multiple plausible (possibly more nuanced) hypotheses.\nAssessment/quantification of evidence consistent with different hypotheses.\nIdentification of opportunities to design experiments/learn.\nInsight into the effect size."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#note-this-does-not-mean-null-hypothesis-testing-is-useless",
    "href": "slides/lecture02-1-nhst.html#note-this-does-not-mean-null-hypothesis-testing-is-useless",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Note: This Does Not Mean Null Hypothesis Testing Is Useless!",
    "text": "Note: This Does Not Mean Null Hypothesis Testing Is Useless!\n\n\nExamining and testing the implications of competing models is important, including “null” models!\n\n\n\n\nNull Hypothesis Selection Good Vs. Bad\n\n\n\n\nDeborah Mayo discusses this interpretation as “severe testing”: apply different levels of scrutiny to a scientific model to see what level of severity breaks the model.\nThe idea of a p-value (not necessarily “significance”) being a piece of inductive evidence rather than a threshold for validity (especially for an individual experiment) reflects the original views of Fisher."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#hypothesis-testing-1",
    "href": "slides/lecture02-1-nhst.html#hypothesis-testing-1",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\n\nClassical framework: Compare a null hypothesis (no effect) to an alternative (some effect)\n\\(p\\)-value: probability (under \\(H_0\\)) of more extreme test statistic than observed.\n“Significant” if \\(p\\)-value is below a significance level reflecting acceptable Type I error rate."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#problems-with-nhst-framework",
    "href": "slides/lecture02-1-nhst.html#problems-with-nhst-framework",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Problems with NHST framework",
    "text": "Problems with NHST framework\n\n\\(p\\)-values are often over-interpreted and are often be incorrectly calculated, with negative outcomes!\nImportant: “Big” data can make things worse, as NHST is highly sensitive to small but evidence effects."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#next-classes",
    "href": "slides/lecture02-1-nhst.html#next-classes",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: What is a generative model?\nNext Week+: Prob/Stats “Review” and Fundamentals"
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#assessments",
    "href": "slides/lecture02-1-nhst.html#assessments",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "Assessments",
    "text": "Assessments\nHomework 1 available; due next Friday (2/7)."
  },
  {
    "objectID": "slides/lecture02-1-nhst.html#references-scroll-for-full-list",
    "href": "slides/lecture02-1-nhst.html#references-scroll-for-full-list",
    "title": "Hypothesis Testing and Decision-Making",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nIoannidis, J. P. A. (2005). Why most published research findings are false. PLoS Med., 2, e124. https://doi.org/10.1371/journal.pmed.0020124\n\n\nMcElreath, R. (2020). Statistical rethinking : A bayesian course with examples in R and Stan (Second). Boca Raton, Florida: CRC. Retrieved from https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919\n\n\nSzucs, D., & Ioannidis, J. P. A. (2017). When null hypothesis significance testing is unsuitable for research: A reassessment. Front. Hum. Neurosci., 11, 390. https://doi.org/10.3389/fnhum.2017.00390"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#time-series-and-environmental-data",
    "href": "slides/lecture03-2-timeseries.html#time-series-and-environmental-data",
    "title": "Time Series",
    "section": "Time Series and Environmental Data",
    "text": "Time Series and Environmental Data\n\n\nMany environmental datasets involve time series: repeated observations over time:\n\\[X = \\{X_t, X_{t+h}, X_{t+2h}, \\ldots, X_{t+nh}}\\]\nMore often: ignore sampling time in notation,\n\\[X = \\{X_1, X_2, X_3, \\ldots, X_n}\\]\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Example of time series; trapped lynx populations in Canada."
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#when-do-we-care-about-time-series",
    "href": "slides/lecture03-2-timeseries.html#when-do-we-care-about-time-series",
    "title": "Time Series",
    "section": "When Do We Care About Time Series?",
    "text": "When Do We Care About Time Series?\nDependence: History or sequencing of the data matters\n\\(p(y_t) = f(y_1, \\ldots, y_{t-1})\\)\nThis is most commonly expressed through autocorrelation:\n\\(\\rho(y_t, y_{t-i}) \\neq 0\\)"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#stationarity",
    "href": "slides/lecture03-2-timeseries.html#stationarity",
    "title": "Time Series",
    "section": "Stationarity",
    "text": "Stationarity\nI.I.D. Data: All data drawn from the same distribution, \\(y_i \\sim \\mathcal{D}\\).\nEquivalent for time series \\(\\mathbf{y} = \\{y_t\\}\\) is stationarity.\n\nStrict stationarity: \\(\\{y_t, \\ldots, y_{t+k-1}\\} \\sim \\mathcal{D}\\) for all \\(t\\).\nWeak stationarity: \\(\\mathbb{E}[X_1] = \\mathbb{E}[X_t]\\) and \\(\\text{Cov}(X_1, X_k) = \\text{Cov}(X_t, X_{t+k-1})\\) for all \\(t\\)."
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#stationary-vs.-non-stationary-series",
    "href": "slides/lecture03-2-timeseries.html#stationary-vs.-non-stationary-series",
    "title": "Time Series",
    "section": "Stationary vs. Non-Stationary Series",
    "text": "Stationary vs. Non-Stationary Series"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#autoregressive-ar-models",
    "href": "slides/lecture03-2-timeseries.html#autoregressive-ar-models",
    "title": "Time Series",
    "section": "Autoregressive (AR) Models",
    "text": "Autoregressive (AR) Models\nAR(p): (autoregressive of order \\(p\\)):\n\\[\n\\begin{align*}\ny_t &= \\sum_{i=1}^p \\rho_{i} y_{t-i} + \\varepsilon \\\\\n\\varepsilon &\\sim N(0, \\sigma^2)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#uses-of-ar-models",
    "href": "slides/lecture03-2-timeseries.html#uses-of-ar-models",
    "title": "Time Series",
    "section": "Uses of AR Models",
    "text": "Uses of AR Models\nAR models are commonly used for prediction: bond yields, prices, electricity demand.\nBut may have little explanatory power: what causes the autocorrelation?"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#ar1-models",
    "href": "slides/lecture03-2-timeseries.html#ar1-models",
    "title": "Time Series",
    "section": "AR(1) Models",
    "text": "AR(1) Models\n\n\n\\[\ny_t &= {\\color{blue}\\rho} y_{t-1} + \\varepsilon \\\\\n\\varepsilon &\\sim N(0, \\sigma^2)\n\\]\n\n\n\n\n\n\n\n\n\n\n0.3\n\n\nFigure 2: AR(1) model"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#how-do-global-sea-levels-respond-to-warming",
    "href": "slides/lecture03-2-timeseries.html#how-do-global-sea-levels-respond-to-warming",
    "title": "Time Series",
    "section": "How Do Global Sea Levels Respond to Warming?",
    "text": "How Do Global Sea Levels Respond to Warming?\n\nCode\n# load sea-level data into a DataFrame\nsl_dat = DataFrame(CSV.File(joinpath(@__DIR__, \"data\", \"sealevel\", \"CSIRO_Recons_gmsl_yr_2015.csv\")))\nrename!(sl_dat, [:Year, :GMSLR, :SD]) # rename to make columns easier to work with\nsl_dat[!, :Year] .-= 0.5 # shift year to line up with years instead of being half-year \nsl_dat[!, :GMSLR] .-= mean(filter(row -&gt; row.Year ∈ 1880:1900, sl_dat)[!, :GMSLR]) # rescale to be relative to 1880-1900 mean for consistency with temperature anomaly\n\n# load temperature data\ntemp_dat = DataFrame(CSV.File(joinpath(@__DIR__, \"data\", \"climate\", \"HadCRUT.5.0.1.0.analysis.summary_series.global.annual.csv\")))\nrename!(temp_dat, [:Year, :Temp, :Lower, :Upper]) # rename to make columns easier to work with\nfilter!(row -&gt; row.Year ∈ sl_dat[!, :Year], temp_dat) # reduce to the same years that we have SL data for\ntemp_normalize = mean(filter(row -&gt; row.Year ∈ 1880:1900, temp_dat)[!, :Temp]) # get renormalization to rescale temperature to 1880-1900 mean\ntemp_dat[!, :Temp] .-= temp_normalize\ntemp_dat[!, :Lower] .-= temp_normalize\ntemp_dat[!, :Upper] .-=  temp_normalize\n\nsl_plot = plot(sl_dat[!, :Year], sl_dat[!, :GMSLR], yerr=sl_dat[!, :SD], color=:black, label=\"Observations\", ylabel=\"(mm)\", xlabel=\"Year\", title=\"Sea Level Anomaly\")\nplot!(sl_plot, size=(600, 450))\n\ntemp_plot = plot(temp_dat[!, :Year], temp_dat[!, :Temp], yerr=(temp_dat[!, :Temp] - temp_dat[!, :Lower], temp_dat[!, :Upper] - temp_dat[!, :Temp]), color=:black, label=\"Observations\", ylabel=\"(°C)\", xlabel=\"Year\", title=\"Temperature\")\nplot!(temp_plot, size=(600, 450))\n\ndisplay(sl_plot)\ndisplay(temp_plot)\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Sea level data from CSIRO and temperature data from HadCRUT5.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 3"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#does-regressing-slr-on-temperature-make-sense",
    "href": "slides/lecture03-2-timeseries.html#does-regressing-slr-on-temperature-make-sense",
    "title": "Time Series",
    "section": "Does Regressing SLR on Temperature Make Sense?",
    "text": "Does Regressing SLR on Temperature Make Sense?\nWhy might temperature exert an influence on sea levels?"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#processes-contributing-to-slr",
    "href": "slides/lecture03-2-timeseries.html#processes-contributing-to-slr",
    "title": "Time Series",
    "section": "Processes Contributing to SLR",
    "text": "Processes Contributing to SLR\n\nSea Level Processes\nSource: Milne et al. (2009)"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#why-model-total-slr",
    "href": "slides/lecture03-2-timeseries.html#why-model-total-slr",
    "title": "Time Series",
    "section": "Why Model “Total” SLR?",
    "text": "Why Model “Total” SLR?\n\nSea Level Budget\nSource: Fox-Kemper et al. (2021)"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#linear-regression-specification",
    "href": "slides/lecture03-2-timeseries.html#linear-regression-specification",
    "title": "Time Series",
    "section": "Linear Regression Specification",
    "text": "Linear Regression Specification\nStandard assumption of independent normal residuals:\n\n\n\\[\n\\begin{align*}\n\\text{SLR}_t &\\sim \\beta_0 + \\beta_1 \\text{x}_t + \\varepsilon \\\\\n\\varepsilon & \\sim N(0, \\sigma^2)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#ols-estimate",
    "href": "slides/lecture03-2-timeseries.html#ols-estimate",
    "title": "Time Series",
    "section": "OLS Estimate",
    "text": "OLS Estimate\n\n\n\\[\n\\begin{align*}\n\\hat{\\beta_1} &= \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2} \\\\\n&= 176 \\text{mm}/^\\circ \\text{C} \\\\\n\\hat{\\beta_0} &= \\bar{y} - \\hat{\\beta_1}\\bar{x} = 27 \\text{mm} \\\\\n\\hat{\\sigma} &= \\sqrt{\\frac{SSE}{n-1}} = 27 \\text{mm}\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#stationarity-1",
    "href": "slides/lecture03-2-timeseries.html#stationarity-1",
    "title": "Time Series",
    "section": "Stationarity",
    "text": "Stationarity\nFor independent data \\(y = \\{y_i\\}\\),"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#next-classes",
    "href": "slides/lecture03-2-timeseries.html#next-classes",
    "title": "Time Series",
    "section": "Next Classes",
    "text": "Next Classes\nNext Week+: Prob/Stats “Review” and Fundamentals"
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#assessments",
    "href": "slides/lecture03-2-timeseries.html#assessments",
    "title": "Time Series",
    "section": "Assessments",
    "text": "Assessments\nHomework 1 available; due next Friday (2/7)."
  },
  {
    "objectID": "slides/lecture03-2-timeseries.html#references-scroll-for-full-list",
    "href": "slides/lecture03-2-timeseries.html#references-scroll-for-full-list",
    "title": "Time Series",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nFox-Kemper, B., Hewitt, H. T., Xiao, C., Aðalgeirsdóttir, G., Drijfhout, S. S., Edwards, T. L., et al. (2021). Ocean, Cryosphere and Sea Level Change. In A. Pirani, S. L. Connors, C. Péan, S. Berger, N. Caud, Y. Chen, et al. (Eds.), Climate Change 2021: The Physical Science Basis. Contribution of Working Group I to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change (pp. 1211–1362). Cambridge, United Kingdom; New York, NY, USA: Cambridge University Press. https://doi.org/10.1017/9781009157896.011\n\n\nMilne, G. A., Gehrels, W. R., Hughes, C. W., & Tamisiea, M. E. (2009). Identifying the causes of sea-level change. Nat. Geosci., 2, 471–478. https://doi.org/10.1038/ngeo544"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BEE 4850/5850: Environmental Data Analysis and Simulation",
    "section": "",
    "text": "This is the course website for the Spring 2025 edition of BEE 4850/5850, Environmental Data Analysis and Simulation, taught at Cornell University by Vivek Srikrishnan.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "BEE 4850/5850: Environmental Data Analysis and Simulation",
    "section": "Course Description",
    "text": "Course Description\nThis class teaches data analysis with an emphasis on conceptual models for scientific insights. In particular, we will:\n\nlearn how to construct generative probability models for common environmental datasets;\ndevelop a toolkit for model calibration, uncertainty quantification, and simulation;\nuse cross-validation and predictive information criteria to evaluate over-fitting and test hypotheses;\nidentify potential confounds and explore implications for data gathering and experimental design.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#course-information",
    "href": "index.html#course-information",
    "title": "BEE 4850/5850: Environmental Data Analysis and Simulation",
    "section": "Course Information",
    "text": "Course Information\n\nDetails on the class and course policies are provided in the syllabus.\nTopics and weekly materials can be found in the schedule.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "index.html#regular-weekly-schedule",
    "href": "index.html#regular-weekly-schedule",
    "title": "BEE 4850/5850: Environmental Data Analysis and Simulation",
    "section": "Regular Weekly Schedule",
    "text": "Regular Weekly Schedule\n\n\n\n\n\n\n\nLectures\nMW 11:40-12:55pm, 160 Riley-Robb Hall\n\n\nOffice Hours\nInstructor: MW 1:00-2:00pm, 318 Riley-Robb Hall, TA: MTh 10:30-11:30, 319 Riley-Robb Hall\n\n\nAssignments\nDue Fridays at 9pm on Gradescope.",
    "crumbs": [
      "Course Information",
      "Overview"
    ]
  },
  {
    "objectID": "resources/ai.html",
    "href": "resources/ai.html",
    "title": "Generative AI and LLMs",
    "section": "",
    "text": "TL; DR: My recommendation is that you do not use ChatGPT, NotebookLLM, or similar large language model (LLM) tools in this class for substantive tasks (annotating/summarizing readings, writing solutions), though they can be useful for programming tasks. If you do use these tools, please do so after independent effort, and clearly document how you’ve used them.\nI am not opposed to the thoughtful use of LLMs. Contrary to the endless hype and marketing served by their creators and the media, these tools have strong limitations and over-reliance on them can greatly impede the discovery and learning processes. However, once you understand these limitations and have enough domain knowledge to look for red flags, they can be useful for certain (albeit limited) tasks, and learning how to engage with them responsibly is a legitimate professional skill.\nThe framing and policies on this page are heavily influenced by Andrew Heiss.",
    "crumbs": [
      "Resources",
      "Generative AI and LLMs"
    ]
  },
  {
    "objectID": "resources/ai.html#bullshit",
    "href": "resources/ai.html#bullshit",
    "title": "Generative AI and LLMs",
    "section": "Bullshit",
    "text": "Bullshit\nThe fundamental problem with LLMs is that they are bullshit generators (Hannigan et al., 2024; Hicks et al., 2024). Bullshit, in the philosophical sense, is text produced without care for the truth (Frankfurt, 2005). It is not a lie, which specifically exists in opposition of truth, or a mistake, which is subject to correction when exposed to divergence from the truth, but it is agnostic to the truth; it simply exists to make the author sound like an authority. Truth simply does not matter to a bullshitter.\n\nHannigan, T. R., McCarthy, I. P., & Spicer, A. (2024). Beware of botshit: How to manage the epistemic risks of generative chatbots. Bus. Horiz., 67, 471–486. https://doi.org/10.1016/j.bushor.2024.03.001\n\nHicks, M. T., Humphries, J., & Slater, J. (2024). ChatGPT is bullshit. Ethics Inf. Technol., 26, 1–10. https://doi.org/10.1007/s10676-024-09775-5\n\nFrankfurt, H. G. (2005). On bullshit. Princeton, NJ: Princeton University Press.\nLLMs literally exist only to produce bullshit. They use a predictive statistical model to guess what next word is likely; there is no reference to whether the underlying idea produced by this sequence of words is truthful or even coherent. This means that, once an LLM goes off track, they are subject to wild hallucinations where they may invent concepts or artifacts (books, articles, etc) that do not exist, merely because they seem plausible as a string of text. Given that LLMs are trained on publicly available text, an increasing amount of which is now generated by LLMs (so-called “AI slop”), the uncritical use of these results can just perpetuate the bullshit cycle.\nAs we are environmental scientists and engineers, there’s another problem, which is the impact on the environment of the computers needed to train and run LLMs. The more judicious we can be with these tools, the better we can manage their energy and water needs for when they’re actually useful.",
    "crumbs": [
      "Resources",
      "Generative AI and LLMs"
    ]
  },
  {
    "objectID": "resources/ai.html#writing-and-reading",
    "href": "resources/ai.html#writing-and-reading",
    "title": "Generative AI and LLMs",
    "section": "Writing And Reading",
    "text": "Writing And Reading\nSince LLMs only produce bullshit, they cannot help you with the process of writing or engaging with readings. Writing, whether prose or technical solutions, forces you to clarify your ideas and confront where they are vague or half-baked. This is a critical part of the educational experience! Producing plausible-looking but substantively-empty text cannot achieve this goal.\nIf you have written your own initial text but would like to clean it up (grammar, concision, etc), LLMs may be helpful since the substance is already present in your text. Just make sure to carefully edit it to ensure that your ideas are still present and clear. This type of engagement with LLM output can be useful and reflects a similar need to engage with the statistical models and methods we are working with in class.\nIf you do use an LLM at some part in your writing process, you should cite it and make clear how you engaged with the output. This includes:\n\nWhat prompt(s) did you use?\nHow did the LLM output influence your writing or framing?\n\nThis not only makes it clear to me whether you used the LLM responsibly (otherwise, this borders on plagiarism), but it actually helps you in case there is some bullshit that made it into your answer that is not actually a reflection of your understanding. I will not bother trying to guess if your writing is AI-generated1. Your work will be graded on its own merits, and since we’re looking for thoughtfulness and engagement in your written work, LLM-generated materials are likely to be penalized2\n1 While there are tools that purport to do this, they do not reliably work.2 And if you did not disclose the role of LLMs in generating the work, this will not be a convincing reason for why you should not lose points.",
    "crumbs": [
      "Resources",
      "Generative AI and LLMs"
    ]
  },
  {
    "objectID": "resources/ai.html#coding",
    "href": "resources/ai.html#coding",
    "title": "Generative AI and LLMs",
    "section": "Coding",
    "text": "Coding\nUsing LLMs for programming is a little different. There are many programming tasks (autocompletion of syntax, interpreting error messages) that are greatly facilitated by the use of LLM tools such as ChatGPT or GitHub CoPilot. Even people who already know what they’re doing tend to solve syntax and debugging problems by Googling or going to forums like Stack Overflow. LLMs are a shorcut for this approach.\nHowever, if you’re trying to learn how to program (or program in a new language or using a new toolkit), the use of LLMs, even for debugging, can be greatly detrimental to this process. As they training data for LLMs are often didactic examples, the output code is often wildly inefficient3. There are also likely to be errors due to the generation mechanism: all the LLM can do is guess what the next line of code is, not reason about whether the overall logic of the code makes sense or if it will run. It’s hard to track down these errors if you played no role in the development of the code. This is particularly true if you’re dependent on an LLM to think through debugging, since you won’t know how to find where the LLM.\n3 This is not going to be a problem in our class, but might be in your future.My suggestion for how to use LLMs for coding are:\n\nTry to write your own code first. At the very least, think through the logic of what a solution would look like and write down any relevant equations. Then try to write down a version of that in code form. It’s okay to use syntax-checking and autocomplete tools here, but try to think about what the command is and look at some documentation (or ask on Ed Discussion) if it’s not clear to you.\nIf you run into errors, first see if they’re obvious. In Julia, for example, many errors are the result of not using broadcasting. Being able to spot these common error messages is useful and fast.\nIf you run into further errors, or cannot tell why a particular piece of code is not working, then feel free to use an LLM4. Just make sure you don’t just copy and paste output, but, if the code works, try to understand how it differed from your own so you can not make the same mistakes next time.\n\n\n\n4 Appropriately documented, of course.",
    "crumbs": [
      "Resources",
      "Generative AI and LLMs"
    ]
  },
  {
    "objectID": "resources/github.html",
    "href": "resources/github.html",
    "title": "git and GitHub",
    "section": "",
    "text": "You’ll need to create a https://github.com account for this course, if you don’t already have one. While the use of GitHub is not required (you can work on the homework assignments without touching the GitHub Classroom links), knowing the version control workflow is a useful skill to have, and we’ll ask for a link to a GitHub repository over sharing files directly if you have a question about code on e.g. a homework assignment.\nThe following basic commands are all the git you will need: * git clone &lt;github-repository-url&gt;: This is needed to “clone” your assignment repository (initialize your local repository). * git commit -m &lt;message&gt;: This is used to “lock in” changes that you’ve made to your files. You should make commits frequently as you make changes so you can revert to prior versions if something goes wrong (and make your messages meaningful so you know what changes you’ve made!) * git push: This syncs any committed changes to the remote GitHub repository. You must do this prior to using your repository to ask for help.\nFor some additional resources about git and GitHub:\n\nGit Basics from The Odin Project.\nLearn Git Branching: An interactive, visual tutorial to how git works.\nVersion Control from MIT’s “CS: Your Missing Semester” course.\nGit and GitHub for Poets: YouTube playlist covering the basics of git and GitHub.",
    "crumbs": [
      "Resources",
      "git and GitHub"
    ]
  },
  {
    "objectID": "weeks/week02.html",
    "href": "weeks/week02.html",
    "title": "Week 2 Information",
    "section": "",
    "text": "Null Hypothesis Testing and Statistical Significance\nProbability Fundamentals",
    "crumbs": [
      "Weeks",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#lectures",
    "href": "weeks/week02.html#lectures",
    "title": "Week 2 Information",
    "section": "",
    "text": "Null Hypothesis Testing and Statistical Significance\nProbability Fundamentals",
    "crumbs": [
      "Weeks",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#homework",
    "href": "weeks/week02.html#homework",
    "title": "Week 2 Information",
    "section": "Homework",
    "text": "Homework\n\nHomework 1 due 2/7/25 by 9PM.",
    "crumbs": [
      "Weeks",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week02.html#readings",
    "href": "weeks/week02.html#readings",
    "title": "Week 2 Information",
    "section": "Readings",
    "text": "Readings\n\nLloyd, E. A., & Oreskes, N. (2018). Climate change attribution: When is it appropriate to accept new methods? Earth’s Future, 6(3), 311–325. https://doi.org/10.1002/2017ef000665\nAs you’re reading, please use the collaborative annotation in Canvas to make notes and read and respond to others’ thoughts and questions. We will discuss the paper in the 02/03 class period. You will be graded based on annotation and participation in the discussion.\nIn particular, please focus on:\n\nHow does the framing of climate change attribution relate to the Type I/Type II error framework we discussed in class?\nWhat are the implications of the storyline approach for statements about climate attribution of extreme events?\nWhat are your thoughts on the storyline vs. FAR approach for scientific insights and communication of climate impacts?",
    "crumbs": [
      "Weeks",
      "Week 2"
    ]
  },
  {
    "objectID": "weeks/week01.html",
    "href": "weeks/week01.html",
    "title": "Week 1 Information",
    "section": "",
    "text": "Welcome to BEE 4850/5850! Make sure to read the syllabus and make sure you’re enrolled in Gradescope and Ed Discussion (particularly if you added the class after 1/20).",
    "crumbs": [
      "Weeks",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#announcements",
    "href": "weeks/week01.html#announcements",
    "title": "Week 1 Information",
    "section": "",
    "text": "Welcome to BEE 4850/5850! Make sure to read the syllabus and make sure you’re enrolled in Gradescope and Ed Discussion (particularly if you added the class after 1/20).",
    "crumbs": [
      "Weeks",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week01.html#homework",
    "href": "weeks/week01.html#homework",
    "title": "Week 1 Information",
    "section": "Homework",
    "text": "Homework\n\nIf you don’t have a GitHub account, make sure you set one up.\nHomework 1 assigned, due 2/7/25. Check Ed Discussion for the link to create your repository. You will be prompted to link your GitHub account to the class roster, which will not be required once you do it once.",
    "crumbs": [
      "Weeks",
      "Week 1"
    ]
  },
  {
    "objectID": "weeks/week03.html",
    "href": "weeks/week03.html",
    "title": "Week 3 Information",
    "section": "",
    "text": "Probability Models for Data\nTime Series",
    "crumbs": [
      "Weeks",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#lectures",
    "href": "weeks/week03.html#lectures",
    "title": "Week 3 Information",
    "section": "",
    "text": "Probability Models for Data\nTime Series",
    "crumbs": [
      "Weeks",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#homework",
    "href": "weeks/week03.html#homework",
    "title": "Week 3 Information",
    "section": "Homework",
    "text": "Homework\n\nHomework 1 due 2/7/25 by 9PM.",
    "crumbs": [
      "Weeks",
      "Week 3"
    ]
  },
  {
    "objectID": "weeks/week03.html#readings",
    "href": "weeks/week03.html#readings",
    "title": "Week 3 Information",
    "section": "Readings",
    "text": "Readings\n\nShmueli, G. (2010). To explain or to predict? Statistical Science: A Review Journal of the Institute of Mathematical Statistics, 25(3), 289–310. https://doi.org/10.1214/10-STS330\nAs you’re reading, please use the collaborative annotation in Canvas to make notes and read and respond to others’ thoughts and questions. We will discuss the paper in the 02/10 class period. You will be graded based on annotation and participation in the discussion.\nIn particular, please focus on:\n\nWhat differentiates the explanation vs. predictive modeling paradigms? How does this relate to the choices we’ve discussed that are involved in statistical modeling?\nWhat are the dangers in conflating explanation and prediction?\nIs it possible to fully separate explanation and prediction? Why or why not?\n\nThis paper is quite broad, and includes discussion of topics that we will not cover until later in the semester (such as model validation and selection). It’s ok to gloss over these, particularly if the terminology is unknown to you, but keep them in mind as we move forward!",
    "crumbs": [
      "Weeks",
      "Week 3"
    ]
  },
  {
    "objectID": "hw/hw01/hw01.html",
    "href": "hw/hw01/hw01.html",
    "title": "Homework 1: Thinking About Data",
    "section": "",
    "text": "Due Date\n\n\n\nFriday, 2/7/25, 9:00pm",
    "crumbs": [
      "Homework",
      "Homework 01"
    ]
  },
  {
    "objectID": "hw/hw01/hw01.html#overview",
    "href": "hw/hw01/hw01.html#overview",
    "title": "Homework 1: Thinking About Data",
    "section": "Overview",
    "text": "Overview\n\nInstructions\nThe goal of this homework assignment is to introduce you to simulation-based data analysis.\n\nProblem 1 asks you to find an example of a spurious correlation.\nProblem 2 asks you to explore whether a difference between data collected from two groups might be statistically meaningful or the result of noise. This problem repeats the analysis from Statistics Without The Agonizing Pain by John Rauser (which is a neat watch!).\nProblem 3 asks you to explore the impacts of selection biases on statistical relationships.\nProblem 4 asks you to evaluate an interview method for finding the level of cheating on a test to determine whether cheating was relatively high or low. This problem was adapted from Bayesian Methods for Hackers.\nProblem 5 (only required for students in BEE 5850) asks you to propose several causal models for the same observational phenomenon.\n\n\n\nLoad Environment\nThe following code loads the environment and makes sure all needed packages are installed. This should be at the start of most Julia scripts.\n\n\nimport Pkg\nPkg.activate(@__DIR__)\nPkg.instantiate()\n\n\nThe following packages are included in the environment (to help you find other similar packages in other languages). The code below loads these packages for use in the subsequent notebook (the desired functionality for each package is commented next to the package).\n\n\nusing Random # random number generation and seed-setting\nusing DataFrames # tabular data structure\nusing CSVFiles # reads/writes .csv files\nusing Distributions # interface to work with probability distributions\nusing Plots # plotting library\nusing StatsBase # statistical quantities like mean, median, etc\nusing StatsPlots # some additional statistical plotting tools",
    "crumbs": [
      "Homework",
      "Homework 01"
    ]
  },
  {
    "objectID": "hw/hw01/hw01.html#problems",
    "href": "hw/hw01/hw01.html#problems",
    "title": "Homework 1: Thinking About Data",
    "section": "Problems",
    "text": "Problems\n\nScoring\nEach problem is worth 5 points.\n\n\nProblem 1\nFind an example of a spurious correlation (don’t just pull from Spurious Correlations!!).\nIn this problem:\n\nDescribe and illustrate the correlation;\nExplain why you think the correlation is spurious.\n\n\n\nProblem 2\nThe underlying question we would like to address is: what is the influence of drinking beer on the likelihood of being bitten by mosquitoes? There is a mechanistic reason why this might occur: mosquitoes are attracted by changes in body temperature and released CO2, and it might be that drinking beer induces these changes. We’ll analyze this question using (synthetic) data which separates an experimental population into two groups, one which drank beer and the other which drank only water.\nFirst, we’ll load data for the number of bites reported by the participants who drank beer. This is in a comma-delimited file, data/bites.csv (which is grossly overkill for this assignment). Each row contains two columns: the group (beer and water) the person belonged to and the number of times that person was bitten.\nIn Julia, we can do this using CSVFiles.jl, which will read in the .csv file into a DataFrame, which is a typical data structure for tabular data (and equivalent to a Pandas DataFrame in Python or a dataframe in R).\n\n\ndata = DataFrame(load(\"data/bites.csv\")) # load data into DataFrame\n\n# print data variable (semi-colon suppresses echoed output in Julia, which in this case would duplicate the output)\n@show data;\n\n\nHow can we tell if there’s a meaningful difference between the two groups? Naively, we might just look at the differences in group means.\n\n\n\n\n\n\n\nBroadcasting\n\n\n\nThe subsetting operations in the below code use .==, which “broadcasts” the element-wise comparison operator == across every element. The decimal in front of == indicates that this should be used element-wise (every pair of elements compared for equality, returning a vector of true or false values); otherwise Julia would try to just check for vector equality (returning a single true or false value).\nBroadcasting is a very specific feature of Julia, so this syntax would look different in a different programming language.\n\n\n\n# split data into vectors of bites for each group\nbeer = data[data.group .== \"beer\", :bites]\nwater = data[data.group .== \"water\", :bites]\n\nobserved_difference = mean(beer) - mean(water)\n@show observed_difference;\n\nobserved_difference = 4.37777777777778\n\n\n\nThis tells us that, on average, the participants in the experiment who drank beer were bitten approximately 4.4 more times than the participants who drank water! Does that seem like a meaningful difference, or could it be the result of random chance?\nWe will use a simulation approach to address this question, as follows.\n\nSuppose someone is skeptical of the idea that drinking beer could result in a higher attraction to mosquitoes, and therefore more bites. To this skeptic, the two datasets are really just different samples from the same underlying population of people getting bitten by mosquitoes, rather than two different populations with different propensities for being bitten. This is the skeptic’s hypothesis, versus our hypothesis that drinking beer changes body temperature and CO2 release sufficiently to attract mosquitoes.\nIf the skeptic’s hypothesis is true, then we can “shuffle” all of the measurements between the two datasets and re-compute the differences in the means. After repeating this procedure a large number of times, we would obtain a distribution of the differences in means under the assumption that the skeptic’s hypothesis is true.\nComparing our experimentally-observed difference to this distribution, we can then evaluate the consistency of the skeptic’s hypothesis with the experimental results.\n\n\n\n\n\n\n\n\nWhy Do We Call This A Simulation-Based Approach?\n\n\n\nThis is a simulation-based approach because the “shuffling” is a non-parametric way of generating new samples from the underlying distribution (more on this later!).\nThe alternative to this approach is to use a statistical test, such as a t-test, which may have other assumptions which may not be appropriate for this setting, particularly given the seemingly small sample sizes.\n\n\n\nIn this problem:\n\nConduct the above procedure to generate 50,000 simulated datasets under the skeptic’s hypothesis.\nPlot a histogram of the results and add a dashed vertical line to show the experimental difference (if you are using Julia, feel free to look at the Making Plots with Julia tutorial on the class website).\nDraw conclusions about the plausibility of the skeptic’s hypothesis that there is no difference between groups. Feel free to use any quantitative or qualitative assessments of your simulations and the observed difference.\n\n\n\nProblem 3\nA scientific funding agency receives 200 proposals in response to a call. The panel is asked to evaluate each proposal on two criteria: scientific rigor and potential impact (or “newsworthiness”). After standardizing each of these scores to independently follow a standard normal distribution (\\(\\text{Normal}(0, 1)\\)), the two standardized scores are summed to get the total score for the proposal. Based on these total scores, the top 10% of the proposals are selected for funding.\nA researcher who is studying the relationship between rigor and impact has used data on the funded proposals to claim high-impact proposals are necessarily less rigorous, and indeed found a statistically significant negative correlation between the rigor and impact scores for the funded proposals. You are more skeptical and believe this effect is an artifact from the selection process, which would make the claim a bit ironic.\nIn this problem:\n\nCreate a generative model for the grant-selection procedure under the null assumption of no correlation between rigor and impact. You can sample rigor and impact scores directly from \\(\\text{Normal}(0, 1)\\).\nUsing 1,000 simulations, compare the distribution of correlations between rigor and impact for the funded proposals to the general proposal population.\nExplain why, when conditioning on selection for funding, there might be a negative correlation between rigor and impact even when none generally exists.\n\n\n\n\n\n\n\n\nSelection-Distortion Effects\n\n\n\nThese selection-distortion effects are pretty common in observational data, going back to work by Dawes in the 1970s on the lack of predictive ability of admission variables on student success and including studies claiming to identify a causal fingerprint of genes on outcomes.\nThis is, of course, just one example of how not thinking carefully about data-generating processes can fundamentally contaminate statistical analyses, emphasizing that data do not have meaning absent a model for how they were generated.\n\n\n\nProblem 4\nYou are trying to detect how prevalent cheating was on an exam. You are skeptical of the efficacy of just asking the students if they cheated. You are also concerned about privacy — your goal is not to punish individual students, but to see if there are systemic problems that need to be addressed. Someone proposes the following interview procedure, which the class agrees to participate in:\n\nEach student flips a fair coin, with the results hidden from the interviewer. The student answers honestly if the coin comes up heads. Otherwise, if the coin comes up tails, the student flips the coin again, and answers “I did cheat” if heads, and “I did not cheat”, if tails.\n\nWe have a hypothesis that cheating was not prevalent, and the proportion of cheaters was no more than 5% of the class; in other words, we expect 5 “true” cheaters out of a class of 100 students. Our TA is more jaded and thinks that cheating was more rampant, and that 30% of the class cheated. The proposed interview procedure is noisy: the interviewer does not know if an admission means that the student cheated, or the result of a heads. However, it gives us a data-generating process that we can model and analyze for consistency with our hypothesis and that of the TA.\nIn this problem:\n\nDerive and code a simulation model for the above interview procedure given the “true” probability of cheating \\(p\\).\nSimulate your model (for a class of 100 students) 50,000 times under a null hypothesis of no cheating, your hypothesis of 5% cheating, the TA’s hypothesis of 30% cheating, and plot the resulting datasets.\nIf you received 31 “Yes, I cheated” responses while interviewing your class, what could you conclude?\nHow useful do you think the interview procedure is to identify systemic cheating? What changes to the design might you make?\n\n\n\nProblem 5\nThis problem is only required for students in BEE 5850.\nAdjusting for inflation, there has been an increase in tropical cyclone damages on the U.S. Atlantic coast between 1900 and 2022 (see Figure 1).\n\n\n\n\n\n\n\nFigure 1: Change in tropical cyclone damages in U.S. Atlantic coastal counties from 1900–2022, aggregated by pentad and normalized to 2018 U.S. dollars. The red line shows an exponential trend of 1.06%/yr growth in damages. Reproduced from Willoughby, Hernandez, & Pinnock (2024).\n\n\n\n\nIn this problem:\n\nDescribe two different mechanisms by which tropical cyclone damages in coastal counties might have increased over this period.\nHow might you propose to examine the relative strengths of these mechanisms? You don’t need to actually do this (this is indeed a somewhat thorny debate in the literature), but think about potential modeling and data approaches.",
    "crumbs": [
      "Homework",
      "Homework 01"
    ]
  },
  {
    "objectID": "hw/hw01/hw01.html#references",
    "href": "hw/hw01/hw01.html#references",
    "title": "Homework 1: Thinking About Data",
    "section": "References",
    "text": "References",
    "crumbs": [
      "Homework",
      "Homework 01"
    ]
  },
  {
    "objectID": "homework.html",
    "href": "homework.html",
    "title": "Homework Assignments",
    "section": "",
    "text": "This page contains information about and a schedule of the homework assignments for the semester.",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "homework.html#general-information",
    "href": "homework.html#general-information",
    "title": "Homework Assignments",
    "section": "General Information",
    "text": "General Information\n\nWhile the instructions for each assignment are available through the linked pages for quick and public access, if you are in the class you must use the link provided in Ed Discussion to accept the assignment. This will ensure that:\n\nYou have compatible versions of all relevant packages provided in the environment;\nYou have a GitHub repository that you can use to share your code.\n\nSubmit assignments by 9:00pm Eastern Time on the due date on Gradescope.\nSubmissions must be PDFs. Make sure that you tag the pages corresponding to each question; points will be deducted otherwise.",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "homework.html#rubric",
    "href": "homework.html#rubric",
    "title": "Homework Assignments",
    "section": "Rubric",
    "text": "Rubric\nThe standard rubric for homework problems is available here.",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "homework.html#schedule",
    "href": "homework.html#schedule",
    "title": "Homework Assignments",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\n\n\nAssignment\nInstructions\nDue Date\nSolutions\n\n\n\n\nHW1\n\nFeb 07, 2025\n\n\n\nHW2\n\nFeb 21, 2025\n\n\n\nHW3\n\nMar 14, 2025\n\n\n\nHW4\n\nMar 28, 2025\n\n\n\nHW5\n\nApr 18, 2025\n\n\n\nHW6\n\nMay 2, 2025",
    "crumbs": [
      "Homework"
    ]
  },
  {
    "objectID": "resources/homework.html",
    "href": "resources/homework.html",
    "title": "Homework Policies",
    "section": "",
    "text": "This page includes some information on the homework assignments for BEE 4850/5850, including policies and logistics. The goal is to help you get as many points on your homework as possible and to return them as soon as possible.\nThis document is long, but please do read the whole thing. Hopefully most of this is obvious, but some of it may not be.",
    "crumbs": [
      "Resources",
      "Homework Policies"
    ]
  },
  {
    "objectID": "resources/homework.html#homework-logistics",
    "href": "resources/homework.html#homework-logistics",
    "title": "Homework Policies",
    "section": "Homework Logistics",
    "text": "Homework Logistics\nHomework assignments will be assigned on Monday and are generally due two Fridays hence (so you have two “school weeks” to work on the assignment). Solutions should be submitted as PDFs to Gradescope by 9pm on the due date. You do not need to include code unless you want to, but you should always include some description of the logic of the modeling or problem-solving process that your code implements.",
    "crumbs": [
      "Resources",
      "Homework Policies"
    ]
  },
  {
    "objectID": "resources/homework.html#how-to-write-assignments",
    "href": "resources/homework.html#how-to-write-assignments",
    "title": "Homework Policies",
    "section": "How To Write Assignments",
    "text": "How To Write Assignments\nHere are some tips for how to make the grader (the TA or Prof. Srikrishnan) understand what you mean in the time they’re looking at your problem: as noted in the rubrics, if your solution isn’t clear enough for us to follow, you will not receive the points even if your answer is technically correct, because part of demonstrating understanding is the ability to organize and communicate.\n\nBe Honest\n\nWrite everything in your own words. It’s perfectly ok to work with others; in fact, we encourage it! But you should write up and implement everything yourself; it’s very easy to convince yourself that you understand something when you’re mimicking someone else’s solution, and often we realize we don’t actually understand something when we try to write it ourselves.\nCite every outside source you use. You are allowed, in fact encouraged, to use any outside source1 Getting an idea from an outside source is not a problem, and will not lower your grade; if you’re critically evaluating the idea and implementing and writing your solution yourself (see above), then you’re demonstrating understanding even if the idea originated with someone else. But you must give appropriate credit to that source. Taking credit for someone else’s ideas and failing to properly cite an outside source, including your classmates, is plagiarism, and will be treated accordingly.\nThe only sources you do not have to cite are official class materials. If you use the lectures, lecture notes, website materials, homework solutions, etc, you do not have to cite these.\nList everyone that you worked with. Give your classmates proper credit for their assistance. If you get an idea from Ed Discussion, credit the poster. If you’re not sure if you should list someone as a collaborator, err on the side of including them. For discussions in class or in office hours, you don’t have to list everyone who participated in the discussion (though you should if you worked one on one with them), but mention that the class discussion or the office hour was useful.\n\n1 Yes, including ChatGPT, though you should ask and describe how you used it.\n\nBe Clear\n\nWrite legibly. This doesn’t refer to handwriting (since you’ll be submitting PDFs of Jupyter notebooks), but the text itself should be clearly written and well organized, so that it’s easy for the grader to follow your reasoning or explanation. Structuring your solution, and not writing in a stream of consciousness, helps you think more clearly. To reiterate: You will be given no points if the grader cannot easily follow your answer, and the graders have complete discretion to determine this.\nWrite clearly. Use proper spelling, grammar, logic, etc. We will try not to penalize people for not having complete mastery of English, but again, we need to be able to follow your reasoning.\nWrite carefully and completely. We can only grade what you write; nobody can read your mind, and we will not try. The solution that you submit must stand on its own. If your answer is ambiguous, the TA has been instructed to interpret it the wrong way. Regrade requests also cannot be used to add more information to a solution.\nDon’t submit your first draft. For most people, first drafts are terrible. They are often poorly organized, unclear, and contain gaps or jumps in reasoning. You will likely need to revise, possibly several times, for your answer to be clear, careful, and complete. This is another reason to start the assignment early — if you start late, you may be stuck with your first draft, and your grade is likely to suffer for it.\nState your assumptions. If you think a problem statement is ambiguous and your solution depends on a particular interpretation, or you need to make some assumptions to solve the problem, make it explicit (though do also ask for clarification in class or on Ed Discussion).\nDon’t rely on your code. The TA will not try to scrutinize your code, which is a waste of time (again, the code is a means to an end — it works or it doesn’t). If you make it clear what your code is supposed to do, then the TA can tell if this logic is correct and any mistake must be something minor in the code. If you just provide code (even if commented), the TA can’t do this without running your code and debugging, which is not a valuable use of time. We want to focus on your ideas.\n\n\n\nBe Concise\n\nKeep solutions short. Organized answers should not be long. There’s a fine balance between conciseness and completeness: find it!\nDon’t regurgitate. You can reference concepts, models, etc from class without repeating them. Just make it clear what you’re modifying and how you’re using those concepts for that particular problem.\nDon’t bullshit. You will get no points for word salad, even if you accidentally hit on the right answer.",
    "crumbs": [
      "Resources",
      "Homework Policies"
    ]
  },
  {
    "objectID": "resources/data.html",
    "href": "resources/data.html",
    "title": "Data",
    "section": "",
    "text": "There are a number of places you can look to find interesting datasets.\n\nData is Plural: A weekly newsletter from Jèremy Singer-Vine. Not just (or typically) environmental data, but interesting nevertheless.\nGoogle Dataset Search\nClimate Trace: Emissions tracking data.\nOECD Environmental Data\nOur World in Data\nResource Watch\nEPA Data Catalog\nU.S. Government Open Data Portal",
    "crumbs": [
      "Resources",
      "Data Sources"
    ]
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#types-of-uncertainty",
    "href": "slides/lecture04-1-bayes.html#types-of-uncertainty",
    "title": "Bayesian Statistics",
    "section": "Types of Uncertainty",
    "text": "Types of Uncertainty\n\n\n\n\n\n\n\n\n\n\n\nUncertainty Type\nSource\nExample(s)\n\n\n\n\nAleatory uncertainty\nRandomness\nDice rolls, Instrument imprecision\n\n\nEpistemic uncertainty\nLack of knowledge\nClimate sensitivity, Premier League champion\n\n\n\n\n\n\n\nWhich Uncertainty Type Meme\n\n\n\n\n\nNote that the distinction between aleatory and epistemic uncertainty is somewhat arbitrary (aside from maybe some quantum effects). For example, we often think of coin tosses as aleatory, but if we had perfect information about the toss, we might be able to predict the outcome with less uncertainty. There’s a famous paper by Persi Diaconis where he collaborated with engineers to build a device which could arbitrary bias a “fair” coin toss.\nBut in practice, this doesn’t really matter: the key thing is whether for a given model we’re treating the uncertainty as entirely random (e.g. white noise) versus being interested in the impacts of that uncertainty on the outcome of interest. And there’s a representation theorem by the Bayesian actuary Bruno de Finetti which shows that, under a condition called exchangeability, we can think of any random sequence as arising from an independent and identically distributed process, so the practical difference can collapse further."
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#frequentist-vs-bayesian-probability",
    "href": "slides/lecture04-1-bayes.html#frequentist-vs-bayesian-probability",
    "title": "Bayesian Statistics",
    "section": "Frequentist vs Bayesian Probability",
    "text": "Frequentist vs Bayesian Probability\n\n\nFrequentist:\n\nProbability as frequency over repeated observations.\nData are random, but parameters are not.\nHow consistent are estimates for different data?\n\n\n\nBayesian:\n\nProbability as degree of belief/betting odds.\nData and parameters are random variables;\nEmphasis on conditional probability."
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#but-what-like-is-probability",
    "href": "slides/lecture04-1-bayes.html#but-what-like-is-probability",
    "title": "Bayesian Statistics",
    "section": "But What, Like, Is Probability?",
    "text": "But What, Like, Is Probability?\n\n\n\nFrequentist vs. Bayesian: different interpretations with some different methods and formalisms.\nWe will freely borrow from each school depending on the purpose and goal of an analysis.\n\n\n\n\nDefinitions of Probability Meme"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#bayes-rule",
    "href": "slides/lecture04-1-bayes.html#bayes-rule",
    "title": "Bayesian Statistics",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\n\nOriginal version\nBayes (1763):\n\\[\n\\begin{gather*}\nP(A | B) = \\frac{P(B | A) \\times P(A)}{P(B)} \\\\[0.5em]\n\\text{if} \\quad P(B) \\neq 0.\n\\end{gather*}\n\\]\n\n“Modern” version\nLaplace (1774):\n\\[\\underbrace{{p(\\theta | y)}}_{\\text{posterior}} = \\frac{\\overbrace{p(y | \\theta)}^{\\text{likelihood}}}{\\underbrace{p(y)}_\\text{normalization}} \\overbrace{p(\\theta)}^\\text{prior}\\]"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#logic-of-conditional-probability",
    "href": "slides/lecture04-1-bayes.html#logic-of-conditional-probability",
    "title": "Bayesian Statistics",
    "section": "Logic of Conditional Probability",
    "text": "Logic of Conditional Probability\nBayes’ Rule is the logic of conditional probability:\nWhat can we say about consistency of uncertainties with an observed sample \\(y\\)?\n\nModel configurations (\\(p(\\mathcal{M} | y)\\) or \\(p(\\mathcal{x} | y, \\mathcal{M})\\))\nHypotheses (\\(p(\\mathcal{H} | y)\\))"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#on-the-normalizing-constant",
    "href": "slides/lecture04-1-bayes.html#on-the-normalizing-constant",
    "title": "Bayesian Statistics",
    "section": "On The Normalizing Constant",
    "text": "On The Normalizing Constant\nThe normalizing constant (also called the marginal likelihood) is the integral \\[p(y) = \\int_\\Theta p(y | \\theta) p(\\theta) d\\theta.\\]\nSince this generally doesn’t depend on \\(\\theta\\), it can often be ignored, as the relative probabilities don’t change."
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#bayes-rule-ignoring-normalizing-constants",
    "href": "slides/lecture04-1-bayes.html#bayes-rule-ignoring-normalizing-constants",
    "title": "Bayesian Statistics",
    "section": "Bayes’ Rule (Ignoring Normalizing Constants)",
    "text": "Bayes’ Rule (Ignoring Normalizing Constants)\nThe version of Bayes’ rule which matters the most for 95% (approximate) of Bayesian statistics:\n\\[p(\\theta | y) \\propto p(y | \\theta) \\times p(\\theta)\\]\n\n“The posterior is the prior times the likelihood…”"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#example-is-a-coin-fair",
    "href": "slides/lecture04-1-bayes.html#example-is-a-coin-fair",
    "title": "Bayesian Statistics",
    "section": "Example: Is a Coin Fair?",
    "text": "Example: Is a Coin Fair?\n\n\nIn 10 flips, we’ve observed 3 H and 7 T.\nWhat can we say about the probability of heads given this information?\n\n\n\n\nFry Coin Uncertainty meme"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#coin-flipping-likelihood",
    "href": "slides/lecture04-1-bayes.html#coin-flipping-likelihood",
    "title": "Bayesian Statistics",
    "section": "Coin Flipping Likelihood",
    "text": "Coin Flipping Likelihood\n\n\nWe can represent the outcome of a coin flip with a heads-probability of \\(p\\) as a sample from a Bernoulli distribution,\n\\[y_i \\sim \\text{Bernoulli}(p).\\]"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#correlated-parameters-and-bayesian-inference",
    "href": "slides/lecture04-1-bayes.html#correlated-parameters-and-bayesian-inference",
    "title": "Bayesian Statistics",
    "section": "Correlated Parameters and Bayesian Inference",
    "text": "Correlated Parameters and Bayesian Inference\n\n\n\n\n\nCorrelated Climate Parameters\n\n\n\n\n\n\nCorrelated Climate Parameters\n\n\n\n\nSource: Errickson et al. (2021)"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#why-choose-a-particular-prior",
    "href": "slides/lecture04-1-bayes.html#why-choose-a-particular-prior",
    "title": "Bayesian Statistics",
    "section": "Why Choose A Particular Prior?",
    "text": "Why Choose A Particular Prior?\n\\[p(\\theta | y) \\propto p(y | \\theta) \\times {\\color{purple}p(\\theta)}\\]\nPriors can serve several purposes (more shortly…)\n\nOne perspective: Priors should reflect “actual knowledge” independent of the analysis (Jaynes, 2003)\nAnother: Priors are part of the probability model, and can be specified/changed accordingly based on predictive skill (Gelman et al., 2017; Gelman & Shalizi, 2013)"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#why-choose-a-particular-prior-1",
    "href": "slides/lecture04-1-bayes.html#why-choose-a-particular-prior-1",
    "title": "Bayesian Statistics",
    "section": "Why Choose A Particular Prior?",
    "text": "Why Choose A Particular Prior?\n\n\n\nThere are no “correct” priors, but more or less justified.\nDon’t assign zero probability to possible values.\nPriors matter less for simple models with lots of data, matter more in other cases.\nBase on information external to the data.\n\n\n\n\n\nCow Prior Meme\n\n\n\nSource: Richard McElreath"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#coin-flipping-prior",
    "href": "slides/lecture04-1-bayes.html#coin-flipping-prior",
    "title": "Bayesian Statistics",
    "section": "Coin-Flipping Prior",
    "text": "Coin-Flipping Prior\n\n\nSuppose that we spoke to a friend who knows something about coins, and she tells us that it is extremely difficult to make a passable weighted coin which comes up heads or tails more than 75% of the time.\n\nWe might try \\(p \\sim \\text{Beta}(7, 7)\\).\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Beta prior for coin flipping example"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#prior-predictive-simulation",
    "href": "slides/lecture04-1-bayes.html#prior-predictive-simulation",
    "title": "Bayesian Statistics",
    "section": "Prior Predictive Simulation",
    "text": "Prior Predictive Simulation\n\n\nSimulate from prior to understand implications for estimands/observables.\nThis produces the prior predictive distribution:\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Coin flipping prior predictive distribution"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#coin-flipping-probability-model",
    "href": "slides/lecture04-1-bayes.html#coin-flipping-probability-model",
    "title": "Bayesian Statistics",
    "section": "Coin-Flipping Probability Model",
    "text": "Coin-Flipping Probability Model\n\n\n\\[\n\\begin{align*}\ny_i &\\sim Bernoulli(p) \\\\\np &\\sim Beta(7, 7)\n\\end{align*}\n\\]\nWe can use a grid approximation to analyze the posterior.\n\nNotice how the prior regularizes the inference.\n\n\n\n\nCode\np_grid = 0:0.005:1\nloglikelihood(p) = sum(logpdf.(Bernoulli(p), flips .== \"H\"))\nlogprior(p) = logpdf(Beta(7, 7), p)\nlogposterior(p) = logprior(p) + loglikelihood(p)\n\nlik = loglikelihood.(p_grid)\npri = logprior.(p_grid)\npost = logposterior.(p_grid)\n\nplot(\n    p_grid,\n    exp.(lik) ./ sum(exp.(lik));\n    xlabel=L\"$p$\",\n    ylabel=\"Density\",\n    label=\"Likelihood\",\n    legend=:outerbottom,\n    linewidth=2,\n)\nplot!(p_grid, exp.(pri) ./ sum(exp.(pri)); label=\"Prior\", linewidth=2)\nplot!(p_grid, exp.(post) ./ sum(exp.(post)); label=\"Posterior\", linewidth=4)\nplot!(size=(600, 600))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Grid approximation to the coin-flipping posterior"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#posterior-approximation",
    "href": "slides/lecture04-1-bayes.html#posterior-approximation",
    "title": "Bayesian Statistics",
    "section": "Posterior Approximation",
    "text": "Posterior Approximation\nFor more complex models, grid approximations are too high-dimensional and/or computationally complex.\nFor now, we can optimize the posterior to find the maximum a posteriori estimate (MAP), which is analogous to the MLE.\nFor probabilistic inference, need more efficient sampling methods: will discuss briefly in roughly another week."
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#how-does-tds-affect-river-flow",
    "href": "slides/lecture04-1-bayes.html#how-does-tds-affect-river-flow",
    "title": "Bayesian Statistics",
    "section": "How Does TDS Affect River Flow?",
    "text": "How Does TDS Affect River Flow?\n\n\nQuestion: Does river flow affect the concentration of total dissolved solids?\nData: Cuyahoga River (1969 – 1973), from Helsel et al. (2020, Chapter 9).\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Observations from the Cuyahoga River."
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#how-does-tds-affect-river-flow-1",
    "href": "slides/lecture04-1-bayes.html#how-does-tds-affect-river-flow-1",
    "title": "Bayesian Statistics",
    "section": "How Does TDS Affect River Flow?",
    "text": "How Does TDS Affect River Flow?\n\n\nQuestion: Does river flow affect the concentration of total dissolved solids?\nModel:\n\\[D \\rightarrow S \\ {\\color{purple}\\leftarrow U}\\]\n\\[S = f(D, U)\\]"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#prior-selection",
    "href": "slides/lecture04-1-bayes.html#prior-selection",
    "title": "Bayesian Statistics",
    "section": "Prior Selection",
    "text": "Prior Selection\n\nDo we have prior scientific information we can express?\nConsider the units of the covariates (rescale if needed!).\nWhat are the general behaviors we should expect to see?"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#the-prior-is-part-of-the-model",
    "href": "slides/lecture04-1-bayes.html#the-prior-is-part-of-the-model",
    "title": "Bayesian Statistics",
    "section": "The Prior Is Part of the Model",
    "text": "The Prior Is Part of the Model\nThe impact of a prior is not independent of the data-generating model (likelihood) (Gelman et al., 2017).\nKey point: Need to understand implications of prior selection for resulting data generation."
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#prior-distributions",
    "href": "slides/lecture04-1-bayes.html#prior-distributions",
    "title": "Bayesian Statistics",
    "section": "Prior Distributions",
    "text": "Prior Distributions\n\n\nLet’s try:\n\\[\n\\begin{align*}\nS &= \\beta_0 + \\beta_1 \\log(D) + U\\\\\nU &\\sim \\mathcal{N}(0, \\sigma^2)\\\\\nH &\\sim \\text{Uniform}(0, 100) \\\\\n\\beta_0 &\\sim \\mathcal{TN}(500, 100; 0, \\infty) \\\\\n\\beta_1 &\\sim \\mathcal{TN}(0, 50; -\\infty, 0) \\\\\n\\sigma &\\sim \\mathcal{TN}(0, 25; 0, \\infty)\n\\end{align*}\n\\]\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Priors for model parameters."
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#prior-predictive-distribution",
    "href": "slides/lecture04-1-bayes.html#prior-predictive-distribution",
    "title": "Bayesian Statistics",
    "section": "Prior Predictive Distribution",
    "text": "Prior Predictive Distribution\n\n\nLet’s try:\n\\[\n\\begin{align*}\nS &= \\beta_0 + \\beta_1 \\log(D) + U\\\\\nU &\\sim \\mathcal{N}(0, \\sigma^2)\\\\\nH &\\sim \\text{Uniform}(0, 100) \\\\\n\\beta_0 &\\sim \\mathcal{TN}(500, 100; 0, \\infty) \\\\\n\\beta_1 &\\sim \\mathcal{TN}(0, 50; -\\infty, 0) \\\\\n\\sigma &\\sim \\mathcal{TN}(0, 25; 0, \\infty)\n\\end{align*}\n\\]\n\n\n\n\nCode\nfunction predictive_dist(p_samples, x)\n    ŷ = zeros(size(p_samples)[1], length(x))\n    for (i, row) in pairs(eachslice(p_samples; dims=1))\n        β₀, β₁, σ = row\n        ŷ[i, :] = β₀ .+ β₁ * log.(x) + rand(Normal(0, σ), length(x))\n    end\n    return ŷ\nend\n\n# sample from prior distributions\nn = 1_000\np_samples = zeros(n, 3)\np_samples[:, 1] = rand(truncated(Normal(500, 100); lower=0), n)\np_samples[:, 2] = rand(truncated(Normal(0, 50); upper=0), n)\np_samples[:, 3] = rand(truncated(Normal(0, 25); lower=0), n)\n\nxpred = 1:1:60\nypred = predictive_dist(p_samples, xpred)\nqpred = mapslices(row -&gt; quantile(row, [0.0, 0.025, 0.05, 0.5, 0.95, 0.975, 1.0]), ypred; dims=1)\np_prior = plot(; xlabel=L\"Discharge (m$^3$/s)\", ylabel=\"Total dissolved solids (mg/L)\")\nfor idx in 1:n\n    label = idx == 1 ? \"Prior Simulation\" : false\n    plot!(p_prior, xpred, ypred[idx, :], color=:black, alpha=0.1, label=label)\nend\nplot!(p_prior, size=(600, 500))\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Samples from the prior predictive distribution."
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#maximize-posterior",
    "href": "slides/lecture04-1-bayes.html#maximize-posterior",
    "title": "Bayesian Statistics",
    "section": "Maximize Posterior",
    "text": "Maximize Posterior\n\n\n\\(\\hat{\\beta_0}\\): 598\n\\(\\hat{\\beta_1}\\): -107\n\\(\\hat{\\sigma}^2\\): 68\\(^2\\)\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: MAP predictive interval."
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#comparison-of-mle-and-map",
    "href": "slides/lecture04-1-bayes.html#comparison-of-mle-and-map",
    "title": "Bayesian Statistics",
    "section": "Comparison of MLE and MAP",
    "text": "Comparison of MLE and MAP\n\n\n\n\n\nParameter\nMLE\nMAP\n\n\n\n\n\\(\\beta_0\\)\n610\n598\n\n\n\\(\\beta_1\\)\n-112\n-107\n\n\n\\(\\sigma^2\\)\n72\\(^2\\)\n68\\(^2\\)\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Comparison of the MAP and MLE projections."
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#posterior-predictive-distribution",
    "href": "slides/lecture04-1-bayes.html#posterior-predictive-distribution",
    "title": "Bayesian Statistics",
    "section": "Posterior Predictive Distribution",
    "text": "Posterior Predictive Distribution\nBayes’ Theorem gives us probabilities of different parameter combinations.\nOnce we have the ability to sample from the posterior, we can integrate over it to obtain the posterior predictive distribution:\n\\[p(\\tilde{y} | \\mathbf{y}) = \\int_{\\Theta} p(\\tilde{y} | \\theta) p(\\theta | \\mathbf{y}) d\\theta\\]"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#hierarchical-models",
    "href": "slides/lecture04-1-bayes.html#hierarchical-models",
    "title": "Bayesian Statistics",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\nBayesian frameworks also lend themselves well to hierarchical models, with uncertain prior distributions:\n\\[\\begin{align}\ny_j | \\theta_j, \\phi &\\sim P(y_j | \\theta_j, \\phi) \\nonumber \\\\\n\\theta_j | \\phi &\\sim P(\\theta_j | \\phi) \\nonumber \\\\\n\\phi &\\sim P(\\phi) \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#recall-confidence-intervals",
    "href": "slides/lecture04-1-bayes.html#recall-confidence-intervals",
    "title": "Bayesian Statistics",
    "section": "Recall: Confidence Intervals",
    "text": "Recall: Confidence Intervals\n\n\nFrequentist confidence intervals reflect consistency of estimation over repeated experiments.\n\\(\\alpha\\)% of intervals produced should contain the “true” value.\n\n\nCode\n# set up distribution\nmean_true = 0.4\nn_cis = 100 # number of CIs to compute\ndist = Normal(mean_true, 2)\n\n# use sample size of 100\nsamples = rand(dist, (100, n_cis))\n# mapslices broadcasts over a matrix dimension, could also use a loop\nsample_means = mapslices(mean, samples; dims=1)\nsample_sd = mapslices(std, samples; dims=1) \nmc_sd = 1.96 * sample_sd / sqrt(100)\nmc_ci = zeros(n_cis, 2) # preallocate\nfor i = 1:n_cis\n    mc_ci[i, 1] = sample_means[i] - mc_sd[i]\n    mc_ci[i, 2] = sample_means[i] + mc_sd[i]\nend\n# find which CIs contain the true value\nci_true = (mc_ci[:, 1] .&lt; mean_true) .&& (mc_ci[:, 2] .&gt; mean_true)\n# compute percentage of CIs which contain the true value\nci_frac1 = 100 * sum(ci_true) ./ n_cis\n\n# plot CIs\np1 = plot([mc_ci[1, :]], [1, 1], linewidth=3, color=:deepskyblue, label=\"95% Confidence Interval\", yticks=:false, legend=:false)\nfor i = 2:n_cis\n    if ci_true[i]\n        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:deepskyblue, label=:false)\n    else\n        plot!(p1, [mc_ci[i, :]], [i, i], linewidth=2, color=:red, label=:false)\n    end\nend\nvline!(p1, [mean_true], color=:black, linewidth=2, linestyle=:dash, label=\"True Value\") # plot true value as a vertical line\nxaxis!(p1, \"Estimate\")\nplot!(p1, size=(500, 500)) # resize to fit slide\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Sample Size 100\n\n\n\n\n\nDisplay of 95% confidence intervals"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#credible-intervals-1",
    "href": "slides/lecture04-1-bayes.html#credible-intervals-1",
    "title": "Bayesian Statistics",
    "section": "Credible Intervals",
    "text": "Credible Intervals\n\n\nBayesian equivalent: credible intervals\n\\(\\alpha\\)% credible interval contains \\(\\alpha\\)% of the posterior probability.\nKey: These are not unique, so be clear what kind of interval you’re using.\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Two different 95% credible intervals."
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#credible-vs.-prediction-intervals",
    "href": "slides/lecture04-1-bayes.html#credible-vs.-prediction-intervals",
    "title": "Bayesian Statistics",
    "section": "Credible vs. Prediction Intervals",
    "text": "Credible vs. Prediction Intervals\n\n\nBayesians view both parameters and data as stochastic variables.\nWe’ll use prediction intervals for future data, credible intervals for missing data/parameters.\n\n\n\n\nParameters vs Data Bayes Meme"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#bayesian-statistics-1",
    "href": "slides/lecture04-1-bayes.html#bayesian-statistics-1",
    "title": "Bayesian Statistics",
    "section": "Bayesian Statistics",
    "text": "Bayesian Statistics\n\nProbability as degree of belief\nBayes’ Rule as the fundamental theorem of conditional probability\nBoth data and parameters are random variables\nCan describe probability of any random variable conditional on observations\nBayesian updating as an information filter"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#bayesian-workflow",
    "href": "slides/lecture04-1-bayes.html#bayesian-workflow",
    "title": "Bayesian Statistics",
    "section": "Bayesian Workflow",
    "text": "Bayesian Workflow\n\nPrior selection important: softly express scientific knowledge and regularization.\nPrior predictive simulation to understand implications of prior selections.\nDon’t look at the data when assessing priors.\nBut be open to revisiting model specification (including priors) based on quality of fit."
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#communicating-uncertainty",
    "href": "slides/lecture04-1-bayes.html#communicating-uncertainty",
    "title": "Bayesian Statistics",
    "section": "Communicating Uncertainty",
    "text": "Communicating Uncertainty\n\nA single confidence interval tells you nothing about the “true” value of a parameter without understanding of sampling/measurement.\nCredible intervals do communicate probability of “true” values.\nPrediction intervals fit into either paradigm: what is the probability (understood in whatever sense) that you see a given value in the future?"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#next-classes",
    "href": "slides/lecture04-1-bayes.html#next-classes",
    "title": "Bayesian Statistics",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Bayesian Statistics\nNext Week: Temporal and Spatial Models and Errors"
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#assessments",
    "href": "slides/lecture04-1-bayes.html#assessments",
    "title": "Bayesian Statistics",
    "section": "Assessments",
    "text": "Assessments\nHomework 1 due Friday (2/7)."
  },
  {
    "objectID": "slides/lecture04-1-bayes.html#references-scroll-for-full-list",
    "href": "slides/lecture04-1-bayes.html#references-scroll-for-full-list",
    "title": "Bayesian Statistics",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nBayes, T. (1763). An Essay towards Solving a Problem in the Doctrine of Chance. Philosophical Transactions of the Royal Society of London, 53, 370–418.\n\n\nErrickson, F. C., Keller, K., Collins, W. D., Srikrishnan, V., & Anthoff, D. (2021). Equity is more important for the social cost of methane than climate uncertainty. Nature, 592, 564–570. https://doi.org/10.1038/s41586-021-03386-6\n\n\nGelman, A., & Shalizi, C. R. (2013). Philosophy and the practice of Bayesian statistics. Br. J. Math. Stat. Psychol., 66, 8–38. https://doi.org/10.1111/j.2044-8317.2011.02037.x\n\n\nGelman, A., Simpson, D., & Betancourt, M. (2017). The prior can often only be understood in the context of the likelihood. Entropy, 19, 555. https://doi.org/10.3390/e19100555\n\n\nHelsel, D. R., Hirsch, R. M., Ryberg, K. R., Archfield, S. A., & Gilroy, E. J. (2020). Statistical methods in water resources (research report No. 4-A3) (p. 484). Reston, VA: U.S. Geological Survey. Retrieved from http://pubs.er.usgs.gov/publication/tm4A3\n\n\nJaynes, E. T. (2003). Probability theory: the Logic of Science. (G. L. Bretthorst, Ed.). Cambridge, UK ; New York, NY: Cambridge University Press. Retrieved from https://market.android.com/details?id=book-tTN4HuUNXjgC\n\n\nLaplace, P. S. (1774). Mémoire sur la Probabilité des Causes par les évènemens. In Mémoires de Mathematique et de Physique, Presentés à l’Académie Royale des Sciences, Par Divers Savans & Lus Dans ses Assemblées (pp. 621–656)."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#probability-fundamentals",
    "href": "slides/lecture03-1-probmodels.html#probability-fundamentals",
    "title": "Linear Regression",
    "section": "Probability Fundamentals",
    "text": "Probability Fundamentals\n\nBayesian vs. Frequentist Interpretations\nDistributions reflect assumptions on probability of data.\nNormal distributions: “least informative” distribution for a given mean/variance.\nFit distributions by maximizing likelihood.\nCommunicating uncertainty: confidence vs. predictive intervals."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#how-does-tds-affect-river-flow",
    "href": "slides/lecture03-1-probmodels.html#how-does-tds-affect-river-flow",
    "title": "Linear Regression",
    "section": "How Does TDS Affect River Flow?",
    "text": "How Does TDS Affect River Flow?\n\n\nQuestion: Does river flow affect the concentration of total dissolved solids?\nData: Cuyahoga River (1969 – 1973), from Helsel et al. (2020, Chapter 9)."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#how-does-tds-affect-river-flow-1",
    "href": "slides/lecture03-1-probmodels.html#how-does-tds-affect-river-flow-1",
    "title": "Linear Regression",
    "section": "How Does TDS Affect River Flow?",
    "text": "How Does TDS Affect River Flow?\n\n\nQuestion: Does river flow affect the concentration of total dissolved solids?\nModel: \\[D \\rightarrow S \\ {\\color{purple}\\leftarrow U}\\] \\[S = f(D, U)\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#log-linear-relationships",
    "href": "slides/lecture03-1-probmodels.html#log-linear-relationships",
    "title": "Linear Regression",
    "section": "Log-Linear Relationships",
    "text": "Log-Linear Relationships\n\n\n\\[\n\\begin{align*}\nS &= \\beta_0 + \\beta_1 \\log(D) + U\\\\\nU &\\sim \\mathcal{N}(0, \\sigma^2)\n\\end{align*}\n\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#likelihood",
    "href": "slides/lecture03-1-probmodels.html#likelihood",
    "title": "Linear Regression",
    "section": "Likelihood",
    "text": "Likelihood\nHow do we find \\(\\beta_i\\) and \\(\\sigma\\)?\nLikelihood of data to have come from distribution \\(f(\\mathbf{x} | \\theta)\\): \\[\\mathcal{L}(\\theta | \\mathbf{x}) = \\underbrace{f(\\mathbf{x} | \\theta)}_{\\text{PDF}}\\]\nHere the randomness comes from \\(U\\): \\[S \\sim \\mathcal{N}(\\beta_0 + \\beta_1 \\log(D), \\sigma^2)\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#maximizing-gaussian-likelihood-least-squares",
    "href": "slides/lecture03-1-probmodels.html#maximizing-gaussian-likelihood-least-squares",
    "title": "Linear Regression",
    "section": "Maximizing Gaussian Likelihood ⇔ Least Squares",
    "text": "Maximizing Gaussian Likelihood ⇔ Least Squares\n\\[y_i \\sim \\mathcal{N}(F(x_i), \\sigma)\\]\n\n\\[\\mathcal{L}(\\theta | \\mathbf{y}; F) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{y_i - F(x_i)^2}{2\\sigma^2})\\]\n\n\n\\[\\log \\mathcal{L}(\\theta | \\mathbf{y}; F) = \\sum_{i=1}^n \\left[\\log \\frac{1}{\\sqrt{2\\pi}} - \\frac{1}{2\\sigma^2}(y_i - F(x_i))^2 \\right]\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#simplifying-log-likelihood",
    "href": "slides/lecture03-1-probmodels.html#simplifying-log-likelihood",
    "title": "Linear Regression",
    "section": "",
    "text": "\\[\n\\begin{align}\n\\log \\mathcal{L}(\\theta | \\mathbf{y}, F) &= \\sum_{i=1}^n \\left[\\log \\frac{1}{\\sqrt{2\\pi}} - \\frac{1}{2\\sigma^2}(y_i - F(x_i))  ^2 \\right] \\\\\n&= n \\log \\frac{1}{\\sqrt{2\\pi}} - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - F(x_i))^2\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#simplifying-constants-ignore",
    "href": "slides/lecture03-1-probmodels.html#simplifying-constants-ignore",
    "title": "Linear Regression",
    "section": "",
    "text": "Ignoring constants (including \\(\\sigma\\)):\n\\[\\log \\mathcal{L}(\\theta | \\mathbf{y}, F) \\propto -\\sum_{i=1}^n (y_i - F(x_i))^2.\\]\n\nMaximizing \\(f(x)\\) is equivalent to minimizing \\(-f(x)\\):\n\\[\n-\\log \\mathcal{L}(\\theta | \\mathbf{y}, F) \\propto \\sum_{i=1}^n (y_i - F(x_i))^2 = \\text{MSE}\n\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#back-to-the-problem",
    "href": "slides/lecture03-1-probmodels.html#back-to-the-problem",
    "title": "Linear Regression",
    "section": "Back to the Problem…",
    "text": "Back to the Problem…\n\n\n\n\nCode\n# tds_riverflow_loglik: function to compute the log-likelihood for the tds model\n# θ: vector of model parameters (coefficients β₀ and β₁ and stdev σ)\n# tds, flow: vectors of data\nfunction tds_riverflow_loglik(θ, tds, flow)\n    β₀, β₁, σ = θ # unpack parameter vector\n    μ = β₀ .+ β₁ * log.(flow) # find mean\n    ll = sum(logpdf.(Normal.(μ, σ), tds)) # compute log-likelihood\n    return ll\nend\n\nlb = [0.0, -1000.0, 1.0]\nub = [1000.0, 1000.0, 100.0]\nθ₀ = [500.0, 0.0, 50.0]\noptim_out = Optim.optimize(θ -&gt; -tds_riverflow_loglik(θ, tds.tds_mgL, tds.discharge_cms), lb, ub, θ₀)\nθ_mle = round.(optim_out.minimizer; digits=0)\n@show θ_mle;\n\n\nθ_mle = [610.0, -112.0, 72.0]\n\n\n\n\n\nCode\n# simulate 10,000 predictions\nx = 1:0.1:60\nμ = θ_mle[1] .+ θ_mle[2] * log.(x)\n\ny_pred = zeros(length(x), 10_000)\nfor i = 1:length(x)\n    y_pred[i, :] = rand(Normal(μ[i], θ_mle[3]), 10_000)\nend\n# take quantiles to find prediction intervals\ny_q = mapslices(v -&gt; quantile(v, [0.05, 0.5, 0.95]), y_pred; dims=2)\n\nplot(p1, \n    x,\n    y_q[:, 2],\n    ribbon = (y_q[:, 2] - y_q[:, 1], y_q[:, 3] - y_q[:, 2]),\n    linewidth=3, \n    fillalpha=0.2, \n    label=\"Best Fit\",\n    size=(600, 550))"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#residual-analysis",
    "href": "slides/lecture03-1-probmodels.html#residual-analysis",
    "title": "Linear Regression",
    "section": "Residual Analysis",
    "text": "Residual Analysis\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Residuals for the TDS-Riverflow model.\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\nFigure 1"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#structure-of-a-probability-model",
    "href": "slides/lecture03-1-probmodels.html#structure-of-a-probability-model",
    "title": "Linear Regression",
    "section": "Structure of a Probability Model",
    "text": "Structure of a Probability Model\nUnpacking linear regression:\n\\[\\begin{align*}\ny_i &\\sim N(\\mu_i, \\sigma^2) \\\\\n\\mu_i &= \\sum_j \\beta_j x^j_i\n\\end{align*}\n\\]\n\nNote: Can handle heteroskedastic errors with a regression \\[\\sigma_i^2 = \\sum_j \\alpha_j z^j_i\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#think-generatively",
    "href": "slides/lecture03-1-probmodels.html#think-generatively",
    "title": "Linear Regression",
    "section": "Think Generatively",
    "text": "Think Generatively\n\n\nComponents of probability model for observations:\n\nModel for “system state” (LR: \\(\\sum_j \\beta_j x^j_i\\))\nChoice of “error” distribution (LR: \\(N(0, \\sigma^2)\\))\n\n\n\n\n\nBrain on Regression Meme\n\n\n\nSource: Richard McElreath"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#you-can-always-fit-models",
    "href": "slides/lecture03-1-probmodels.html#you-can-always-fit-models",
    "title": "Linear Regression",
    "section": "You Can Always Fit Models…",
    "text": "You Can Always Fit Models…\n\n\nBut not all models are theoretically justifiable.\n\n\n\n\nIan Malcolm meme"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#how-do-we-choose-what-to-model",
    "href": "slides/lecture03-1-probmodels.html#how-do-we-choose-what-to-model",
    "title": "Linear Regression",
    "section": "How Do We Choose What To Model?",
    "text": "How Do We Choose What To Model?\n\nXKCD 2620\nSource: XKCD 2620"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#example-modeling-counts",
    "href": "slides/lecture03-1-probmodels.html#example-modeling-counts",
    "title": "Linear Regression",
    "section": "Example: Modeling Counts",
    "text": "Example: Modeling Counts\n\n\nCode\nfish = CSV.File(\"data/ecology/Fish.csv\") |&gt; DataFrame\nfish[1:3, :]\n\n\n3×6 DataFrame\n\n\n\nRow\nfish_caught\nlivebait\ncamper\npersons\nchild\nhours\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nFloat64\n\n\n\n\n1\n0\n0\n0\n1\n0\n21.124\n\n\n2\n0\n1\n1\n1\n0\n5.732\n\n\n3\n0\n1\n0\n1\n0\n1.323"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#what-distribution",
    "href": "slides/lecture03-1-probmodels.html#what-distribution",
    "title": "Linear Regression",
    "section": "What Distribution?",
    "text": "What Distribution?\n\nCount data can be modeled using a Poisson or negative binomial distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Poisson\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Negative Binomial"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#exploring-the-data",
    "href": "slides/lecture03-1-probmodels.html#exploring-the-data",
    "title": "Linear Regression",
    "section": "Exploring the Data",
    "text": "Exploring the Data\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Fishing Data\n\n\n\n\n\nMean: 3.3\nVariance: 135.4"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#model-specification",
    "href": "slides/lecture03-1-probmodels.html#model-specification",
    "title": "Linear Regression",
    "section": "Model Specification",
    "text": "Model Specification\nWe might hypothesize that more people fishing for more hours results in a greater chance of catching fish.\n\\[\\begin{align*}\ny_i &\\sim Poisson(\\lambda_i) \\\\\nf(\\lambda_i) &= \\beta_0 + \\beta_1 P_i + \\beta_2 H_i\n\\end{align*}\n\\]\n\n\\(\\lambda_i\\): positive “rate”\n\\(f(\\cdot)\\): maps positive reals (rate scale) to all reals (linear model scale)"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#link-functions",
    "href": "slides/lecture03-1-probmodels.html#link-functions",
    "title": "Linear Regression",
    "section": "Link Functions",
    "text": "Link Functions\n\n\n\\(\\color{brown}f\\) is the link function.\n\\(\\color{royalblue}f^{-1}\\) is the inverse link.\n\n\\[\\begin{align*}\ny_i &\\sim Poisson(\\lambda_i) \\\\\n{\\color{brown}f}(\\lambda_i) &= \\beta_0 + \\beta_1 P_i + \\beta_2 H_i\n\\end{align*}\n\\]\n\\[\n\\lambda_i = {\\color{royalblue}f^{-1}}\\left(\\beta_0 + \\beta_1 P_i + \\beta_2 H_i\\right)\n\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#choosing-a-link-function",
    "href": "slides/lecture03-1-probmodels.html#choosing-a-link-function",
    "title": "Linear Regression",
    "section": "Choosing a Link Function",
    "text": "Choosing a Link Function\nLink functions are typically linked to distributions.\n\nPoisson models usually use the log link (positives → reals).\nBinomial/Bernoulli models use the logit link (\\([0, 1]\\) → reals) \\[\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right)\\]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#fitting-the-model",
    "href": "slides/lecture03-1-probmodels.html#fitting-the-model",
    "title": "Linear Regression",
    "section": "Fitting the Model",
    "text": "Fitting the Model\n\nfunction fish_model(params, persons, hours, fish_caught)\n    β₀, β₁, β₂ = params\n    λ = exp.(β₀ .+ β₁ * persons + β₂ * hours)\n    loglik = sum(logpdf.(Poisson.(λ), fish_caught))\nend\n\nlb = [-100.0, -100.0, -100.0]\nub = [100.0, 100.0, 100.0]\ninit = [0.0, 0.0, 0.0]\n\noptim_out = optimize(θ -&gt; -fish_model(θ, fish.persons, fish.hours, fish.fish_caught), lb, ub, init)\nθ_mle = optim_out.minimizer\n@show round.(θ_mle; digits=1);\n\nround.(θ_mle; digits = 1) = [-1.5, 0.8, 0.0]"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#evaluating-model-fit",
    "href": "slides/lecture03-1-probmodels.html#evaluating-model-fit",
    "title": "Linear Regression",
    "section": "Evaluating Model Fit",
    "text": "Evaluating Model Fit\n\n\n\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Predictive distribution for fitted fish model."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#what-was-the-problem",
    "href": "slides/lecture03-1-probmodels.html#what-was-the-problem",
    "title": "Linear Regression",
    "section": "What Was The Problem?",
    "text": "What Was The Problem?\n\nModel neglected other plausible contributors, e.g. live bait.\nData is over-dispersed: higher variance than mean.\nMight be better described by a zero-inflated Poisson model:\n\nVisitors who spent little time are likely to catch no fish.\nVisitors who spent more time are likely to catch positive fish."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#probability-models",
    "href": "slides/lecture03-1-probmodels.html#probability-models",
    "title": "Linear Regression",
    "section": "Probability Models",
    "text": "Probability Models\n\nThink of regression as modeling system state (or expectation).\nFull probability model includes distribution of “errors” from expected state.\n\nError models can be complex! More on this later…\n\nGeneralized linear models may require a link to map regression to parameters.\nFit models by maximizing likelihood: no problem using optimization routines."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#next-classes",
    "href": "slides/lecture03-1-probmodels.html#next-classes",
    "title": "Linear Regression",
    "section": "Next Classes",
    "text": "Next Classes\nWednesday: Modeling Time Series\nNext Week: Bayesian Statistics and Inference"
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#assessments",
    "href": "slides/lecture03-1-probmodels.html#assessments",
    "title": "Linear Regression",
    "section": "Assessments",
    "text": "Assessments\nHomework 1 Due Friday (2/7)."
  },
  {
    "objectID": "slides/lecture03-1-probmodels.html#references-scroll-for-full-list",
    "href": "slides/lecture03-1-probmodels.html#references-scroll-for-full-list",
    "title": "Linear Regression",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nHelsel, D. R., Hirsch, R. M., Ryberg, K. R., Archfield, S. A., & Gilroy, E. J. (2020). Statistical methods in water resources (research report No. 4-A3) (p. 484). Reston, VA: U.S. Geological Survey. Retrieved from http://pubs.er.usgs.gov/publication/tm4A3"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#about-me",
    "href": "slides/lecture01-1-intro.html#about-me",
    "title": "Welcome to BEE 4850/5850!",
    "section": "About Me",
    "text": "About Me\nInstructor: Prof. Vivek Srikrishnan, viveks at cornell dot edu\nInterests:\n\nBridging Earth science, data science, and decision science to improve climate risk management;\nUnintended consequences which result from neglecting uncertainty or system dynamics."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#meet-my-supervisors",
    "href": "slides/lecture01-1-intro.html#meet-my-supervisors",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Meet My Supervisors",
    "text": "Meet My Supervisors\n\n\n\n\nMy Supervisors"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#what-do-you-hope-to-get-out-of-this-course",
    "href": "slides/lecture01-1-intro.html#what-do-you-hope-to-get-out-of-this-course",
    "title": "Welcome to BEE 4850/5850!",
    "section": "What Do You Hope To Get Out Of This Course?",
    "text": "What Do You Hope To Get Out Of This Course?\nTake a moment, write it down, and we’ll share!"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#why-does-data-analysis-matter",
    "href": "slides/lecture01-1-intro.html#why-does-data-analysis-matter",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Why Does Data Analysis Matter?",
    "text": "Why Does Data Analysis Matter?\n\nScientific insight;\nDecision-making;\nUnderstanding uncertainty"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#the-ideal",
    "href": "slides/lecture01-1-intro.html#the-ideal",
    "title": "Welcome to BEE 4850/5850!",
    "section": "The Ideal",
    "text": "The Ideal\n\n\nGoal: Obtain data that is so self-evident that you don’t need to do statistics!\n\n\n\n\nXKCD 2400\n\n\n\nSource: XKCD 2400"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#but-in-actuality",
    "href": "slides/lecture01-1-intro.html#but-in-actuality",
    "title": "Welcome to BEE 4850/5850!",
    "section": "But in Actuality",
    "text": "But in Actuality\n\nExcept in the most controlled experiments, data are noisy;\nThe causes of the data cannot be extracted from the data alone;\nThe reasons for statistical analyses are not found in the data themselves."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#uniquechallenging-features-of-data",
    "href": "slides/lecture01-1-intro.html#uniquechallenging-features-of-data",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Unique/Challenging Features Of Data",
    "text": "Unique/Challenging Features Of Data\nThere are many features of environmental (and biological!) data which make data analysis interesting and hard."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#extreme-events",
    "href": "slides/lecture01-1-intro.html#extreme-events",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Extreme Events",
    "text": "Extreme Events\n\n\n\n\nExtreme Events\n\n\n\n\nSource: Doss-Gollin & Keller (2023)"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#extreme-events-1",
    "href": "slides/lecture01-1-intro.html#extreme-events-1",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Extreme Events",
    "text": "Extreme Events\n\n\n\n\nXKCD 2107\n\n\n\n\nSource: XKCD 2107"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#correlated-uncertainties",
    "href": "slides/lecture01-1-intro.html#correlated-uncertainties",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Correlated Uncertainties",
    "text": "Correlated Uncertainties\n\n\n \n\n\nSource: Errickson et al. (2021)"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#non-stationarity",
    "href": "slides/lecture01-1-intro.html#non-stationarity",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Non-Stationarity",
    "text": "Non-Stationarity\n\n\n\n\nNon-Stationary Trends\n\n\n\n\nSource: Fagnant et al. (2020)"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#forcing-model-uncertainty",
    "href": "slides/lecture01-1-intro.html#forcing-model-uncertainty",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Forcing & Model Uncertainty",
    "text": "Forcing & Model Uncertainty\n\n\n\n\nDifferent Uncertainties for SLR\n\n\n\n\nSource: Doss-Gollin & Keller (2023)"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#deep-uncertainty",
    "href": "slides/lecture01-1-intro.html#deep-uncertainty",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Deep Uncertainty",
    "text": "Deep Uncertainty\n\n\n\n\nDeep Uncertainty for Future Climate Projections\n\n\n\n\nSource: Srikrishnan et al. (2022)"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#causation-is-not-association",
    "href": "slides/lecture01-1-intro.html#causation-is-not-association",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Causation Is Not Association",
    "text": "Causation Is Not Association\n\n\n\nWe’ve all heard “correlation (association) does not imply causation”\nBut confounds/noise can mean causation does not imply association\n\n\n\n\n\n\nCorrelation not Causation Meme\n\n\n\n\nSource: Richard McElreath"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#description-and-inference-are-connected",
    "href": "slides/lecture01-1-intro.html#description-and-inference-are-connected",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Description and Inference are Connected",
    "text": "Description and Inference are Connected\n\n\nExcept for superficial tasks:\n\nKnowing what is important to describe requires a model;\nUnderstanding how the data might differ from the population requires a model.\n\n\n\n\n\n\nSpidermen Meme"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#some-problems-with-the-standard-data-analysis-toolkit",
    "href": "slides/lecture01-1-intro.html#some-problems-with-the-standard-data-analysis-toolkit",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Some Problems With The “Standard” Data Analysis Toolkit",
    "text": "Some Problems With The “Standard” Data Analysis Toolkit\n\n\nStatistical assumptions may not be valid;\n“Null” vs “Alternative” hypotheses and tests may be chosen for computational convenience, not scientific relevance.\nMany different substantive models can imply the same statistical model.\n\n\n\nImportant: “Big” data doesn’t solve the problem!"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#my-philosophical-position",
    "href": "slides/lecture01-1-intro.html#my-philosophical-position",
    "title": "Welcome to BEE 4850/5850!",
    "section": "My Philosophical Position",
    "text": "My Philosophical Position\n\n\n\nProbability theory helps us deduce logical implications of theories conditional on our assumptions\nCannot use an “objective” procedure to avoid subjective responsibility\n\n\n\n\n\n\nBart Statistics Meme"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#model-based-data-analysis",
    "href": "slides/lecture01-1-intro.html#model-based-data-analysis",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Model-Based Data Analysis",
    "text": "Model-Based Data Analysis\nWe can (transparently):\n\nExamine logical implications of model assumptions (including interventions/out-of-sample generation).\nAssess evidence for multiple hypotheses by generating simulated data.\nIdentify opportunities to design future experiments or observations to distinguish between competing hypotheses."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#model-based-data-analysis-1",
    "href": "slides/lecture01-1-intro.html#model-based-data-analysis-1",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Model-Based Data Analysis",
    "text": "Model-Based Data Analysis\n\n\nModels are how we assess evidence for theories.\n\n\n\n\n\nRock Paper Scissors meme"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#course-organization",
    "href": "slides/lecture01-1-intro.html#course-organization",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Course Organization",
    "text": "Course Organization\n\n\n\n\n\ntimeline\n      Introduction (Weeks 1-2): Overview\n                  : Hypothesis Testing and Scientific Inference\n      Probability Fundamentals (Weeks 2-5): Prob/Stats \"Review\"\n                                          : Modeling Data-Generating Processes\n                                          : Bayesian Statistics\n                                          : Time Series\n                                          : Model Fitting\n      Simulation Methods (Weeks 6-7): Monte Carlo\n                                    : Bootstrap\n      Model Evaluation (Weeks 8-9): Cross-Validation\n                      : Model Selection\n      Useful Models (Weeks 10-13): Extreme Values\n                                 : Missing Data\n                                 : Mixture Models\n                                 : Gaussian Processes\n      Experimental Design (Weeks 14): Confounds\n                                       : Controls and Designing Experiments"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#background-knowledge-computing",
    "href": "slides/lecture01-1-intro.html#background-knowledge-computing",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Background Knowledge: Computing",
    "text": "Background Knowledge: Computing\n\nBasics (at the level of CS 111x)\nNo specific language requirement.\nSome extra work/effort may be needed if you haven’t coded in a while.\nMay need some additional familiarity with statistical packages (and “light” optimization)"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#background-knowledge-probabilitystatistics",
    "href": "slides/lecture01-1-intro.html#background-knowledge-probabilitystatistics",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Background Knowledge: Probability/Statistics",
    "text": "Background Knowledge: Probability/Statistics\n\nENGRD 2700/CEE 3040\nSummary statistics of data\nProbability distributions\nBasic visualizations\nMonte Carlo basics"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#grades",
    "href": "slides/lecture01-1-intro.html#grades",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Grades",
    "text": "Grades\n\n\n\nAssessment\nWeight\n\n\n\n\nParticipation\n5%\n\n\nReadings\n10%\n\n\nQuizzes\n10%\n\n\nLiterature Critique\n15%\n\n\nHomework Assignments\n30%\n\n\nTerm Project\n30%"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#overall-guidelines",
    "href": "slides/lecture01-1-intro.html#overall-guidelines",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Overall Guidelines",
    "text": "Overall Guidelines\n\nCollaboration highly encouraged, but all work must reflect your own understanding\nSubmit PDFs on Gradescope\n50% penalty for late submission (up to 24 hours)\nStandard rubric available on website\nAlways cite external references"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#literature-critique",
    "href": "slides/lecture01-1-intro.html#literature-critique",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Literature Critique",
    "text": "Literature Critique\n\nSelect a paper which involves some type of statistical or data analysis\nCritique choices: do they support the scientific conclusions?\nSubmit a 2-3 page writeup at the end of the semester\nIf you’re unsure where to look for a paper, talk to Prof. Srikrishnan"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#readings",
    "href": "slides/lecture01-1-intro.html#readings",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Readings",
    "text": "Readings\n\nSeveral readings assigned for discussion throughout the semester.\nAnnotation assignments on Canvas: by end of the week.\nWill discuss paper at start of class next Monday.\n5850 students: Write a one-page summary."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#quizzes",
    "href": "slides/lecture01-1-intro.html#quizzes",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Quizzes",
    "text": "Quizzes\n\nAssigned on Gradescope (almost) weekly.\nShort (2-3 problems), focused on material from that week.\nDue before class next Monday."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#homework-assignments",
    "href": "slides/lecture01-1-intro.html#homework-assignments",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Homework Assignments",
    "text": "Homework Assignments\n\nMore in-depth problems\nRoughly 2 weeks to complete\nRegrade requests must be made within one week\n5850 Students: Some extra problems"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#term-project",
    "href": "slides/lecture01-1-intro.html#term-project",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Term Project",
    "text": "Term Project\n\nAnalyze a data set of interest using model(s) of your choice\nCan work individually or groups of 2\nSeveral deliverables throughout the semester\nFinal in-class presentation and report"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#attendance",
    "href": "slides/lecture01-1-intro.html#attendance",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Attendance",
    "text": "Attendance\nNot required, but students tend to do better when they’re actively engaged in class."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#office-hours",
    "href": "slides/lecture01-1-intro.html#office-hours",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Office Hours",
    "text": "Office Hours\n\nInstructor: MW 1-2 PM, 318 Riley-Robb\nTA: MTh 10:30-11:30 AM, 319 Riley-Robb\nAlmost impossible to find a time that works for all (or even most); please feel free to make appointments as/if needed."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#accomodations",
    "href": "slides/lecture01-1-intro.html#accomodations",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Accomodations",
    "text": "Accomodations\nIf you have any access barriers in this class, please seek out any helpful accomodations.\n\nGet an SDS letter.\nIf you need an accomodation before you have an official letter, please reach out to me ASAP!"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#academic-integrity",
    "href": "slides/lecture01-1-intro.html#academic-integrity",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nHopefully not a concern…\n\nCollaboration is great and is encouraged!\nKnowing how to find and use helpful resources is a skill we want to develop.\nDon’t just copy…learn from others and give credit.\nSubmit your own original work."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#academic-integrity-1",
    "href": "slides/lecture01-1-intro.html#academic-integrity-1",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Academic Integrity",
    "text": "Academic Integrity\nObviously, just copying down answers from Chegg or ChatGPT and passing them off as your own is not ok."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#llms-bullshit-generators",
    "href": "slides/lecture01-1-intro.html#llms-bullshit-generators",
    "title": "Welcome to BEE 4850/5850!",
    "section": "LLMs: Bullshit Generators",
    "text": "LLMs: Bullshit Generators\nThink about ChatGPT as a drunk who tells stories for drinks.\nIt will give you plausible-looking text or code on any topic, but it doesn’t know anything beyond what it “overheard.”\nChatGPT can be useful for certain tasks (e.g. understanding code errors), but may neglect context for why/when certain information or solutions work."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#chatgpt-the-stochastic-parrot",
    "href": "slides/lecture01-1-intro.html#chatgpt-the-stochastic-parrot",
    "title": "Welcome to BEE 4850/5850!",
    "section": "ChatGPT: The Stochastic Parrot",
    "text": "ChatGPT: The Stochastic Parrot\nMust specifically call out where you used ChatGPT in your work (beyond simple referencing; see syllabus for details)."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#communications",
    "href": "slides/lecture01-1-intro.html#communications",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Communications",
    "text": "Communications\nUse Ed Discussion for questions and discussions about class, homework assignments, etc.\n\nTry to use public posts so others can benefit from questions and can weigh in.\nI will make announcements through Ed."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#email",
    "href": "slides/lecture01-1-intro.html#email",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Email",
    "text": "Email\nWhen urgency or privacy is required, email is ok.\n\n\n\n\n\n\nImportant\n\n\nPlease include BEE4850 in your email subject line! This will ensure it doesn’t get lost in the shuffle.\nBetter: Use Ed Discussion and reserve email for matters that are particular urgent and/or require privacy."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#course-website",
    "href": "slides/lecture01-1-intro.html#course-website",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Course Website",
    "text": "Course Website\nhttps://viveks.me/simulation-data-analysis\n\nCentral hub for information, schedule, and policies\nWill add link and some information to Canvas (assignment due dates, etc)"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#computing-tools",
    "href": "slides/lecture01-1-intro.html#computing-tools",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Computing Tools",
    "text": "Computing Tools\n\nCourse is programming language-agnostic.\nAssignments will have notebooks set up for Julia (environments, etc) on GitHub."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#some-tips-for-success",
    "href": "slides/lecture01-1-intro.html#some-tips-for-success",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Some Tips For Success",
    "text": "Some Tips For Success\n\nStart the homeworks early; this gives time to sort out conceptual problems and debug.\nAsk questions (in class and online) and try to help each other.\nGive me feedback!"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#next-classes",
    "href": "slides/lecture01-1-intro.html#next-classes",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Next Classes",
    "text": "Next Classes\n\nHypothesis testing and decision-making\nGenerative models"
  },
  {
    "objectID": "slides/lecture01-1-intro.html#assessments",
    "href": "slides/lecture01-1-intro.html#assessments",
    "title": "Welcome to BEE 4850/5850!",
    "section": "Assessments",
    "text": "Assessments\nHomework 1 available; due next Friday (2/7)."
  },
  {
    "objectID": "slides/lecture01-1-intro.html#references-scroll-for-full-list",
    "href": "slides/lecture01-1-intro.html#references-scroll-for-full-list",
    "title": "Welcome to BEE 4850/5850!",
    "section": "References (Scroll for Full List)",
    "text": "References (Scroll for Full List)\n\n\n\n\nDoss-Gollin, J., & Keller, K. (2023). A subjective Bayesian framework for synthesizing deep uncertainties in climate risk management. Earths Future, 11, e2022EF003044. https://doi.org/10.1029/2022ef003044\n\n\nErrickson, F. C., Keller, K., Collins, W. D., Srikrishnan, V., & Anthoff, D. (2021). Equity is more important for the social cost of methane than climate uncertainty. Nature, 592, 564–570. https://doi.org/10.1038/s41586-021-03386-6\n\n\nFagnant, C., Gori, A., Sebastian, A., Bedient, P. B., & Ensor, K. B. (2020). Characterizing spatiotemporal trends in extreme precipitation in Southeast Texas. Nat. Hazards, 104, 1597–1621. https://doi.org/10.1007/s11069-020-04235-x\n\n\nSrikrishnan, V., Guan, Y., Tol, R. S. J., & Keller, K. (2022). Probabilistic projections of baseline twenty-first century CO2 emissions using a simple calibrated integrated assessment model. Clim. Change, 170, 37. https://doi.org/10.1007/s10584-021-03279-7"
  },
  {
    "objectID": "slides/notebooks/rahmstorf.html",
    "href": "slides/notebooks/rahmstorf.html",
    "title": "Calibration of the Rahmstorf (2007) SLR Model",
    "section": "",
    "text": "import Pkg\nPkg.activate(dirname(@__DIR__))\nPkg.instantiate()\n\n  Activating project at `~/Teaching/BEE4850/website/slides`\n\n\n\nusing Distributions\nusing StatsBase\nusing CSV\nusing DataFrames\nusing Optim\nusing Turing\nusing Plots\nusing StatsPlots\nusing XLSX\nusing Interpolations"
  },
  {
    "objectID": "slides/notebooks/rahmstorf.html#load-environment",
    "href": "slides/notebooks/rahmstorf.html#load-environment",
    "title": "Calibration of the Rahmstorf (2007) SLR Model",
    "section": "",
    "text": "import Pkg\nPkg.activate(dirname(@__DIR__))\nPkg.instantiate()\n\n  Activating project at `~/Teaching/BEE4850/website/slides`\n\n\n\nusing Distributions\nusing StatsBase\nusing CSV\nusing DataFrames\nusing Optim\nusing Turing\nusing Plots\nusing StatsPlots\nusing XLSX\nusing Interpolations"
  },
  {
    "objectID": "slides/notebooks/rahmstorf.html#load-data",
    "href": "slides/notebooks/rahmstorf.html#load-data",
    "title": "Calibration of the Rahmstorf (2007) SLR Model",
    "section": "Load Data",
    "text": "Load Data\n\ndata_path = joinpath(dirname(@__DIR__), \"data\")\nnorm_yrs = 1951:1980\n\nsl_dat = DataFrame(CSV.File(joinpath(data_path, \"sealevel\", \"CSIRO_Recons_gmsl_yr_2015.csv\")))\n\nrename!(sl_dat, [:Year, :GMSLR, :SD]) # rename to make columns easier to work with\nsl_dat[!, :Year] .-= 0.5 # shift year to line up with years instead of being half-year \nsl_dat[!, :GMSLR] .-= mean(filter(row -&gt; row.Year ∈ norm_yrs, sl_dat)[!, :GMSLR]) # rescale to be relative to 1880-1900 mean for consistency with temperature anomaly\nsl_dat\n\n134×3 DataFrame109 rows omitted\n\n\n\nRow\nYear\nGMSLR\nSD\n\n\n\nFloat64\nFloat64\nFloat64\n\n\n\n\n1\n1880.0\n-118.157\n24.2\n\n\n2\n1881.0\n-112.557\n24.2\n\n\n3\n1882.0\n-129.357\n23.0\n\n\n4\n1883.0\n-124.057\n22.8\n\n\n5\n1884.0\n-103.157\n22.2\n\n\n6\n1885.0\n-104.657\n21.9\n\n\n7\n1886.0\n-107.057\n20.8\n\n\n8\n1887.0\n-112.657\n20.8\n\n\n9\n1888.0\n-110.557\n20.8\n\n\n10\n1889.0\n-108.957\n20.8\n\n\n11\n1890.0\n-106.957\n20.7\n\n\n12\n1891.0\n-108.657\n20.7\n\n\n13\n1892.0\n-105.457\n20.7\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n123\n2002.0\n68.9433\n6.8\n\n\n124\n2003.0\n78.1433\n6.9\n\n\n125\n2004.0\n77.7433\n6.9\n\n\n126\n2005.0\n77.8433\n6.8\n\n\n127\n2006.0\n82.1433\n6.8\n\n\n128\n2007.0\n84.0433\n7.1\n\n\n129\n2008.0\n92.7433\n6.8\n\n\n130\n2009.0\n98.5433\n6.9\n\n\n131\n2010.0\n106.243\n7.1\n\n\n132\n2011.0\n107.843\n7.5\n\n\n133\n2012.0\n116.643\n8.3\n\n\n134\n2013.0\n108.243\n8.9\n\n\n\n\n\n\n\n\n# load temperature data\ntemp_dat = DataFrame(CSV.File(joinpath(data_path, \"climate\", \"HadCRUT.5.0.1.0.analysis.summary_series.global.annual.csv\")))\nrename!(temp_dat, [:Year, :Temp, :Lower, :Upper]) # rename to make columns easier to work with\nfilter!(row -&gt; row.Year ∈ sl_dat[!, :Year], temp_dat) # reduce to the same years that we have SL data for\ntemp_normalize = mean(filter(row -&gt; row.Year ∈ norm_yrs, temp_dat)[!, :Temp]) # get renormalization to rescale temperature to 1880-1900 mean\ntemp_dat[!, :Temp] .-= temp_normalize\ntemp_dat[!, :Lower] .-= temp_normalize\ntemp_dat[!, :Upper] .-=  temp_normalize\n\nsl_plot = scatter(sl_dat[!, :Year], sl_dat[!, :GMSLR], yerr=sl_dat[!, :SD], color=:black, label=\"Observations\", ylabel=\"(mm)\", xlabel=\"Year\", title=\"Sea Level Anomaly\")\n\ntemp_plot = scatter(temp_dat[!, :Year], temp_dat[!, :Temp], yerr=(temp_dat[!, :Temp] - temp_dat[!, :Lower], temp_dat[!, :Upper] - temp_dat[!, :Temp]), color=:black, label=\"Observations\", ylabel=\"(°C)\", xlabel=\"Year\", title=\"Temperature\")\n\nplot(sl_plot, temp_plot, layout=(2, 1))"
  },
  {
    "objectID": "slides/notebooks/rahmstorf.html#model-specification",
    "href": "slides/notebooks/rahmstorf.html#model-specification",
    "title": "Calibration of the Rahmstorf (2007) SLR Model",
    "section": "Model Specification",
    "text": "Model Specification\nThe Rahmstorf (2007) model is:\n\\[\\frac{dH}{dt} = \\alpha (T - T_0)\\]\nDiscretizing:\n\\[H(t+1) = H(t) + \\Delta t \\alpha (T - T_0)\\]\nWe’ll set \\(\\Delta t = 1 \\text{yr}\\). This means we need the following parameters:\n\n\\(\\alpha\\): SLR sensitivity to temperature anomalies \\((mm/(yr \\cdot ^\\circ C))\\),\n\\(T\\_0\\): Null temperature anomaly (no SLR at this temperature) \\((^\\circ C)\\).\n\\(H_0 = H(0)\\): Initial SL anomaly \\((mm)\\).\n\nThis is the generative model.\nSimplest way to add residuals/measurement errors:\n\\[y_t \\sim N(H(t), \\sigma^2)\\]\n\nfunction rahmstorf_slr(params, temps; Δt=1)\n    α, b, T₀, H₀ = params\n    temp_diff = temps .- T₀\n    ΔT = [0; diff(temps)]\n    H = zeros(length(temps) + 1) # initialize storage\n    H[1] = H₀\n    for i = 2:length(H)\n        H[i] = H[i-1] + α * Δt * temp_diff[i-1] + b * ΔT[i-1]\n    end\n    return H[2:end]\nend\n\nrahmstorf_slr (generic function with 1 method)\n\n\n\nslr_ex = rahmstorf_slr((3.0, 8.0, -0.6, -120.0), temp_dat.Temp)\nsl_plot = scatter(sl_dat[!, :Year], sl_dat[!, :GMSLR], yerr=sl_dat[!, :SD], color=:black, label=\"Observations\", ylabel=\"(mm)\", xlabel=\"Year\", title=\"Sea Level Anomaly\")\nplot!(sl_plot, sl_dat[!, :Year], slr_ex, color=:red, linewidth=2, label=\"Baseline Values\")"
  },
  {
    "objectID": "slides/notebooks/rahmstorf.html#model-calibration-1-gaussian-iid-errors",
    "href": "slides/notebooks/rahmstorf.html#model-calibration-1-gaussian-iid-errors",
    "title": "Calibration of the Rahmstorf (2007) SLR Model",
    "section": "Model Calibration 1: Gaussian iid Errors",
    "text": "Model Calibration 1: Gaussian iid Errors\n\n# now the params include the standard deviation of the error model as well\nfunction gaussian_iid_homosked(params, temp_dat, slr_dat, Δt=1.0)\n    α, b, T₀, H₀, σ = params \n    slr_sim = rahmstorf_slr((α, b, T₀, H₀), temp_dat; Δt = Δt)\n    ll = sum(logpdf.(Normal.(slr_sim, σ), slr_dat))\n    return ll\nend\n\ngaussian_iid_homosked((4.6, 50.0, -0.5, -100.0, 5.0), temp_dat.Temp, sl_dat.GMSLR)\n\n-8874.472323023909\n\n\n\nlow_bds = [0.0, -100.0, -5.0, -200.0, 0.0]\nup_bds = [10.0, 100.0, 0.0, 0.0, 50.0]\np₀ = [3.4, 20.0, -0.5, -100.0, 10.0]\n\nmle_optim = optimize(p -&gt; -gaussian_iid_homosked(p, temp_dat.Temp, sl_dat.GMSLR), low_bds, up_bds, p₀)\np_mle = mle_optim.minimizer\n\n5-element Vector{Float64}:\n    1.9954491263556067\n   -7.531720172168937\n   -0.8657256422225121\n -117.60853103544478\n    5.837542474551414\n\n\n\nrahmstorf_homogauss_mle = rahmstorf_slr(p_mle, temp_dat.Temp)\n\n134-element Vector{Float64}:\n -116.35838317756556\n -115.57099317163348\n -113.80369210845214\n -112.23099107864499\n -110.23454730682374\n -109.45392169294495\n -108.79168143354109\n -107.32002185169173\n -107.09601755178424\n -106.6895289311967\n    ⋮\n   81.02621423657114\n   84.30750091691806\n   87.22436356816803\n   90.98330316458731\n   93.0668769172916\n   95.67532357715741\n   99.70322535899919\n  102.43559944097373\n  105.21406676172056\n\n\n\nplot!(sl_plot, sl_dat[!, :Year], rahmstorf_homogauss_mle, color=:blue, linewidth=2, label=\"Homoskedastic Gaussian Fit\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfunction gaussian_iid_ar(params, temp_dat, slr_dat, slr_err, Δt=1.0)\n    α, b, T₀, H₀, ρ, σ = params \n    slr_sim = rahmstorf_slr((α, b, T₀, H₀), temp_dat; Δt = Δt)\n\n    function ar_covariance_mat(σ, ρ, y_err)\n        H = abs.((1:length(y_err)) .- (1:(length(y_err)))') # compute the outer product to get the correlation lags\n        ζ_var = σ^2 / (1-ρ^2)\n        Σ = ρ.^H * ζ_var\n        for i in 1:length(y_err)\n            Σ[i, i] += y_err[i]^2\n        end\n        return Σ\n    end\n\n    # whiten residuals\n    Σ = ar_covariance_mat(σ, ρ, slr_err)\n    residuals = slr_dat - slr_sim\n    ll = logpdf(MvNormal(zeros(length(slr_dat)), Σ), residuals)\n    return ll\nend\n\ngaussian_iid_ar (generic function with 2 methods)\n\n\n\nlow_bds = [0.0, -100.0, -5.0, -200.0, -1.0, 0.0]\nup_bds = [10.0, 100.0, 0.0, 0.0, 1.0, 50.0]\np₀ = [3.4, 20.0, -0.5, -100.0, 0.0, 10.0]\n\nmle_optim = optimize(p -&gt; -gaussian_iid_ar(p, temp_dat.Temp, sl_dat.GMSLR, sl_dat.SD), low_bds, up_bds, p₀)\np_mle2 = mle_optim.minimizer\n@show p_mle;\n@show p_mle2;\n\np_mle = [1.9954491263556067, -7.531720172168937, -0.8657256422225121, -117.60853103544478, 5.837542474551414]\np_mle2 = [2.0016286168212845, -0.2098840209850033, -0.8304798210051328, -114.40746139053981, 0.855235460804076, 1.3967479590276937]\n\n\n\nrahmstorf_ar_mle = rahmstorf_slr(p_mle2, temp_dat.Temp)\nplot!(sl_plot, sl_dat[!, :Year], rahmstorf_ar_mle, color=:green, linewidth=2, label=\"Autoregressive Fit\")\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscatter(rahmstorf_ar_mle - rahmstorf_homogauss_mle)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresids_homogauss = sl_dat.GMSLR - rahmstorf_homogauss_mle\nqqnorm(resids_homogauss)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresids_ar = sl_dat.GMSLR - rahmstorf_ar_mle\nqqnorm(resids_ar)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npacf_homogauss = pacf(resids_homogauss, 1:5)\n\n5-element Vector{Float64}:\n  0.5192505198690146\n  0.08329400514537694\n  0.01867936459019353\n  0.04266165780968207\n -0.08903475089498139\n\n\n\npacf_ar = pacf(resids_ar, 1:5)\n\n5-element Vector{Float64}:\n  0.5449757003914797\n  0.10324950203859268\n  0.04874197808999718\n  0.05282629618990788\n -0.06474394579715283"
  },
  {
    "objectID": "slides/notebooks/rahmstorf.html#projections",
    "href": "slides/notebooks/rahmstorf.html#projections",
    "title": "Calibration of the Rahmstorf (2007) SLR Model",
    "section": "Projections",
    "text": "Projections\n\n\nrcpdf = XLSX.readtable(joinpath(data_path, \"climate\", \"cmip6_temps.xlsx\"), \"data\") |&gt; DataFrame\n\n# select only rows and columns with scenario names and data   \nselect!(rcpdf, Not([:Model, :Region, :Variable, :Unit, :Notes]))\nrcpdf = rcpdf[1:5, :]\n\n5×12 DataFrame\n\n\n\nRow\nScenario\n2005.0\n2010.0\n2020.0\n2030.0\n2040.0\n2050.0\n2060.0\n2070.0\n2080.0\n2090.0\n2100.0\n\n\n\nAny\nAny\nAny\nAny\nAny\nAny\nAny\nAny\nAny\nAny\nAny\nAny\n\n\n\n\n1\nSSP3-Baseline\n0.913386\n0.988055\n1.22575\n1.52381\n1.86419\n2.20251\n2.55093\n2.91152\n3.28435\n3.67242\n4.0711\n\n\n2\nSSP4-Baseline\n0.913386\n0.988562\n1.23721\n1.53287\n1.87389\n2.21631\n2.54615\n2.87451\n3.20099\n3.49421\n3.75829\n\n\n3\nSSP1-Baseline\n0.913386\n0.988441\n1.22283\n1.50579\n1.77074\n2.02206\n2.24641\n2.46035\n2.66872\n2.85621\n3.01774\n\n\n4\nSSP2-Baseline\n0.913386\n0.988977\n1.24141\n1.48262\n1.7615\n2.05281\n2.35626\n2.68133\n3.02315\n3.38796\n3.76329\n\n\n5\nSSP5-Baseline\n0.913386\n0.989124\n1.2658\n1.60296\n2.01275\n2.48049\n2.9755\n3.50379\n4.0499\n4.5757\n5.05197\n\n\n\n\n\n\n\n# reformat scenario names to SSPn-x.x\nrcpdf[!, :Scenario] = first.(rcpdf[!, :Scenario], 4)\n# sort by scenarios\nsort!(rcpdf, :Scenario)\n\n5×12 DataFrame\n\n\n\nRow\nScenario\n2005.0\n2010.0\n2020.0\n2030.0\n2040.0\n2050.0\n2060.0\n2070.0\n2080.0\n2090.0\n2100.0\n\n\n\nString\nAny\nAny\nAny\nAny\nAny\nAny\nAny\nAny\nAny\nAny\nAny\n\n\n\n\n1\nSSP1\n0.913386\n0.988441\n1.22283\n1.50579\n1.77074\n2.02206\n2.24641\n2.46035\n2.66872\n2.85621\n3.01774\n\n\n2\nSSP2\n0.913386\n0.988977\n1.24141\n1.48262\n1.7615\n2.05281\n2.35626\n2.68133\n3.02315\n3.38796\n3.76329\n\n\n3\nSSP3\n0.913386\n0.988055\n1.22575\n1.52381\n1.86419\n2.20251\n2.55093\n2.91152\n3.28435\n3.67242\n4.0711\n\n\n4\nSSP4\n0.913386\n0.988562\n1.23721\n1.53287\n1.87389\n2.21631\n2.54615\n2.87451\n3.20099\n3.49421\n3.75829\n\n\n5\nSSP5\n0.913386\n0.989124\n1.2658\n1.60296\n2.01275\n2.48049\n2.9755\n3.50379\n4.0499\n4.5757\n5.05197\n\n\n\n\n\n\n\nrcp_dat = zeros(nrow(rcpdf), 2100 - 1880 + 1)\nsplit_idx = 2013 - 1880 + 1\n\n134\n\n\n\nrcp_dat[:, 1:split_idx] .= temp_dat.Temp'\nrcp_dat\n\n5×221 Matrix{Float64}:\n -0.239226  -0.15564  -0.218924  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n -0.239226  -0.15564  -0.218924     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n -0.239226  -0.15564  -0.218924     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n -0.239226  -0.15564  -0.218924     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n -0.239226  -0.15564  -0.218924     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n\n\n\n\nfor (i, row) in enumerate(eachrow(rcpdf))\n    yrs = collect(2013:2100)\n    xs = collect(2010:10:2100)\n    interp = LinearInterpolation(xs, collect(rcpdf[i, 3:end]))\n    rcp_dat[i, split_idx:end] = interp.(yrs)\nend\nrcp_dat = DataFrame(rcp_dat, string.(collect(1880:2100)))\nrcp_dat.Scenario = rcpdf.Scenario\nrcp_dat\n\n5×222 DataFrame122 columns omitted\n\n\n\nRow\n1880\n1881\n1882\n1883\n1884\n1885\n1886\n1887\n1888\n1889\n1890\n1891\n1892\n1893\n1894\n1895\n1896\n1897\n1898\n1899\n1900\n1901\n1902\n1903\n1904\n1905\n1906\n1907\n1908\n1909\n1910\n1911\n1912\n1913\n1914\n1915\n1916\n1917\n1918\n1919\n1920\n1921\n1922\n1923\n1924\n1925\n1926\n1927\n1928\n1929\n1930\n1931\n1932\n1933\n1934\n1935\n1936\n1937\n1938\n1939\n1940\n1941\n1942\n1943\n1944\n1945\n1946\n1947\n1948\n1949\n1950\n1951\n1952\n1953\n1954\n1955\n1956\n1957\n1958\n1959\n1960\n1961\n1962\n1963\n1964\n1965\n1966\n1967\n1968\n1969\n1970\n1971\n1972\n1973\n1974\n1975\n1976\n1977\n1978\n1979\n⋯\n\n\n\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\n⋯\n\n\n\n\n1\n-0.239226\n-0.15564\n-0.218924\n-0.269868\n-0.415714\n-0.394518\n-0.344298\n-0.42218\n-0.302773\n-0.17329\n-0.430252\n-0.324709\n-0.430953\n-0.418013\n-0.407158\n-0.372146\n-0.207401\n-0.183194\n-0.409186\n-0.278828\n-0.157873\n-0.216823\n-0.362378\n-0.45672\n-0.520955\n-0.331145\n-0.242533\n-0.427552\n-0.437265\n-0.459159\n-0.454418\n-0.462599\n-0.399067\n-0.390547\n-0.185986\n-0.115238\n-0.343604\n-0.466414\n-0.347978\n-0.248912\n-0.221975\n-0.164071\n-0.262622\n-0.241325\n-0.235456\n-0.205819\n-0.0462295\n-0.152799\n-0.130156\n-0.316151\n-0.100199\n-0.0267918\n-0.0688557\n-0.245738\n-0.0977309\n-0.129453\n-0.092915\n0.057407\n0.0644052\n0.0358088\n0.152542\n0.114735\n0.078012\n0.08302\n0.220711\n0.119694\n-0.0422069\n-0.0145996\n-0.0480553\n-0.0671963\n-0.150016\n0.015452\n0.0919605\n0.154237\n-0.0401443\n-0.120704\n-0.18656\n0.041271\n0.0589734\n0.0286011\n-0.0388811\n0.0566085\n0.0125515\n0.0398\n-0.229261\n-0.127782\n-0.0722787\n-0.0409104\n-0.0920264\n0.0452392\n-0.00850064\n-0.129327\n-0.0172212\n0.126539\n-0.0959314\n-0.0341483\n-0.139256\n0.179694\n0.0818617\n0.167464\n⋯\n\n\n2\n-0.239226\n-0.15564\n-0.218924\n-0.269868\n-0.415714\n-0.394518\n-0.344298\n-0.42218\n-0.302773\n-0.17329\n-0.430252\n-0.324709\n-0.430953\n-0.418013\n-0.407158\n-0.372146\n-0.207401\n-0.183194\n-0.409186\n-0.278828\n-0.157873\n-0.216823\n-0.362378\n-0.45672\n-0.520955\n-0.331145\n-0.242533\n-0.427552\n-0.437265\n-0.459159\n-0.454418\n-0.462599\n-0.399067\n-0.390547\n-0.185986\n-0.115238\n-0.343604\n-0.466414\n-0.347978\n-0.248912\n-0.221975\n-0.164071\n-0.262622\n-0.241325\n-0.235456\n-0.205819\n-0.0462295\n-0.152799\n-0.130156\n-0.316151\n-0.100199\n-0.0267918\n-0.0688557\n-0.245738\n-0.0977309\n-0.129453\n-0.092915\n0.057407\n0.0644052\n0.0358088\n0.152542\n0.114735\n0.078012\n0.08302\n0.220711\n0.119694\n-0.0422069\n-0.0145996\n-0.0480553\n-0.0671963\n-0.150016\n0.015452\n0.0919605\n0.154237\n-0.0401443\n-0.120704\n-0.18656\n0.041271\n0.0589734\n0.0286011\n-0.0388811\n0.0566085\n0.0125515\n0.0398\n-0.229261\n-0.127782\n-0.0722787\n-0.0409104\n-0.0920264\n0.0452392\n-0.00850064\n-0.129327\n-0.0172212\n0.126539\n-0.0959314\n-0.0341483\n-0.139256\n0.179694\n0.0818617\n0.167464\n⋯\n\n\n3\n-0.239226\n-0.15564\n-0.218924\n-0.269868\n-0.415714\n-0.394518\n-0.344298\n-0.42218\n-0.302773\n-0.17329\n-0.430252\n-0.324709\n-0.430953\n-0.418013\n-0.407158\n-0.372146\n-0.207401\n-0.183194\n-0.409186\n-0.278828\n-0.157873\n-0.216823\n-0.362378\n-0.45672\n-0.520955\n-0.331145\n-0.242533\n-0.427552\n-0.437265\n-0.459159\n-0.454418\n-0.462599\n-0.399067\n-0.390547\n-0.185986\n-0.115238\n-0.343604\n-0.466414\n-0.347978\n-0.248912\n-0.221975\n-0.164071\n-0.262622\n-0.241325\n-0.235456\n-0.205819\n-0.0462295\n-0.152799\n-0.130156\n-0.316151\n-0.100199\n-0.0267918\n-0.0688557\n-0.245738\n-0.0977309\n-0.129453\n-0.092915\n0.057407\n0.0644052\n0.0358088\n0.152542\n0.114735\n0.078012\n0.08302\n0.220711\n0.119694\n-0.0422069\n-0.0145996\n-0.0480553\n-0.0671963\n-0.150016\n0.015452\n0.0919605\n0.154237\n-0.0401443\n-0.120704\n-0.18656\n0.041271\n0.0589734\n0.0286011\n-0.0388811\n0.0566085\n0.0125515\n0.0398\n-0.229261\n-0.127782\n-0.0722787\n-0.0409104\n-0.0920264\n0.0452392\n-0.00850064\n-0.129327\n-0.0172212\n0.126539\n-0.0959314\n-0.0341483\n-0.139256\n0.179694\n0.0818617\n0.167464\n⋯\n\n\n4\n-0.239226\n-0.15564\n-0.218924\n-0.269868\n-0.415714\n-0.394518\n-0.344298\n-0.42218\n-0.302773\n-0.17329\n-0.430252\n-0.324709\n-0.430953\n-0.418013\n-0.407158\n-0.372146\n-0.207401\n-0.183194\n-0.409186\n-0.278828\n-0.157873\n-0.216823\n-0.362378\n-0.45672\n-0.520955\n-0.331145\n-0.242533\n-0.427552\n-0.437265\n-0.459159\n-0.454418\n-0.462599\n-0.399067\n-0.390547\n-0.185986\n-0.115238\n-0.343604\n-0.466414\n-0.347978\n-0.248912\n-0.221975\n-0.164071\n-0.262622\n-0.241325\n-0.235456\n-0.205819\n-0.0462295\n-0.152799\n-0.130156\n-0.316151\n-0.100199\n-0.0267918\n-0.0688557\n-0.245738\n-0.0977309\n-0.129453\n-0.092915\n0.057407\n0.0644052\n0.0358088\n0.152542\n0.114735\n0.078012\n0.08302\n0.220711\n0.119694\n-0.0422069\n-0.0145996\n-0.0480553\n-0.0671963\n-0.150016\n0.015452\n0.0919605\n0.154237\n-0.0401443\n-0.120704\n-0.18656\n0.041271\n0.0589734\n0.0286011\n-0.0388811\n0.0566085\n0.0125515\n0.0398\n-0.229261\n-0.127782\n-0.0722787\n-0.0409104\n-0.0920264\n0.0452392\n-0.00850064\n-0.129327\n-0.0172212\n0.126539\n-0.0959314\n-0.0341483\n-0.139256\n0.179694\n0.0818617\n0.167464\n⋯\n\n\n5\n-0.239226\n-0.15564\n-0.218924\n-0.269868\n-0.415714\n-0.394518\n-0.344298\n-0.42218\n-0.302773\n-0.17329\n-0.430252\n-0.324709\n-0.430953\n-0.418013\n-0.407158\n-0.372146\n-0.207401\n-0.183194\n-0.409186\n-0.278828\n-0.157873\n-0.216823\n-0.362378\n-0.45672\n-0.520955\n-0.331145\n-0.242533\n-0.427552\n-0.437265\n-0.459159\n-0.454418\n-0.462599\n-0.399067\n-0.390547\n-0.185986\n-0.115238\n-0.343604\n-0.466414\n-0.347978\n-0.248912\n-0.221975\n-0.164071\n-0.262622\n-0.241325\n-0.235456\n-0.205819\n-0.0462295\n-0.152799\n-0.130156\n-0.316151\n-0.100199\n-0.0267918\n-0.0688557\n-0.245738\n-0.0977309\n-0.129453\n-0.092915\n0.057407\n0.0644052\n0.0358088\n0.152542\n0.114735\n0.078012\n0.08302\n0.220711\n0.119694\n-0.0422069\n-0.0145996\n-0.0480553\n-0.0671963\n-0.150016\n0.015452\n0.0919605\n0.154237\n-0.0401443\n-0.120704\n-0.18656\n0.041271\n0.0589734\n0.0286011\n-0.0388811\n0.0566085\n0.0125515\n0.0398\n-0.229261\n-0.127782\n-0.0722787\n-0.0409104\n-0.0920264\n0.0452392\n-0.00850064\n-0.129327\n-0.0172212\n0.126539\n-0.0959314\n-0.0341483\n-0.139256\n0.179694\n0.0818617\n0.167464\n⋯\n\n\n\n\n\n\n\n@show p_mle2;\n@show p_mle;\n\np_mle2 = [2.0016286168212845, -0.2098840209850033, -0.8304798210051328, -114.40746139053981, 0.855235460804076, 1.3967479590276937]\np_mle = [1.9954491263556067, -7.531720172168937, -0.8657256422225121, -117.60853103544478, 5.837542474551414]\n\n\n\nrahmstorf_iid_proj_ssp5 = rahmstorf_slr(p_mle, collect(rcp_dat[5, 1:end-1]))\nrahmstorf_ar_proj_ssp5 = rahmstorf_slr(p_mle2, collect(rcp_dat[5, 1:end-1]))\nplot(1880:2100, rahmstorf_iid_proj_ssp5)\nplot!(1880:2100, rahmstorf_ar_proj_ssp5)\nxlims!((2050, 2100))\nylims!(200, 800)"
  },
  {
    "objectID": "project/presentation.html",
    "href": "project/presentation.html",
    "title": "Project Presentation Instructions",
    "section": "",
    "text": "m These are the instructions for your final project presentation, which will be given in class on May 6, 2024."
  },
  {
    "objectID": "project/presentation.html#presentation-submission",
    "href": "project/presentation.html#presentation-submission",
    "title": "Project Presentation Instructions",
    "section": "Presentation Submission",
    "text": "Presentation Submission\nPlease submit your slides in PDF format by email to Prof. Srikrishnan no later than Friday, May 3, 2024."
  },
  {
    "objectID": "project/presentation.html#presentation-guidelines",
    "href": "project/presentation.html#presentation-guidelines",
    "title": "Project Presentation Instructions",
    "section": "Presentation Guidelines",
    "text": "Presentation Guidelines\nYour presentation should be short (10 minutes, which will be timed and strictly enforced in the interest of time). Your presentation should concisely describe your key question, your scientific hypothesis, and your results and conclusions. Note that a 10 minute presentation means that you should aim for no more than 5-6 slides (not including titles, transitions, or wrap-ups/references)."
  },
  {
    "objectID": "project/report.html",
    "href": "project/report.html",
    "title": "Project Report Instructions",
    "section": "",
    "text": "These are the instructions for your final project report. The report should be submitted on Gradescope during finals week, but no later than May 17, 2024."
  },
  {
    "objectID": "project/report.html#report-guidelines",
    "href": "project/report.html#report-guidelines",
    "title": "Project Report Instructions",
    "section": "Report Guidelines",
    "text": "Report Guidelines\nThe proposal should have the following components with 1 inch margins (on all sides) and at least 11 point font. Some of this may be repeated from your proposal and/or simulation study; this is fine.\n\nBackground: Provide background on your project topic and the relevant environmental system. Why is this topic interesting or valuable to understand? What are you trying to understand or what do you hyptohesize?\nData and Models: What data are you using? Provide exploratory plots. What model(s) are you using to explore the data and your hypotheses? Include details about the probability or statistical assumptions your model is making and how well-founded you think these are in light of the system and the data.\nResults: What have you learned from your analysis? Were your hypotheses well founded? What visual or quantative evidence are you using to support these conclusions?\nDiscussion: Reflect on the data and models that you used. How well suited were they for the analysis? In retrospect, what might have been better? How might they have biased or influenced your results?\nReferences: Make sure to include any external sources referenced during the course of completing your project.\n\nYou do not need to include any code in your reports, but if you do, please reserve this for an appendix at the end of the report, not mixed in with the above sections. If you generated a figure or fit a model using the code, include the figure along with the relevant text but do not include code. There is no page minimum or maximum on the report, but it should include, clearly, all of the above content."
  },
  {
    "objectID": "rubrics/standard.html",
    "href": "rubrics/standard.html",
    "title": "Standard Rubrics",
    "section": "",
    "text": "These rubrics are intended as a description for students and a guide to the grader for how to assign partial credit.",
    "crumbs": [
      "Homework",
      "Standard Rubric"
    ]
  },
  {
    "objectID": "rubrics/standard.html#meta-rubric",
    "href": "rubrics/standard.html#meta-rubric",
    "title": "Standard Rubrics",
    "section": "Meta-Rubric",
    "text": "Meta-Rubric\n\nIndividual problems may vary from these standard rubrics based on the assessed learning outcomes for the problem, but they should provide an overview of what features students ought to be included in a given solution.\nEach bullet point should appear as a separate rubric item in Gradescope. We will use positive grading (points awarded for each component).\nEach standard rubric describes partial credit for a problem with 10 points. Partial credit for (sub)problems worth less than 10 points should be scaled appropriately. Full problems usually combine one of the modeling methods with some interpretation questions, so the rubric will be a combination of the individual components.\nGenerally, rubrics for 10 point “entire problems” can be summarized with the following:\n\n+4 points for an answer with a correct implementation (including model setup).\n+2 points for the correct solution (including details such as labels, units, etc.).\n+4 points for the interpretation. These points may be broken up across subproblems, but the general assignment of points should follow this summary.\n\nNote that the points may not scale with the distribution of work for a given problem. That’s life! It may take more work to derive and implement your model than it does to intrepret your results, but in this class we care a lot about your ability to critically evaluate and interpret modeling results.\nSometimes problems will be broken up into\nThe graders cannot read your mind and will not try to. Submissions that are unclear for any reason, including but not limited to unclear syntax (English or code), uncommented code, lack of reasoning or derivation, too much detail or writing, will not be given credit. The grader has complete discretion here. If something in your solution is ambiguous, the grader has been instructed to interpret it the “wrong” way. Clear responses are a sign of understanding, which is part of what we’re assessing.\nYou will not be doubly penalized for getting the “right” solution to the “wrong” setup (you would have been penalized above), but this requires the grader to be able to easily identify that your implementation is correct given the wrong model. If your implementation is unclear, you are likely to lose points here because we have no way of knowing that your answer is “right” without re-coding your problem.\nYou will not be given credit for your code (this isn’t a programming class); the code is a means to solving the problem, and we’re more interested in how you set up the problem and interpret the solution. This does mean that code that is sloppy but works is perfectly acceptable. However, your code may get you partial credit if the TA can easily find where you made a mistake, so make sure your submitted code is well commented. It’s important to write down the mathematics of what your code is trying to implement, because the TA will only take a cursory look at your code.\nSome rubrics include “deadly sins” which will cause an immediate zero to be given for the problem. Do not do these!\nRegardless of the problem type, the following penalties will be applied per incident:\n\n-1 for missing units.",
    "crumbs": [
      "Homework",
      "Standard Rubric"
    ]
  },
  {
    "objectID": "rubrics/standard.html#standard-rubrics",
    "href": "rubrics/standard.html#standard-rubrics",
    "title": "Standard Rubrics",
    "section": "Standard Rubrics",
    "text": "Standard Rubrics\n\nSimulation Rubric\n10 points =\n\n+4 for the model derivation.\n\nThis includes any relevant reasoning from mass-balance or other principles.\n\n+4 for discretizing the model correctly.\n\n-1 if no justification is provided for the chosen step-size(s).\n\n+2 points for obtaining the correct solution.\n\n-1 if the error is from a minor bug. I recommend providing an explicit sketch of your code’s procedure, so if that is correct any differences are likely to be the result of a minor bug, such as a misentered number (not ideal, but you understand what you’re doing!). Otherwise you are relying on the TA to interpret your code.\n\n\n\n\nMonte Carlo Rubric\n10 points =\n\n+4 for a clear English explanation of the sampling plan\n\nYou must justify each distribution used, including your choice of mean and standard deviation.\n\n+2 for justification of Monte Carlo sample size.\n\nDeadly Sin: No points will be given for a Monte Carlo solution with an arbitrary sample size.\n\n+2 for an estimate of the Monte Carlo standard error.\n+1 for setting a seed for reproducibility.\n+2 for correct estimate.\n\nDeviations from the posted solution are ok if they’re within the Monte Carlo standard error.\n\n\n\n\nFigure Rubrics\n10 points =\n\n+2 for appropriate choice of axes.\n\nDeadly Sin!: No points will be given for the figure if the axes are not labelled.\n\n+4 for the correct data series.\n+2 for a descriptive legend.\n+2 for a succinct description or caption.\n\n-1 for a description which is too wordy but contains the relevant information.\n\n\n\n\nInterpretation Rubrics\n10 points =\n\n+4 for specific reference to the modeling results;\n\n-2 if the results are not specifically referenced and the reader has to refer to the previous problems to understand the interpretation.\n\n+6 for thoughtfulness of the interpretation;\n\n-4 for not specifically referencing relevant model assumptions;\nDeadly Sin: Do not neglect fundamental engineering principles in any of your interpretations. You will be given a zero if you make a recommendation which e.g. violates an engineering code or standard.\nDeadly Sin: Completely generic interpretations will result in zero points.",
    "crumbs": [
      "Homework",
      "Standard Rubric"
    ]
  },
  {
    "objectID": "tutorials/turing-mcmc.html",
    "href": "tutorials/turing-mcmc.html",
    "title": "Markov Chain Monte Carlo With Turing",
    "section": "",
    "text": "This tutorial will give some examples of using Turing.jl and Markov Chain Monte Carlo to sample from posterior distributions.",
    "crumbs": [
      "Julia Tutorials",
      "MCMC with Turing"
    ]
  },
  {
    "objectID": "tutorials/turing-mcmc.html#overview",
    "href": "tutorials/turing-mcmc.html#overview",
    "title": "Markov Chain Monte Carlo With Turing",
    "section": "",
    "text": "This tutorial will give some examples of using Turing.jl and Markov Chain Monte Carlo to sample from posterior distributions.",
    "crumbs": [
      "Julia Tutorials",
      "MCMC with Turing"
    ]
  },
  {
    "objectID": "tutorials/turing-mcmc.html#setup",
    "href": "tutorials/turing-mcmc.html#setup",
    "title": "Markov Chain Monte Carlo With Turing",
    "section": "Setup",
    "text": "Setup\n\nusing Turing\nusing Distributions\nusing Plots\ndefault(fmt = :png) # the tide gauge data is long, this keeps images a manageable size\nusing LaTeXStrings\nusing StatsPlots\nusing Measures\nusing StatsBase\nusing Optim\nusing Random\nusing DataFrames\nusing DataFramesMeta\nusing Dates\nusing CSV\n\nAs this tutorial involves random number generation, we will set a random seed to ensure reproducibility.\n\n\nRandom.seed!(1);",
    "crumbs": [
      "Julia Tutorials",
      "MCMC with Turing"
    ]
  },
  {
    "objectID": "tutorials/turing-mcmc.html#fitting-a-linear-regression-model",
    "href": "tutorials/turing-mcmc.html#fitting-a-linear-regression-model",
    "title": "Markov Chain Monte Carlo With Turing",
    "section": "Fitting A Linear Regression Model",
    "text": "Fitting A Linear Regression Model\nLet’s start with a simple example: fitting a linear regression model to simulated data.\n\n\n\n\n\n\nPositive Control Tests\n\n\n\nSimulating data with a known data-generating process and then trying to obtain the parameters for that process is an important step in any workflow.\n\n\n\nSimulating Data\nThe data-generating process for this example will be: \\[\n\\begin{gather}\ny = 5 + 2x + \\varepsilon \\\\\n\\varepsilon \\sim \\text{Normal}(0, 3),\n\\end{gather}\n\\] where \\(\\varepsilon\\) is so-called “white noise”, which adds stochasticity to the data set. The generated dataset is shown in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Scatterplot of our generated data.\n\n\n\n\n\nModel Specification\nThe statistical model for a standard linear regression problem is \\[\n\\begin{gather}\ny = a + bx + \\varepsilon \\\\\n\\varepsilon \\sim \\text{Normal}(0, \\sigma).\n\\end{gather}\n\\]\nRearranging, we can rewrite the likelihood function as: \\[y \\sim \\text{Normal}(\\mu, \\sigma),\\] where \\(\\mu = a + bx\\). This means that we have three parameters to fit: \\(a\\), \\(b\\), and \\(\\sigma^2\\).\nNext, we need to select priors on our parameters. We’ll use relatively generic distributions to avoid using the information we have (since we generated the data ourselves), but in practice, we’d want to use any relevant information that we had from our knowledge of the problem. Let’s use relatively diffuse normal distributions for the trend parameters \\(a\\) and \\(b\\) and a half-normal distribution (a normal distribution truncated at 0, to only allow positive values) for the variance \\(\\sigma^2\\), as recommended by Gelman (2006).\n\nGelman, A. (2006). Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper). Bayesian Anal., 1(3), 515–533. https://doi.org/10.1214/06-BA117A\n\\[\n\\begin{gather}\na \\sim \\text{Normal(0, 10)} \\\\\nb \\sim \\text{Normal(0, 10)} \\\\\n\\sigma \\sim \\text{Half-Normal}(0, 25)\n\\end{gather}\n\\]\n\n\nUsing Turing\n\nCoding the Model\nTuring.jl uses the @model macro to specify the model function. We’ll follow the setup in the Turing documentation.\nTo specify distributions on parameters (and the data, which can be thought of as uncertain parameters in Bayesian statistics), use a tilde ~, and use equals = for transformations (which we don’t have in this case).\n\n\n@model function linear_regression(x, y)\n    # set priors\n1    σ ~ truncated(Normal(0, 25); lower=0)\n2    a ~ Normal(0, 10)\n    b ~ Normal(0, 10)\n\n    # compute the likelihood\n3    for i = 1:length(y)\n        # compute the mean value for the data point\n        μ = a + b * x[i]\n        y[i] ~ Normal(μ, σ)\n    end\nend\n\n\n1\n\nStandard deviations must be positive, so we use a normal distribution truncated at zero.\n\n2\n\nWe’ll keep these both relative uninformative to reflect a more “realistic” modeling scenario.\n\n3\n\nIn this case, we specify the likelihood with a loop. We could also rewrite this as a joint likelihood over all of the data using linear algebra, which might be more efficient for large and/or complex models or datasets, but the loop is more readable in this simple case.\n\n\n\n\nlinear_regression (generic function with 2 methods)\n\n\n\n\nFitting The Model\nNow we can call the sampler to draw from the posterior. We’ll use the No-U-Turn sampler (Hoffman & Gelman, 2014), which is a Hamiltonian Monte Carlo algorithm (a different category of MCMC sampler than the Metropolis-Hastings algorithm discussed in class). We’ll also use 4 chains so we can test that the chains are well-mixed, and each chain will be run for 5,000 iterations1\n\nHoffman, M. D., & Gelman, A. (2014). The No-U-Turn sampler: Adaptively setting path lengths in Hamiltonian Monte Carlo. J. Mach. Learn. Res., 15(47), 1593–1623.\n1 Hamiltonian Monte Carlo samplers often need to be run for fewer iterations than Metropolis-Hastings samplers, as the exploratory step uses information about the gradient of the statistical model, versus the random walk of Metropolis-Hastings. The disadvantage is that this gradient information must be available, which is not always the case for external simulation models. Simulation models coded in Julia can usually be automatically differentiated by Turing’s tools, however.\n# set up the sampler\n1model = linear_regression(x, y)\n2n_chains = 4\n3n_per_chain = 5000\n4chain = sample(model, NUTS(), MCMCThreads(), n_per_chain, n_chains, drop_warmup=true)\n5@show chain\n\n\n1\n\nInitialize the model with the data.\n\n2\n\nWe use multiple chains to help diagnose convergence.\n\n3\n\nThis sets the number of iterations for each chain.\n\n4\n\nSample from the posterior using NUTS and drop the iterations used to warmup the sampler. The MCMCThreads() call tells the sampler to use available processor threads for the multiple chains, but it will just sample them in serial if only one thread exists.\n\n5\n\nThe @show macro makes the display of the output a bit cleaner.\n\n\n\n\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/Es490/src/sample.jl:307\nSampling (1 threads)   0%|                              |  ETA: N/A\n┌ Info: Found initial step size\n└   ϵ = 0.00625\n┌ Info: Found initial step size\n└   ϵ = 0.000390625\nSampling (1 threads)  25%|███████▌                      |  ETA: 0:00:20\nSampling (1 threads)  50%|███████████████               |  ETA: 0:00:07\n┌ Info: Found initial step size\n└   ϵ = 0.0015625\n┌ Info: Found initial step size\n└   ϵ = 0.0015625\nSampling (1 threads)  75%|██████████████████████▌       |  ETA: 0:00:03\nSampling (1 threads) 100%|██████████████████████████████| Time: 0:00:07\nSampling (1 threads) 100%|██████████████████████████████| Time: 0:00:08\nchain = MCMC chain (5000×15×4 Array{Float64, 3})\n\n\nChains MCMC chain (5000×15×4 Array{Float64, 3}):\n\nIterations        = 1001:1:6000\nNumber of chains  = 4\nSamples per chain = 5000\nWall duration     = 5.92 seconds\nCompute duration  = 5.03 seconds\nparameters        = σ, a, b\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters      mean       std      mcse    ess_bulk    ess_tail      rhat   ⋯\n      Symbol   Float64   Float64   Float64     Float64     Float64   Float64   ⋯\n\n           σ    5.2746    0.9709    0.0100   9701.6225   8323.5830    1.0003   ⋯\n           a    4.5375    2.5885    0.0304   7289.4473   7823.1451    1.0011   ⋯\n           b    1.9524    0.2074    0.0024   7424.2165   8626.2980    1.0012   ⋯\n                                                                1 column omitted\n\nQuantiles\n  parameters      2.5%     25.0%     50.0%     75.0%     97.5%\n      Symbol   Float64   Float64   Float64   Float64   Float64\n\n           σ    3.7793    4.5850    5.1350    5.8165    7.5288\n           a   -0.7045    2.8680    4.5676    6.2559    9.5871\n           b    1.5485    1.8163    1.9506    2.0852    2.3759\n\n\nHow can we interpret the output? The first parts of the summary statistics are straightforward: we get the mean, standard deviation, and Monte Carlo standard error (mcse) of each parameter. We also get information about the effective sample size (ESS)2 and \\(\\hat{R}\\), which measures the ratio of within-chain variance and across-chain variance as a check for convergence3.\n2 The ESS reflects the efficiency of the sampler: this is an estimate of the equivalent number of independent samples; the more correlated the samples, the lower the ESS.3 The closer \\(\\hat{R}\\) is to 1, the better.In this case, we can see that we were generally able to recover the “true” data-generating values of \\(\\sigma = 4\\) and \\(b = 2\\), but \\(a\\) is slightly off (the mean is 3, rather than the data-generating value of 5). In fact, there is substantial uncertainty about \\(a\\), with a 95% credible interval of \\((3.1, 11.4)\\) (compared to \\((1.4, 2.2)\\) for \\(b\\)). This isn’t surprising: given the variance of the noise \\(\\sigma^2\\), there are many different intercepts which could fit within that spread.\nLet’s now plot the chains for visual inspection.\n\nplot(chain)\n\n\n\n\n\n\nFigure 2: Output from the MCMC sampler. Each row corresponds to a different parameter: \\(\\sigma\\), \\(a\\), and \\(b\\). Each chain is shown in a different color. The left column shows the sampler traceplots, and the right column the resulting posterior distributions.\n\n\n\n\nWe can see from Figure 2 that our chains mixed well and seem to have converged to similar distributions! The traceplots have a “hairy caterpiller” appearance, suggesting relatively little autocorrelation. We can also see how much more uncertainty there is with the intercept \\(a\\), while the slope \\(b\\) is much more constrained.\nAnother interesting comparison we can make is with the maximum-likelihood estimate (MLE), which we can obtain through optimization.\n\nmle_model = linear_regression(x, y)\n1mle = optimize(mle_model, MLE())\ncoef(mle)\n\n\n1\n\nThis is where we use the Optim.jl package in this tutorial.\n\n\n\n\n\n3-element Named Vector{Float64}\nA  │ \n───┼────────\nσ  │ 4.65611\na  │ 4.82545\nb  │ 1.93216\n\nWe could also get the maximum a posteriori (MAP) estimate, which includes the prior density, by replacing MLE() with MAP().\n\n\n\nModel Diagnostics and Posterior Predictive Checks\nOne advantage of the Bayesian modeling approach here is that we have access to a generative model, or a model which we can use to generate datasets. This means that we can now use Monte Carlo simulation, sampling from our posteriors, to look at how uncertainty in the parameter estimates propagates through the model. Let’s write a function which gets samples from the MCMC chains and generates datasets.\n\nfunction mc_predict_regression(x, chain)\n    # get the posterior samples\n1    a = Array(group(chain, :a))\n    b = Array(group(chain, :b))\n    σ = Array(group(chain, :σ))\n\n    # loop and generate alternative realizations\n    μ = a' .+ x * b'\n    y = zeros((length(x), length(a)))\n    for i = 1:length(a)\n        y[:, i] = rand.(Normal.(μ[:, i], σ[i]))\n    end\n    return y\nend\n\n\n1\n\nThe Array(group()) syntax is more general than we need, but is useful if we have multiple variables which were sampled as a group, for example multiple regression coefficients. Otherwise, we can just use e.g. Array(chain, :a).\n\n\n\n\nmc_predict_regression (generic function with 1 method)\n\n\nNow we can generate a predictive interval and median and compare to the data.\n\nx_pred = 0:20\ny_pred = mc_predict_regression(x_pred, chain)\n\n21×20000 Matrix{Float64}:\n  7.22896   8.9257    5.93747  -4.03355  …   3.57314  10.7674    3.25831\n  3.8172   13.0222    3.18781  -1.64646     16.6799   16.5975    9.57413\n 12.9365    4.06479   6.9575    3.04053     15.7631    8.27252   8.61278\n 10.8627   15.4615   10.4911    6.97876     -1.16307  11.4081    7.25651\n 17.7051    8.38916  17.1919    8.75481      8.78966  11.2722    8.31788\n 18.5724   17.7875   21.4972   18.7395   …  18.5317   13.493    14.189\n 14.769    19.0975   20.0864   16.474       19.7747   20.2629   17.5398\n 19.7795   19.8856   13.1846   20.2827      20.4983   17.7751   22.641\n 14.9981   11.3935   18.5689    9.5507      10.5856   12.8072   20.2831\n 16.0427   27.8318   18.8602   16.0424      18.397    16.4161   21.9535\n  ⋮                                      ⋱                      \n 25.1538   28.4962   26.6147   23.6988      27.3528   29.1293   27.9151\n 28.2126   28.6421   32.1798   26.4099      25.6423   29.2611   38.6387\n 30.8316   28.7773   34.7861   26.8182      32.1743   27.8347   32.1903\n 39.6775   25.909    40.3352   29.3356   …  38.3202   23.6681   32.6925\n 44.2107   34.9485   26.752    33.1982      39.8699   42.7887   42.9509\n 23.4219   42.2734   41.4761   38.6141      27.0471   29.6089   33.9472\n 36.0279   38.6764   35.0559   33.729       34.7149   38.8271   35.4149\n 41.5712   36.6338   41.387    42.6084      36.8699   41.5654   48.2626\n 39.9282   35.0061   42.2547   45.7744   …  41.7894   40.8785   41.5882\n\n\nNotice the dimension of y_pred: we have 20,000 columns, because we have 4 chains with 5,000 samples each. If we had wanted to subsample (which might be necessary if we had hundreds of thousands or millions of samples), we could have done that within mc_linear_regression before simulation.\n\n# get the boundaries for the 95% prediction interval and the median\ny_ci_low = quantile.(eachrow(y_pred), 0.025)\ny_ci_hi = quantile.(eachrow(y_pred), 0.975)\ny_med = quantile.(eachrow(y_pred), 0.5)\n\nNow, let’s plot the prediction interval and median, and compare to the original data.\n\n# plot prediction interval\n1plot(x_pred, y_ci_low, fillrange=y_ci_hi, xlabel=L\"$x$\", ylabel=L\"$y$\", fillalpha=0.3, fillcolor=:blue, label=\"95% Prediction Interval\", legend=:topleft, linealpha=0)\n2plot!(x_pred, y_med, color=:blue, label=\"Prediction Median\")\n3scatter!(x, y, color=:red, label=\"Data\")\n\n\n1\n\nPlot the 95% posterior prediction interval as a shaded blue ribbon.\n\n2\n\nPlot the posterior prediction median as a blue line.\n\n3\n\nPlot the data as discrete red points.\n\n\n\n\n\n\n\n\n\nFigure 3: Posterior 95% predictive interval and median for the linear regression model. The data is plotted in red for comparison.\n\n\n\n\nFrom Figure 3, it looks like our model might be slightly under-confident, as with 20 data points, we would expect 5% of them (or 1 data point) to be outside the 95% prediction interval. It’s hard to tell with only 20 data points, though! We could resolve this by tightening our priors, but this depends on how much information we used to specify them in the first place. The goal shouldn’t be to hit a specific level of uncertainty, but if there is a sound reason to tighten the priors, we could do so.\nNow let’s look at the residuals from the posterior median and the data. The partial autocorrelations plotted in Figure 4 are not fully convincing, as there are large autocorrelation coefficients with long lags, but the dataset is quite small, so it’s hard to draw strong conclusions. We won’t go further down this rabbit hole as we know our data-generating process involved independent noise, but for a real dataset, we might want to try a model specification with autocorrelated errors to compare.\n\n\n# calculate the median predictions and residuals\ny_pred_data = mc_predict_regression(x, chain)\ny_med_data = quantile.(eachrow(y_pred_data), 0.5)\nresiduals = y_med_data .- y\n\n# plot the residuals and a line to show the zero\nplot(pacf(residuals, 1:4), line=:stem, marker=:circle, legend=:false, grid=:false, linewidth=2, xlabel=\"Lag\", ylabel=\"Partial Autocorrelation\", markersize=8, tickfontsize=14, guidefontsize=16, legendfontsize=16)\nhline!([0], linestyle=:dot, color=:red)\n\n\n\n\n\n\nFigure 4: Partial autocorrelation function of model residuals, relative to the predictive median.",
    "crumbs": [
      "Julia Tutorials",
      "MCMC with Turing"
    ]
  },
  {
    "objectID": "tutorials/turing-mcmc.html#fitting-extreme-value-models-to-tide-gauge-data",
    "href": "tutorials/turing-mcmc.html#fitting-extreme-value-models-to-tide-gauge-data",
    "title": "Markov Chain Monte Carlo With Turing",
    "section": "Fitting Extreme Value Models to Tide Gauge Data",
    "text": "Fitting Extreme Value Models to Tide Gauge Data\nLet’s now look at an example of fitting an extreme value distribution (namely, a generalized extreme value distribution, or GEV) to tide gauge data. GEV distributions have three parameters:\n\n\\(\\mu\\), the location parameter, which reflects the positioning of the bulk of the GEV distribution;\n\\(\\sigma\\), the scale parameter, which reflects the width of the bulk;\n\\(\\xi\\), the shape parameter, which reflects the thickness and boundedness of the tail.\n\nThe shape parameter \\(\\xi\\) is often of interest, as there are three classes of GEV distributions corresponding to different signs:\n\n\\(\\xi &lt; 0\\) means that the distribution is bounded;\n\\(\\xi = 0\\) means that the distribution has a thinner tail, so the “extreme extremes” are less likely;\n\\(\\xi &gt; 0\\) means that the distribution has a thicker tail.\n\n\nLoad Data\nFirst, let’s load the data. We’ll use data from the University of Hawaii Sea Level Center (Caldwell et al., 2015) for San Francisco, from 1897-2013. If you don’t have this data and are working with the notebook, download it here. We’ll assume it’s in a data/ subdirectory, but change the path as needed.\n\nCaldwell, P. C., Merrifield, M. A., & Thompson, P. R. (2015). Sea level measured by tide gauges from global oceans — the joint archive for sea level holdings (NCEI accession 0019568). NOAA National Centers for Environmental Information (NCEI). https://doi.org/10.7289/V5V40S7W\nThe dataset consists of dates and hours and the tide-gauge measurement, in mm. We’ll load the dataset into a DataFrame.\n\nfunction load_data(fname)\n    date_format = DateFormat(\"yyyy-mm-dd HH:MM:SS\")\n1    df = @chain fname begin\n2        CSV.File(; delim=',', header=false)\n3        DataFrame\n4        rename(\"Column1\" =&gt; \"year\",\n                \"Column2\" =&gt; \"month\",\n                \"Column3\" =&gt; \"day\",\n                \"Column4\" =&gt; \"hour\",\n                \"Column5\" =&gt; \"gauge\")\n        # need to reformat the decimal date in the data file\n5        @transform :datetime = DateTime.(:year, :month, :day, :hour)\n        # replace -99999 with missing\n6        @transform :gauge = ifelse.(abs.(:gauge) .&gt;= 9999, missing, :gauge)\n7        select(:datetime, :gauge)\n    end\n    return df\nend\n\n\n1\n\nThis uses the DataFramesMeta.jl package, which makes it easy to string together commands to load and process data\n\n2\n\nLoad the file, assuming there is no header.\n\n3\n\nConvert to a DataFrame.\n\n4\n\nRename columns for ease of access.\n\n5\n\nReformat the decimal datetime provided in the file into a Julia DateTime.\n\n6\n\nReplace missing data with missing.\n\n7\n\nSelect only the :datetime and :gauge columns.\n\n\n\n\nload_data (generic function with 1 method)\n\n\n\ndat = load_data(\"data/h551a.csv\")\nfirst(dat, 6)\n\n\n\nTable 1: Processed hourly tide gauge data from San Francisco, from 8/1/1897-1/31/2023.\n\n\n\n\n6×2 DataFrame\n\n\n\nRow\ndatetime\ngauge\n\n\n\nDateTime\nInt64?\n\n\n\n\n1\n1897-08-01T08:00:00\n3292\n\n\n2\n1897-08-01T09:00:00\n3322\n\n\n3\n1897-08-01T10:00:00\n3139\n\n\n4\n1897-08-01T11:00:00\n2835\n\n\n5\n1897-08-01T12:00:00\n2377\n\n\n6\n1897-08-01T13:00:00\n2012\n\n\n\n\n\n\n\n\n\n1@df dat plot(:datetime, :gauge, label=\"Observations\", bottom_margin=9mm)\nxaxis!(\"Date\", xrot=30)\nyaxis!(\"Mean Water Level\")\n\n\n1\n\nThis uses the DataFrame plotting recipe with the @df macro from StatsPlots.jl. This is not needed (you could replace e.g. :datetime with dat.datetime), but it cleans things up slightly.\n\n\n\n\n\n\n\n\n\nFigure 5: Hourly mean water at the San Francisco tide gauge from 1897-2023.\n\n\n\n\nNext, we need to detrend the data to remove the impacts of sea-level rise. We do this by removing a one-year moving average, centered on the data point, per the recommendation of Arns et al. (2013).\n\n# calculate the moving average and subtract it off\nma_length = 366\nma_offset = Int(floor(ma_length/2))\nmoving_average(series,n) = [mean(@view series[i-n:i+n]) for i in n+1:length(series)-n]\ndat_ma = DataFrame(datetime=dat.datetime[ma_offset+1:end-ma_offset], residual=dat.gauge[ma_offset+1:end-ma_offset] .- moving_average(dat.gauge, ma_offset))\n\n# plot\n@df dat_ma plot(:datetime, :residual, label=\"Detrended Observations\", bottom_margin=9mm)\nxaxis!(\"Date\", xrot=30)\nyaxis!(\"Mean Water Level\")\n\n\n\n\n\n\nFigure 6: Mean water level from the San Francisco tide gauge, detrended using a 1-year moving average centered on the data point, per the recommendation of Arns et al. (2013).\n\n\nArns, A., Wahl, T., Haigh, I. D., Jensen, J., & Pattiaratchi, C. (2013). Estimating extreme water level probabilities: A comparison of the direct methods and recommendations for best practise. Coast. Eng., 81, 51–66. https://doi.org/10.1016/j.coastaleng.2013.07.003\n\n\n\nThe last step in preparing the data is to find the annual maxima. We can do this using the groupby, transform, and combine functions from DataFrames.jl, as below.\n\n# calculate the annual maxima\n1dat_ma = dropmissing(dat_ma)\n2dat_annmax = combine(dat_ma -&gt; dat_ma[argmax(dat_ma.residual), :],\n                groupby(DataFrames.transform(dat_ma, :datetime =&gt; x-&gt;year.(x)), :datetime_function))\n3delete!(dat_annmax, nrow(dat_annmax))\n\n# make a histogram of the maxima to see the distribution\nhistogram(dat_annmax.residual, label=false)\nylabel!(\"Count\")\nxlabel!(\"Mean Water Level (mm)\")\n\n\n1\n\nIf we don’t drop the values which are missing, they will affect the next call to argmax.\n\n2\n\nThis first groups the data based on the year (with groupby and using Dates.year() to get the year of each data point), then pulls the rows which correspond to the maxima for each year (using argmax).\n\n3\n\nThis will delete the last year, in this case 2023, because the dataset only goes until March 2023 and this data point is almost certainly an outlier due to the limited data from that year.\n\n\n\n\n\n\n\n\n\nFigure 7: Histogram of annual block maxima from 1898-2022 from the San Francisco tide gauge dataset.\n\n\n\n\n\n\nFit The Model\n\n@model function gev_annmax(y)               \n1    μ ~ Normal(1000, 100)\n2    σ ~ truncated(Normal(0, 100); lower=0)\n3    ξ ~ Normal(0, 0.5)\n\n4    y ~ GeneralizedExtremeValue(μ, σ, ξ)\nend\n\n5gev_model = gev_annmax(dat_annmax.residual)\n6n_chains = 4\n7n_per_chain = 5000\n8gev_chain = sample(gev_model, NUTS(), MCMCThreads(), n_per_chain, n_chains; drop_warmup=true)\n@show gev_chain\n\n\n1\n\nLocation parameter prior: We know that this is roughly on the 1000 mm order of magnitude, but want to keep this relatively broad.\n\n2\n\nScale parameter prior: This parameter must be positive, so we use a normal truncated at zero.\n\n3\n\nShape parameter prior: These are usually small and are hard to constrain, so we will use a more informative prior.\n\n4\n\nThe data is independently GEV-distributed as we’ve removed the long-term trend and are using long blocks.\n\n5\n\nInitialize the model.\n\n6\n\nWe use multiple chains to help diagnose convergence.\n\n7\n\nThis sets the number of iterations for each chain.\n\n8\n\nSample from the posterior using NUTS and drop the iterations used to warmup the sampler.\n\n\n\n\n┌ Warning: Only a single thread available: MCMC chains are not sampled in parallel\n└ @ AbstractMCMC ~/.julia/packages/AbstractMCMC/Es490/src/sample.jl:307\nSampling (1 threads)   0%|                              |  ETA: N/A\n┌ Info: Found initial step size\n└   ϵ = 0.0125\n┌ Info: Found initial step size\n└   ϵ = 0.05\nSampling (1 threads)  25%|███████▌                      |  ETA: 0:00:09\nSampling (1 threads)  50%|███████████████               |  ETA: 0:00:04\n┌ Info: Found initial step size\n└   ϵ = 0.05\n┌ Info: Found initial step size\n└   ϵ = 0.00625\nSampling (1 threads)  75%|██████████████████████▌       |  ETA: 0:00:01\nSampling (1 threads) 100%|██████████████████████████████| Time: 0:00:04\nSampling (1 threads) 100%|██████████████████████████████| Time: 0:00:04\ngev_chain = MCMC chain (5000×15×4 Array{Float64, 3})\n\n\nChains MCMC chain (5000×15×4 Array{Float64, 3}):\n\nIterations        = 1001:1:6000\nNumber of chains  = 4\nSamples per chain = 5000\nWall duration     = 4.42 seconds\nCompute duration  = 4.07 seconds\nparameters        = μ, σ, ξ\ninternals         = lp, n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size\n\nSummary Statistics\n  parameters        mean       std      mcse     ess_bulk     ess_tail      rh ⋯\n      Symbol     Float64   Float64   Float64      Float64      Float64   Float ⋯\n\n           μ   1257.8424    5.6860    0.0529   11604.4401   11487.8002    1.00 ⋯\n           σ     57.2659    4.2532    0.0391   11801.2532   13349.1353    1.00 ⋯\n           ξ      0.0292    0.0624    0.0006   12247.1213   12849.7696    1.00 ⋯\n                                                               2 columns omitted\n\nQuantiles\n  parameters        2.5%       25.0%       50.0%       75.0%       97.5%\n      Symbol     Float64     Float64     Float64     Float64     Float64\n\n           μ   1246.8060   1253.9978   1257.8181   1261.6068   1269.1128\n           σ     49.6645     54.3097     57.0523     59.9626     66.1986\n           ξ     -0.0812     -0.0150      0.0251      0.0696      0.1617\n\n\n\nplot(gev_chain)\n\n\n\n\n\n\nFigure 8: Traceplots (left) and marginal distributions (right) from the MCMC sampler for the GEV model.\n\n\n\n\nFrom Figure 8, it looks like all of the chains have converged to the same distribution; the Gelman-Rubin diagnostic is also close to 1 for all parameters. Next, we can look at a corner plot to see how the parameters are correlated.\n\ncorner(gev_chain)\n\n\n\n\n\n\nFigure 9: Corner plot for the GEV model.\n\n\n\n\nFigure 9 suggests that the location and scale parameters \\(\\mu\\) and \\(\\sigma\\) are positively correlated. This makes some intuitive sense, as increasing the location parameter shifts the bulk of the distribution in a positive direction, and the increasing scale parameter then increases the likelihood of lower values. However, if these parameters are increased, the shape parameter \\(\\xi\\) decreases, as the tail of the GEV does not need to be as thick due to the increased proximity of outliers to the bulk.",
    "crumbs": [
      "Julia Tutorials",
      "MCMC with Turing"
    ]
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "This is a 3 credit course offered as an elective.\n\n\n\n\n\n Vivek Srikrishnan\n viveks@cornell.edu\n 318 Riley-Robb Hall\n\n\n\n\n\n\n Gabriela Ackermann Logan\n ga345@cornell.edu\n 319 Riley-Robb Hall\n\n\n\n\n\n\n MW\n 11:40-12:55pm\n 160 Riley-Robb Hall\n\n\n\n\n\n\nSimulations from numerical and statistical models are a key tool used to quantify and represent our understanding of many different environmental systems, including but not limited to the climate, sea levels, air pollution, and the electric power system. Data analysis of environmental observations and model simulations is an integral part of developing and evaluating models used to 1) test our assumptions about environmental system dynamics; 2) develop new insights into environmental processes; and 3) project future environmental conditions and outcomes. This course will provide an overview of the use of generative probability models to understand data and examine hypotheses about data-generating processes. The goal is to provide students with a framework and an initial toolkit of methods for data analysis and simulation. Students will actively analyze and use real data from a variety of environmental systems. In particular, over the course of the semester, we will:\n\nconduct exploratory analyses of environmental datasets;\ndiscuss best practices for and complexities of data visualization;\ncalibrate statistical and process-based numerical models using environmental data;\nuse simulations from calibrated models to identify key sources of uncertainty and model error;\nassess model fit and adequacy through predictive ability.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-overview",
    "href": "syllabus.html#course-overview",
    "title": "Syllabus",
    "section": "",
    "text": "This is a 3 credit course offered as an elective.\n\n\n\n\n\n Vivek Srikrishnan\n viveks@cornell.edu\n 318 Riley-Robb Hall\n\n\n\n\n\n\n Gabriela Ackermann Logan\n ga345@cornell.edu\n 319 Riley-Robb Hall\n\n\n\n\n\n\n MW\n 11:40-12:55pm\n 160 Riley-Robb Hall\n\n\n\n\n\n\nSimulations from numerical and statistical models are a key tool used to quantify and represent our understanding of many different environmental systems, including but not limited to the climate, sea levels, air pollution, and the electric power system. Data analysis of environmental observations and model simulations is an integral part of developing and evaluating models used to 1) test our assumptions about environmental system dynamics; 2) develop new insights into environmental processes; and 3) project future environmental conditions and outcomes. This course will provide an overview of the use of generative probability models to understand data and examine hypotheses about data-generating processes. The goal is to provide students with a framework and an initial toolkit of methods for data analysis and simulation. Students will actively analyze and use real data from a variety of environmental systems. In particular, over the course of the semester, we will:\n\nconduct exploratory analyses of environmental datasets;\ndiscuss best practices for and complexities of data visualization;\ncalibrate statistical and process-based numerical models using environmental data;\nuse simulations from calibrated models to identify key sources of uncertainty and model error;\nassess model fit and adequacy through predictive ability.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#learning-outcomes",
    "href": "syllabus.html#learning-outcomes",
    "title": "Syllabus",
    "section": "Learning Outcomes",
    "text": "Learning Outcomes\nAfter completing this class, students will be able to:\n\ncreate, interpret, and critique data visualizations;\ncalibrate environmental models to observations, possibly including censored and missing data;\nsimulate alternative datasets from models using statistical methods such as the bootstrap and Monte Carlo;\nassess model adequacy and performance using predictive simulations;\napply and contextualize model selection criteria;\nevaluate evidence for and against hypotheses about environmental systems using model simulations;\nemulate computationally-complex models with simpler representations.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#prerequisites-preparation",
    "href": "syllabus.html#prerequisites-preparation",
    "title": "Syllabus",
    "section": "Prerequisites & Preparation",
    "text": "Prerequisites & Preparation\nThe following courses/material would be ideal preparation:\n\nOne course in programming (e.g. CS 1110, 1112 or ENGRD/CEE 3200)\nOne course in probability or statistics (ENGRD 2700, CEE 3040, or equivalent)\n\nIn the absence of one or more these prerequisites, you can seek the permission of instructor.\n\n\n\n\n\n\nWhat If My Programming or Stats Skills Are Rusty?\n\n\n\nIf your programming or statistics skills are a little rusty, don’t worry! We will review concepts and build skills as needed.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#typical-topics",
    "href": "syllabus.html#typical-topics",
    "title": "Syllabus",
    "section": "Typical Topics",
    "text": "Typical Topics\n\nIntroduction to exploratory data analysis;\nReview of probability and statistics;\nBayesian decision theory;\nPrinciples of data visualization;\nModel residuals and discrepancies;\nCensored, truncated, and missing data;\nStatistical methods for calibration;\nPredictive model assessment;\nEmulation with surrogate models",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-meetings",
    "href": "syllabus.html#course-meetings",
    "title": "Syllabus",
    "section": "Course Meetings",
    "text": "Course Meetings\nThis course meets MW from 11:40–12:55 in 160 Riley-Robb Hall. In addition to the course meetings (a total of 42 lectures, 50 minutes each), the final project will be due during the university finals period. Students can expect to devote, on average, 6 hours of effort during the exam period.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-philosophy-and-expectations",
    "href": "syllabus.html#course-philosophy-and-expectations",
    "title": "Syllabus",
    "section": "Course Philosophy and Expectations",
    "text": "Course Philosophy and Expectations\nThe goal of our course is to help you gain competancy and knowledge in the area of data analysis. This involves a dual responsibility on the part of the instructor and the student. As the instructor, my responsibility is to provide you with a structure and opportunity to learn. To this end, I will commit to:\n\nprovide organized and focused lectures, in-class activities, and assignments;\nencourage students to regularly evaluate and provide feedback on the course;\nmanage the classroom atmosphere to promote learning;\nschedule sufficient out-of-class contact opportunities, such as office hours;\nallow adequate time for assignment completion;\nmake lecture materials, class policies, activities, and assignments accessible to students.\n\nI encourage you to discuss any concerns with me during office hours or through a course communications channel! Please let me know if you do not feel that I am holding up my end of the bargain.\nStudents can optimize their performance in the course by:\n\nattending all lectures;\ndoing any required preparatory work before class;\nactively participating in online and in-class discussions;\nbeginning assignments and other work early;\nand attending office hours as needed.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbooks-and-course-materials",
    "href": "syllabus.html#textbooks-and-course-materials",
    "title": "Syllabus",
    "section": "Textbooks and Course Materials",
    "text": "Textbooks and Course Materials\nThere is no required text for this class, and all course materials will be made available on the course website or through the Cornell library. However, the following books might be useful as a supplement to/expansion on the topics covered in class:\n\nGelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013). Bayesian Data Analysis (3rd ed.). http://www.stat.columbia.edu/~gelman/book/BDA3.pdf\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd ed.). https://xcelab.net/rm/\nD’Agostini, G. (2003). Bayesian Reasoning in Data Analysis: A Critical Introduction.\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and Other Stories. https://avehtari.github.io/ROS-Examples/index.html",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#community",
    "href": "syllabus.html#community",
    "title": "Syllabus",
    "section": "Community",
    "text": "Community\n\nDiversity and Inclusion\nOur goal in this class is to foster an inclusive learning environment and make everyone feel comfortable in the classroom, regardless of social identity, background, and specific learning needs. As engineers, our work touches on many critical aspects of society, and questions of inclusion and social justice cannot be separated from considerations of how data are generated, collected, and analyzed.\nIn all communications and interactions with each other, members of this class community (students and instructors) are expected to be respectful and inclusive. In this spirit, we ask all participants to:\n\nshare their experiences, values, and beliefs;\nbe open to and respectful of the views of others; and\nvalue each other’s opinions and communicate in a respectful manner.\n\nPlease let me know if you feel any aspect(s) of class could be made more inclusive. Please also share any preferred name(s) and/or your pronouns with me if you wish: I use he/him/his, and you can refer to me either as Vivek or Prof. Srikrishnan.\n\n\n\n\n\n\nPlease, Be Excellent To Teach Other\n\n\n\nWe all make mistakes in our communications with one another, both when speaking and listening. Be mindful of how spoken or written language might be misunderstood, and be aware that, for a variety of reasons, how others perceive your words and actions may not be exactly how you intended them. At the same time, it is also essential that we be respectful and interpret each other’s comments and actions in good faith.\n\n\n\n\nStudent Accomodations\nLet me know if you have any access barriers in this course, whether they relate to course materials, assignments, or communications. If any special accomodations would help you navigate any barriers and improve your chances of success, please exercise your right to those accomodations and reach out to me as early as possible with your Student Disability Services (SDS) accomodation letter. This will ensure that we have enough time to make appropriate arrangements.\n\n\n\n\n\n\nIf you need more immediate accomodations, but do not yet have a letter, please let me know and then follow up with SDS.\n\n\n\n\n\nCourse Communications\nMost course communications will occur via Ed Discussion. Public Ed posts are generally preferred to private posts or emails, as other students can benefit from the discussions. If you would like to discuss something privately, please do reach out through email or a private Ed post (which will only be viewable by you and the course staff).\nAnnouncements will be made on the course website and in Ed.\n\n\n\n\n\n\nEd Tips\n\n\n\n\nIf you wait until the day an assignment is due (or even late the previous night) to ask a question on Ed, there is a strong chance that I will not see your post prior to the deadline.\nBut if you see unanswered questions and you have some insight, please answer! This class will work best when we all work together as a community.\n\n\n\n\n\nMental Health Resources\nWe all have to take care of our mental health, just as we would our physical health. As a student, you may experience a range of issues which can negatively impact your mental health. Please do not ignore any of these stressors, or feel like you have to navigate these challenges alone! You are part of a community of students, faculty, and staff, who have a responsibility to look for one another’s well-being. If you are struggling with managing your mental health, or if you believe a classmate may be struggling, please reach out to the course instructor, the TA, or, for broader support, please take advantage of Cornell’s mental health resources.\n\n\n\n\n\n\nMental Health And This Class\n\n\n\nI am not a trained counselor, but I am here to support you in whatever capacity we can. You should never feel that you need to push yourself past your limits to complete any assignment for this class or any other. If we need to make modifications to the course or assignment schedule, you can certainly reach out to me, and all relevant discussions will be kept strictly confidential.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course Policies",
    "text": "Course Policies\n\nAttendance\nAttendance is not required, but in general, students who attend class regularly will do better and get more out of the class than students who do not. Your class participation grade will reflect both the quantity and quality of your participation, only some of which can occur asynchronously. I will put as many course materials, such as lecture notes and announcements, as possible online, but viewing materials online is not the same as active participation and engagement. Life happens, of course, and this may lead you to miss class. Let me know if you need any appropriate arrangements ahead of time.\n\n\n\n\n\n\nWhat If I’m Sick?\n\n\n\nPlease stay home if you’re feeling sick! This is beneficial for both for your own recovery and the health and safety of your classmates. We will also make any necessary arrangements for you to stay on top of the class material and if whatever is going on will negatively impact your grade, for example by causing you to be unable to submit an assignment on time.\n\n\n\n\nOffice Hours\nOffice hours will be held in 318 Riley-Robb after class (1-2pm) on MW. If these times do not work for you, or you need some additional time outside of office hours, please reach out to Prof. Srikrishnan about scheduling a meeting. Depending on schedules, these requests may not be accepted on short notice (e.g. homework is due on Friday and you reach out late on Thursday), but with several days notice we should be able to find a time that will work.\nOffice hours are intended to help all students who attend. This time is limited, and is best spent on issues that are relevant to as many students as possible. While we will do our best to answer individual questions, students asking us to verify or debug homework solutions will have the lowest priority (but please do ask about how to verify or debug your own solutions!). However, we are happy to discuss conceptual approaches to solving homework problems, which may help to reveal bugs.\nSpace at office hours can be limited (we may shift to the conference room in 316 Riley-Robb if offices are full and it is available). If the room is crowded and you can find an alternative source of assistance, or if your question is low priority (e.g. debugging) please be kind and make room for others.\n\n\nMask Policies\nPlease stay home and rest if you have symptoms of COVID-19 or any other respiratory illness. No masking will be required, but please be respectful of others who may wear masks or take other precautions to avoid illness. This policy may change if there is another outbreak of COVID-19 (or other illness), but will be kept consistent with broader Cornell mask policies.\n\n\nAcademic Integrity\n\n\n\n\n\n\nTL;DR: Don’t cheat, copy, or plagiarize!\n\n\n\nThis class is designed to encourage collaboration, and students are encouraged to discuss their work with other students. However, I expect students to abide by the Cornell University Code of Academic Integrity in all aspects of this class. All work submitted must represent the students’ own work and understanding, whether individually or as a group (depending on the particulars of the assignment). This includes analyses, code, software runs, and reports. Engineering as a profession relies upon the honesty and integrity of its practitioners (see e.g. the American Society for Civil Engineers’ Code of Ethics).\n\n\nExternal Resources\nThe collaborative environment in this class should not be viewed as an invitation for plagiarism. Plagiarism occurs when a writer intentionally misrepresents another’s words or ideas (including code!) as their own without acknowledging the source. All external resources which are consulted while working on an assignment should be referenced, including other students and faculty with whom the assignment is discussed. You will never be penalized for consulting an external source for help and referencing it, but plagiarism will result in a zero for that assignment as well as the potential for your case to be passed on for additional disciplinary action.\n\n\nGenerative AI/ML Policy\nAs noted, all work submitted for a grade in this course must reflect your own understanding. The goal of this course is to build critical data-analytic and statistical skills, including evaluations of the output of statistical models. While generative AI/ML models, such as ChatGPT, NotebookLM, or similar, can be used to summarize text and enhance coding efficiency, appropriate usage of these tools requires similar levels of scrutiny, which cannot be obtained without a general understanding of the underlying material. Relying on these tools as a primary means of engaging with the course content and material without independent effort is therefore discouraged. Further, blind reliance on generative AI can result in the submission of so-called “hallucinations” due to the predictive nature of the AI output.\nHowever, the appropriate and thoughtful use of these tools is a legitimate professional skill and reflects an understanding of the content in this course. As a result, the use and consulation of AI/ML tools, such as ChatGPT or similar, is not prohibited, but must be clearly referenced. In addition to citing the use of the tool, you must:\n\nreference the URL of the service you are using, including the specific date you accessed it;\nprovide the exact prompt(s) used to interact with the tool; and\ndescribe how you (directly or indirectly) integrated the response into your submission.\n\nFailure to fully reference the interaction, as described above, will be treated as plagiarism and referred to the University accordingly.\n\n\nLate Work Policy\nIn general, late work can be submitted up to 24 hours after the due date at a 50% penalty. However, sometimes things come up in life. Please reach out ahead of time if you have extenuating circumstances (including University-approved absences or illnesses) which would make it difficult for you to submit your work on time. Note that e.g. job interviews or a busy schedule outside of this course are not valid reasons for extensions. If an extension is granted, any late penalties will be waived up to the extension date. In extreme circumstances, assignments can be forgiven, and your grade will be computed as though those did not occur, giving your other assignments more weight.\n\n\nRegrade Requests\nRegrade requests can be submitted up to one week after the graded work is released on Gradescope.\nAll regrade requests must include a brief justification for the request or they will not be considered. Good justifications include (but are not limited to): - My answer agrees with the posted solution, but I still lost points. - I lost 4 points for something, but the rubric says it should only be worth 2 points. - You took points off for something, but it’s right here. - My answer is correct, even though it does not match the posted solution; here is an explanation. - There is no explanation for my grade. - I got a perfect score, but my solution has a mistake (you will receive extra credit for this! see below!) - There is a major error in the posted solution; here is an explanation (full credit for everyone, but Prof. Srikrishnan will decide what constitutes a “major error”! see below!).\n\n\n\n\n\n\nWe Can Only Grade What You Submitted\n\n\n\nAll regrades will be assessed based only on the submitted work. You cannot get a higher grade by explanation what you meant (either in person or online) or by adding information or reasoning to what is submitted after the fact. The goal of the regrade is to draw attention to a potential grading problem, not to supplement the submission.\n\n\nOnce Prof. Srikrishnan issues a final response to a regrade request, further requests for that submission will be ignored.\n\n\n\n\n\n\nRegrade Requests Can Be A Gamble!\n\n\n\nWhile you should submit regrade requests for legitimate errors, using them for fishing expeditions can also result in lost points if Prof. Srikrishnan decide that your initial grade was too lenient or if additional errors are identified.\n\n\n\n\n\n\n\n\nWhat If I Find A Different Type of Mistake?\n\n\n\n\nIf you submit a regrade request correctly reporting that a problem was graded too leniently — that is, that your score was higher than it should be based on the rubric — your score will be increased by the difference. For example, if your original score on a problem was 8/10 and you successfully argue that your score should have been 3/10, your new score will be 13/10.\nIf a significant error is discovered in a posted homework solution or in the exam solutions, everyone will in the class will receive full credit for the (sub)problem. Prof. Srikrishnan will decide what is “significant”.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#assessments",
    "href": "syllabus.html#assessments",
    "title": "Syllabus",
    "section": "Assessments",
    "text": "Assessments\n\nTechnologies\nWe will use Canvas as a gradebook, and to distribute PDFs of readings (which also be made available through the website, via the Cornell library). Ed Discussion will be used for course communications. Assignments will be submitted and graded in Gradescope.\nStudents can use any programming language they like to solve problems, though we will make notebooks and package environments available for Julia (which may help structure your assignments if you use a different language) via GitHub. If students use a language other than Julia, we may limited in the programming assistance we can provide (though we’re happy to try to help!).\nWe recommend students create a GitHub account and use GitHub to version control and share their code throughout the semester.\n\n\nGrading\nFinal grades will be computed based on the following assessment weights:\n\n\n\nAssessment\nWeight\n\n\n\n\nParticipation\n5%\n\n\nReadings\n10%\n\n\nQuizzes\n10%\n\n\nLiterature Critique\n15%\n\n\nHomework Assignments\n30%\n\n\nTerm Project\n30%\n\n\n\nThe following grading scale will be used to convert the numerical weighted average to letter grades:\n\n\n\nGrade\nRange\n\n\n\n\nA\n93–100\n\n\nA-\n90–93\n\n\nB+\n87–90\n\n\nB\n83–87\n\n\nB-\n80–83\n\n\nC+\n77–80\n\n\nC\n73–77\n\n\nC-\n70–73\n\n\nD+\n67–70\n\n\nD\n63–67\n\n\nD-\n60–63\n\n\nF\n&lt; 60\n\n\n\n\n\nParticipation\nStudents will be assessed based on their participation across class activities, including lectures and discussions (in-person and online). Routine lack of attendance or engagement will result in the loss of participation points.\n\n\nReadings\nReadings will be assigned for discussion throughout the semester. We will discuss the readings in class the subsequent week after they are assigned. One student will be asked to briefly summarize the reading before the broader discussion. Collaborative annotation assignments will be set up in Canvas to facilitate reading ahead of the discussion. Students in BEE 5850 should also submit a (maximum) one-page summary of the reading, highlighting its key point(s) and their assessment of the effectiveness of the argument, to Gradescope before the Monday after the reading is assigned. Students will be evaluated on both the extent to which their annotations and summaries reflect engagement with the reading and their participation in the class discussion.\n\n\nQuizzes\nQuizzes will be assigned in Gradescope most weeks based on recently discussed content. These will be relatively short and are intended to consolidate material recently discussed in class. The quizzes will be released after Wednesday’s class and will be due prior to next Monday’s. Quizzes can be retaken as many times as are desired prior to the submission deadline. They may consist of multiple choice questions or questions involving small mathematical, computational, or open-ended problems.\n\n\nLiterature Critique\nStudents will select a peer-reviewed journal article related to an application of data analysis and will write a short discussion paper (2-3 pages) analyzing the hypotheses and statistical choices. Students should feel free to select their own paper or can work with Prof. Srikrishnan to identify one of interest, but in all cases should discuss with Prof. Srikrishnan to ensure that the article is appropriate. The discussion paper will be due towards at the end of the semester.\n\n\nHomework Assignments\nApproximately 6 homework assignments will be assigned throughout the semester (roughly one per course module). Homework problems will generally involve more substantial mathematical or computational work than the quiz problems. You will typically have 2 weeks to work on each assignment, though this depends on the module length. Students are encouraged to collaborate and learn from each other on homework assignments, but each student must submit their own solutions reflecting their understanding of the material. Consulting and referencing external resources and your peers is encouraged (engineering is a collaborative discipline!), but plagiarism is a violation of academic integrity.\nSome notes on assignment and grading logistics:\n\nHomeworks are due by 9:00pm Eastern Time on the designed due date. Your assignment notebook (which include your writeup and codes) should be submitted to Gradescope as a PDF with the answers to each question tagged (a failure to do this will result in deductions).\nA standard rubric is available.\nStudents in 5850 will be asked to complete additional homework problems which go more deeply into the underlying concepts or apply more advanced techniques.\nRegrade requests for specific problems must be made within a week of the grading of that assignment.\n\n\n\nTerm Project\nThroughout the semester, students will apply the concepts and methods from class to a data set of their choosing. If a student does not have a data set in mind, we will find one which aligns with their interests.\nThe term project can be completed individually or in groups of 2. The deliverables are:\n\nA proposal describing the research question and hypotheses, the data set, and the numerical or statistical models the student would like to use to test the hypotheses;\nA final presentation and report. The report should be no more than 5 pages (11 point font, 1 inch margins), not including references and figures. The presentation should be no more than 10 minutes and will be delivered in-class; presentations may be spread across multiple class periods if the number of projects requires it. More details and rubrics will be provided later in the semester.",
    "crumbs": [
      "Course Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Term Project",
    "section": "",
    "text": "The term project gives you an opportunity to use and extend the methods we have learned in class to an environmental data set and/or model of your choosing. More details will be provided over the semester. We will discuss each component in class as well as providing relevant information on this page.",
    "crumbs": [
      "Term Project"
    ]
  },
  {
    "objectID": "project.html#overview",
    "href": "project.html#overview",
    "title": "Term Project",
    "section": "",
    "text": "The term project gives you an opportunity to use and extend the methods we have learned in class to an environmental data set and/or model of your choosing. More details will be provided over the semester. We will discuss each component in class as well as providing relevant information on this page.",
    "crumbs": [
      "Term Project"
    ]
  },
  {
    "objectID": "project.html#instructions",
    "href": "project.html#instructions",
    "title": "Term Project",
    "section": "Instructions",
    "text": "Instructions\n\nStudents can work individually or in groups of 2.\nThe project proposal should be no more than 2 pages (1-inch margins, 11 point Calibri font or equivalent) and should include a brief summary of the project question and a data and modeling plan.\nIf the project is being done in a group, the proposal should also include a plan for the allocation of duties to each group member (not included in the 2 page limit).\nPresentations should be no more than 10 minutes (this will be timed!) and should capture the key motivation of the project and key methods and results. If we have more projects than can be fit into class on May 5, we will also do presentations on April 30 (we will discuss this by mid-March).\nSlides for the presentation should be sent to Prof. Srikrishnan by email the evening before the presentation.\nProject reports should be no more than 5 pages (1-inch margins, 11 point Calibri font or equivalent), not including figures and references, but can include an appendix with additional details on methods.",
    "crumbs": [
      "Term Project"
    ]
  },
  {
    "objectID": "project.html#schedule",
    "href": "project.html#schedule",
    "title": "Term Project",
    "section": "Schedule",
    "text": "Schedule\n\n\n\n\n\n\n\nMilestone\nDue Date\n\n\n\n\nProposal\nFri, Mar 14\n\n\nPresentations\nMon, May 05\n\n\nFinal Report\nFri, May 16",
    "crumbs": [
      "Term Project"
    ]
  }
]